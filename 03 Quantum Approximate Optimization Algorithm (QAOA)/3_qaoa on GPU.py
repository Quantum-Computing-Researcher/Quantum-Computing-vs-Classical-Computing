#!/usr/bin/env python3
# -*- coding: utf-8 -*-


"""3 QAOA for GPU
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/3-qaoa.52bc5879-252f-4e60-99d6-c1524a8f04a8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251121/auto/storage/goog4_request%26X-Goog-Date%3D20251121T143802Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2841ef09ebd2cfca693d073178c691c4cce88863101585012c06ccef347a5de6b258c1a3cd7a36009234c3fd5b0ac108b9f7d4fd89b2fee36fe39ab9bcb51df17fa16b574a9fbf47a3176c81e7688ed03f0d465d8a6b27d4883d43a4e76b3c41c7220ccd5c581b3ba2603509600cfcac01b1d969afc46bcb216d1ea92f20b17ba3ad2f5bb4112005abfef8625e751cf7b7bd1cbc1cd8a99db95dec81d36ff922fae5add87b12a73e2fb9fcafa6022f021579b3248e83670b7830cf3d9822102f2b703785991af11bfbbbbc0ec8dd48ef0e1a8f3ce9235f9e389b344bc630ce03dfca91265f653634b33f5169f8b9e3a7d3f05fb75564310c8a87832478202bdf
"""

# GPU-accelerated Empirical Evaluation
# IMPORTANT: Statevector simulation scales as 2^n.
# The script enforces MAX_QUBITS = 16 for safe, fast execution.

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# GPU-accelerated Empirical Evaluation
# 3 QAOA For GPU

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

# ------------********------------ Imports
import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


# ------------********------------ GPU Setup and Dependencies

print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

#This helper function checks whether required Python packages are installed and automatically installs any that are missing.
#It attempts to import each package, collects missing ones, and invokes pip to install them quietly.
#If installation fails, it prints an error message but allows the script to continue gracefully.

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil", "scikit-learn", "networkx", "qiskit"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import networkx as nx

# Qiskit for quantum simulation
from qiskit import QuantumCircuit
from qiskit.quantum_info import Statevector

# ------------********------------  Quantum Limits ============================== #

MAX_QUBITS = 16  # Hard cap for safe statevector simulations
SIGN_CACHE: Dict[int, np.ndarray] = {}  # Cache for Z-sign tables


#This function reports the name of the compute device being used, preferring GPU information when available.
#If a GPU is detected, it queries CUDA properties; if that fails, it returns a generic GPU placeholder.
#When no GPU is present, it defaults to indicating CPU/NumPy execution.


# GPU utility functions
def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

def to_numpy(x):
    """Safely convert CuPy array to NumPy array"""
    if HAS_GPU and isinstance(x, cp.ndarray):
        return cp.asnumpy(x)
    return x

def to_cupy(x):
    """Safely convert NumPy array to CuPy array"""
    if HAS_GPU and isinstance(x, np.ndarray):
        return cp.asarray(x)
    return x

# Thread-safe operations
_gpu_lock = threading.Lock()


#This function wraps a single experiment execution inside a GPU lock 
# to prevent concurrent access from multiple threads.
#It unpacks experiment parameters, acquires the lock, 
#and safely invokes the underlying experiment routine.
#The return value is passed through unchanged, ensuring thread-safe GPU execution.

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    problem_type, n_qubits, seed = args
    with _gpu_lock:
        return _run_single_experiment(problem_type, n_qubits, seed)

# ------------********------------  QAOA Core Implementation =========================

#This function validates that a requested qubit count does not exceed the allowed statevector simulation limit.
def _check_qubit_limit(n: int):
    """Enforce qubit limit for statevector simulation"""
    if n > MAX_QUBITS:
        raise ValueError(
            f"Requested n={n} qubits exceeds statevector cap MAX_QUBITS={MAX_QUBITS}.\n"
            f"Please use smaller sizes, e.g.: 8, 10, 12, 14"
        )

#This function generates a random weighted undirected graph for Max-Cut 
#by sampling edges with a given probability and assigning random weights.
#It keeps only the upper triangle of the matrix, mirrors it for symmetry, 
#and removes self-loops by zeroing the diagonal.
#The resulting float matrix serves as a reproducible Max-Cut instance 
#for benchmarking or quantum circuit generation.

def random_weighted_graph(n: int, edge_prob: float = 0.5, seed: int = 0) -> np.ndarray:
    """Generate random weighted graph for Max-Cut problem"""
    rng = np.random.default_rng(seed)
    A = rng.random((n, n))
    W = (A < edge_prob).astype(float)
    weights = rng.integers(1, 11, size=(n, n))
    W = W * weights
    W = np.triu(W, 1)
    W = W + W.T
    np.fill_diagonal(W, 0.0)
    return W.astype(float)

#This function generates a random QUBO matrix 
#by sampling entries under a density mask 
#and assigning integer coefficients within a specified range.
#It symmetrizes the matrix by keeping the upper triangle, mirroring it, and preserving a single diagonal.
#The resulting float-valued QUBO matrix provides a randomized problem instance for optimization or quantum experiments.

def random_qubo(n: int, density: float = 0.3, seed: int = 0) -> np.ndarray:
    """Generate random QUBO problem"""
    rng = np.random.default_rng(seed)
    mask = rng.random((n, n)) < density
    Q = rng.integers(-5, 11, size=(n, n)) * mask
    Q = np.triu(Q)
    Q = Q + Q.T - np.diag(np.diag(Q))
    return Q.astype(float)

#The following function computes the Max-Cut objective by converting a bitstring into spin form and applying the standard quadratic cost formula.


def maxcut_cost_bitstring(x: np.ndarray, W: np.ndarray) -> float:
    """Compute Max-Cut cost for a given bitstring"""
    s = 1 - 2 * x
    return 0.25 * (np.sum(W) - np.sum(W * np.outer(s, s)))

#The following function converts a QUBO matrix into an equivalent Ising model by extracting constant terms,
# linear coefficients, and pairwise couplings.
#It produces the components needed for Ising-based solvers 
#or quantum circuit constructions.

def qubo_to_ising(Q: np.ndarray) -> Tuple[float, np.ndarray, Dict[Tuple[int,int], float]]:
    """Convert QUBO to Ising model formulation"""
    n = Q.shape[0]
    const = 0.5 * np.trace(Q) + 0.5 * np.sum(np.triu(Q, 1))
    a = np.zeros(n, dtype=float)
    b: Dict[Tuple[int,int], float] = {}

    for i in range(n):
        a[i] -= 0.5 * Q[i, i]
        for j in range(i+1, n):
            a[i] -= 0.5 * Q[i, j]
            a[j] -= 0.5 * Q[i, j]
            b[(i, j)] = 0.5 * Q[i, j]

    # Negate for maximization
    const = -const
    a = -a
    b = {k: -v for k, v in b.items()}
    return const, a, b

#This function generates (and caches) Z-basis sign patterns for all qubits and all computational basis states and it builds a matrix where each 
#row corresponds to a qubit and each column encodes its ¬±1 Z-eigenvalue across bitstrings. #Caching avoids recomputation and speeds up 
# repeated expectation-value calculations.

def _z_signs(n: int) -> np.ndarray:
    """Cache Z operator signs for expectation calculations"""
    if n in SIGN_CACHE:
        return SIGN_CACHE[n]
    M = 1 << n
    idx = np.arange(M, dtype=np.uint64)
    signs = np.empty((n, M), dtype=np.int8)
    for i in range(n):
        bit = (idx >> i) & 1
        signs[i, :] = 1 - 2 * bit  # 0 -> +1, 1 -> -1
    SIGN_CACHE[n] = signs
    return signs

def _apply_maxcut_cost_layer(circ: QuantumCircuit, W: np.ndarray, gamma: float):
    """Apply Max-Cut cost unitary"""
    n = W.shape[0]
    for i in range(n):
        for j in range(i+1, n):
            w = float(W[i, j])
            if w != 0.0:
                circ.rzz(-gamma * w, i, j)

def _apply_qubo_cost_layer(circ: QuantumCircuit, a: np.ndarray, b: Dict[Tuple[int,int], float], gamma: float):
    """Apply QUBO cost unitary"""
    n = len(a)
    for i in range(n):
        ai = float(a[i])
        if ai != 0.0:
            circ.rz(2.0 * gamma * ai, i)
    for (i, j), bij in b.items():
        if bij != 0.0:
            circ.rzz(2.0 * gamma * float(bij), i, j)

def _apply_mixer_layer(circ: QuantumCircuit, beta: float):
    """Apply QAOA mixer unitary"""
    for q in range(circ.num_qubits):
        circ.rx(2.0 * beta, q)

#This function constructs a full QAOA circuit by first preparing qubits in the |+‚ü© state
# and then applying alternating cost and mixer layers.
# Depending on whether the problem is Max-Cut or QUBO, 
#it invokes the appropriate cost-layer builder, converting QUBO to Ising form when needed.
#It enforces qubit limits before circuit creation and returns the completed parameterized QAOA circuit.

def build_qaoa_circuit(problem: str, mat: np.ndarray, p_layers: int,
                       gamma: float, beta: float) -> QuantumCircuit:
    """Build QAOA circuit for given problem and parameters"""
    n = mat.shape[0]
    _check_qubit_limit(n)
    qc = QuantumCircuit(n)

    # Initial state |+>^n
    qc.h(range(n))

    for _ in range(p_layers):
        if problem == "maxcut":
            _apply_maxcut_cost_layer(qc, mat, gamma)
        else:  # qubo
            const, a, b = qubo_to_ising(mat)
            _apply_qubo_cost_layer(qc, a, b, gamma)
        _apply_mixer_layer(qc, beta)

    return qc

def statevector_from_qc(qc: QuantumCircuit) -> np.ndarray:
    """Compute statevector from quantum circuit"""
    sv = Statevector.from_instruction(qc)
    return np.asarray(sv.data, dtype=np.complex128)

def expectation_maxcut(probs: np.ndarray, W: np.ndarray) -> Tuple[float, np.ndarray]:
    """Compute Max-Cut expectation value"""
    n = W.shape[0]
    S = _z_signs(n)
    cut = 0.0

    for i in range(n):
        for j in range(i+1, n):
            w = W[i, j]
            if w != 0.0:
                zz = float(np.dot(probs, (S[i, :] * S[j, :]).astype(np.float64)))
                cut += w * 0.5 * (1.0 - zz)

    # Most probable bitstring
    max_k = int(np.argmax(probs))
    x_bits = np.array([(max_k >> i) & 1 for i in range(n)], dtype=np.int64)
    return cut, x_bits

#This function computes the QUBO expectation value 
#by converting the QUBO matrix into its Ising form and using cached Z-sign tables.
#It evaluates ‚ü®Z‚ü© and ‚ü®ZZ‚ü© terms via probability‚Äìweighted dot products, 
# accumulating constant, linear, and pairwise contributions.
#It also extracts the most probable bitstring from the distribution 
#and returns both the expectation value and that bitstring.

def expectation_qubo(probs: np.ndarray, Q: np.ndarray) -> Tuple[float, np.ndarray]:
    """Compute QUBO expectation value"""
    n = Q.shape[0]
    const, a, b = qubo_to_ising(Q)
    S = _z_signs(n)

    z_exp = np.array([float(np.dot(probs, S[i, :].astype(np.float64))) for i in range(n)])
    exp_val = const + float(np.dot(a, z_exp))

    for (i, j), bij in b.items():
        zz = float(np.dot(probs, (S[i, :] * S[j, :]).astype(np.float64)))
        exp_val += bij * zz

    max_k = int(np.argmax(probs))
    x_bits = np.array([(max_k >> i) & 1 for i in range(n)], dtype=np.int64)
    return exp_val, x_bits

#This function performs a full grid search over QAOA parameters (Œ≥, Œ≤) 
#to find the best-performing configuration for a given problem.
#For each parameter pair, it builds the QAOA circuit, obtains its statevector, 
#computes probabilities, and evaluates the Max-Cut or QUBO expectation.
#It tracks and returns the best bitstring, its objective value, 
#and metadata about the search space.

def run_qaoa_grid_search(problem: str, mat: np.ndarray, p_layers: int = 1) -> Tuple[np.ndarray, float, Dict[str, Any]]:
    """Run QAOA with grid search over parameters"""
    n = mat.shape[0]
    _check_qubit_limit(n)

    best_obj = -1e308
    best_x = None
    gamma_grid = [0.2, 0.4, 0.6, 0.8, 1.0]
    beta_grid = [0.2, 0.4, 0.6, 0.8, 1.0]

    for gamma in gamma_grid:
        for beta in beta_grid:
            qc = build_qaoa_circuit(problem, mat, p_layers, gamma, beta)
            psi = statevector_from_qc(qc)
            probs = (psi.real**2 + psi.imag**2).astype(np.float64)

            if problem == "maxcut":
                exp_val, x_bits = expectation_maxcut(probs, mat)
            else:
                exp_val, x_bits = expectation_qubo(probs, mat)

            if exp_val > best_obj:
                best_obj = exp_val
                best_x = x_bits

    return best_x, float(best_obj), {
        "gamma_candidates": len(gamma_grid),
        "beta_candidates": len(beta_grid),
        "layers": p_layers
    }

def plot_maxcut_graph(W: np.ndarray, x: np.ndarray, out_path: str, title: str = "") -> None:
    """Visualize Max-Cut solution"""
    G = nx.Graph()
    n = W.shape[0]

    for i in range(n):
        G.add_node(i, part=int(x[i]))

    for i in range(n):
        for j in range(i + 1, n):
            w = float(W[i, j])
            if w > 0.0:
                G.add_edge(i, j, weight=w, cut=(x[i] != x[j]))

    pos = nx.spring_layout(G, seed=42)
    node_colors = ["tab:blue" if G.nodes[i]["part"] == 0 else "tab:orange" for i in G.nodes()]
    cut_edges = [(u, v) for u, v, d in G.edges(data=True) if d["cut"]]
    in_edges = [(u, v) for u, v, d in G.edges(data=True) if not d["cut"]]

    all_w = [G[u][v]["weight"] for u, v in G.edges()] or [1.0]
    maxw = max(all_w)
    lw_cut = [1.0 + 3.0 * (G[u][v]["weight"] / maxw) for u, v in cut_edges]
    lw_in = [0.5 + 2.0 * (G[u][v]["weight"] / maxw) for u, v in in_edges]

    plt.figure(figsize=(7, 6))
    nx.draw_networkx_nodes(G, pos, node_size=140, node_color=node_colors)
    nx.draw_networkx_edges(G, pos, edgelist=in_edges, width=lw_in, alpha=0.45)
    nx.draw_networkx_edges(G, pos, edgelist=cut_edges, width=lw_cut, style="dashed")
    nx.draw_networkx_labels(G, pos, font_size=8)

    cut_val = maxcut_cost_bitstring(x, W)
    plt.title(title or f"Max-Cut partition (weighted cut = {cut_val:.2f})")
    plt.axis("off")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()

#This function runs a complete QAOA experiment for a single problem instance, 
#handling seeding, problem generation, execution, and error recovery.
#It records runtime, peak memory usage, objective values, 
#and metadata while ensuring GPU memory cleanup and thread safety.
#Returned results provide a full diagnostic snapshot of the experiment, including device info and success status.
def _run_single_experiment(problem_type: str, n_qubits: int, seed: int) -> dict:
    """
    Core experiment logic - run single QAOA experiment.
    """
    # Fixed parameters
    p_layers = 1

    # Set random seed
    np.random.seed(seed)
    if HAS_GPU:
        cp.random.seed(seed)

    tracemalloc.start()
    t0 = time.perf_counter()

    try:
        # Generate problem instance
        if problem_type == "maxcut":
            mat = random_weighted_graph(n_qubits, edge_prob=0.5, seed=seed)
        else:
            mat = random_qubo(n_qubits, density=0.3, seed=seed)

        # Run QAOA
        best_x, best_obj, meta = run_qaoa_grid_search(problem_type, mat, p_layers)

        # Compute additional metrics
        if problem_type == "maxcut":
            actual_cost = maxcut_cost_bitstring(best_x, mat)
            success = True
        else:
            # For QUBO, use the expectation value directly
            actual_cost = best_obj
            success = True

    except Exception as e:
        print(f"‚ùå Experiment failed for {problem_type}, n={n_qubits}, seed={seed}: {e}")
        best_x = np.zeros(n_qubits, dtype=int)
        best_obj = 0.0
        actual_cost = 0.0
        success = False
        meta = {"gamma_candidates": 0, "beta_candidates": 0, "layers": p_layers}

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "problem_type": str(problem_type),
        "n_qubits": int(n_qubits),
        "seed": int(seed),
        "p_layers": int(p_layers),
        "success": bool(success),
        "objective_value": float(best_obj),
        "actual_cost": float(actual_cost),
        "gamma_candidates": int(meta["gamma_candidates"]),
        "beta_candidates": int(meta["beta_candidates"]),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "device": get_device_name()
    }

# ------------********------------ Carbon I/O

#This function resolves a user-provided Excel filename by checking whether it is an absolute, existing path.
#If not, it falls back to searching for the file on the user‚Äôs Desktop, returning whichever path exists.

def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

#The following code loads a carbon-intensity Excel file and automatically detects columns for 
#country, year, and CO‚ÇÇ intensity using flexible name matching.
#It normalizes the table, keeps the latest year per country if requested, 
#and heuristically converts g/kWh values to kg/kWh when needed.
#The cleaned DataFrame is returned with standardized column names 
#and only valid rows, ready for downstream carbon calculations.
def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------ Plot Helpers 

#This code nippet defines common Matplotlib save parameters 
# and provides a compatibility wrapper for boxplots across Matplotlib versions.
#It attempts to use the newer `tick_labels` argument and falls back to the older `labels` parameter if needed.
#This ensures boxplots render correctly regardless of the installed Matplotlib version.

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    # Runtime vs qubits
    g = df.groupby("n_qubits")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g["n_qubits"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number of Qubits")
    plt.ylabel("Runtime (s)")
    plt.title(f"Performance: Runtime vs Qubits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Memory usage vs qubits
    g2 = df.groupby("n_qubits")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["n_qubits"], g2["peak_mem_mb"], marker="s", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Qubits")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Performance: Memory vs Qubits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_memory_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Objective value vs qubits
    g3 = df.groupby("n_qubits")["objective_value"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g3["n_qubits"], g3["objective_value"], marker="^", linewidth=2, markersize=8, color='purple')
    plt.xlabel("Number of Qubits")
    plt.ylabel("Objective Value")
    plt.title(f"Performance: Objective Value vs Qubits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_objective_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Success rate
    success_rates = df.groupby("n_qubits")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["n_qubits"].astype(str), success_rates["success"])
    plt.xlabel("Number of Qubits")
    plt.ylabel("Success Rate")
    plt.title(f"Performance: Success Rate by Qubit Count\n{get_device_name()}")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_success_rate.png"), **plt_kwargs)
    plt.close()

def plot_scalability(df: pd.DataFrame, outdir: str):
    # Runtime vs qubits (log-log)
    g = df.groupby("n_qubits")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.loglog(g["n_qubits"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number of Qubits")
    plt.ylabel("Runtime (s)")
    plt.title(f"Scalability: Runtime vs Qubits (log-log)\n{get_device_name()}")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Memory vs qubits
    g2 = df.groupby("n_qubits")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.semilogy(g2["n_qubits"], g2["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Qubits")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Scalability: Memory vs Qubits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Parameter search complexity
    g3 = df.groupby("n_qubits")[["gamma_candidates", "beta_candidates"]].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.plot(g3["n_qubits"], g3["gamma_candidates"] * g3["beta_candidates"],
             marker="s", linewidth=2, markersize=8, color='orange')
    plt.xlabel("Number of Qubits")
    plt.ylabel("Parameter Combinations")
    plt.title(f"Scalability: Parameter Search Complexity\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_parameter_complexity.png"), **plt_kwargs)
    plt.close()

def plot_reliability(df: pd.DataFrame, outdir: str):
    # Objective value distribution by qubit count
    data = [df[df["n_qubits"] == n]["objective_value"].values for n in sorted(df["n_qubits"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"Qubits={n}" for n in sorted(df["n_qubits"].unique())])
    plt.xlabel("Number of Qubits")
    plt.ylabel("Objective Value")
    plt.title(f"Reliability: Objective Distribution by Qubit Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_objective_distribution.png"), **plt_kwargs)
    plt.close()

    # Success rate by qubit count
    success_rates = df.groupby("n_qubits")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["n_qubits"].astype(str), success_rates["success"])
    plt.xlabel("Number of Qubits")
    plt.ylabel("Success Rate")
    plt.title(f"Reliability: Success Rate by Qubit Count\n{get_device_name()}")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "rel_success_rate.png"), **plt_kwargs)
    plt.close()

    # Runtime distribution
    data = [df[df["n_qubits"] == n]["runtime_s"].values for n in sorted(df["n_qubits"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"Qubits={n}" for n in sorted(df["n_qubits"].unique())])
    plt.yscale("log")
    plt.xlabel("Number of Qubits")
    plt.ylabel("Runtime (s, log scale)")
    plt.title(f"Reliability: Runtime Distribution\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_runtime_distribution.png"), **plt_kwargs)
    plt.close()

    # Objective heatmap
    pivot_data = df.pivot_table(values="objective_value", index="n_qubits", columns="problem_type", aggfunc="mean")
    plt.figure(figsize=(10, 6))
    plt.imshow(pivot_data.values, aspect="auto", cmap="viridis", origin="lower")
    plt.colorbar(label="Objective Value")
    plt.xticks(range(len(pivot_data.columns)), pivot_data.columns)
    plt.yticks(range(len(pivot_data.index)), pivot_data.index)
    plt.xlabel("Problem Type")
    plt.ylabel("Number of Qubits")
    plt.title(f"Reliability: Objective Value Heatmap\n{get_device_name()}")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "rel_objective_heatmap.png"), **plt_kwargs)
    plt.close()

def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------ Utility Functions 

#The ensure_dir(d: str) function guarantees that a directory exists by creating it if necessary, 
#avoiding errors on repeated calls.


def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)

#The def to_excel function saves multiple DataFrames into a single Excel workbook, 
#placing each one in its own sheet while respecting Excel‚Äôs sheet-name length limit.
#Together, def ensure_dir(d: str): and def to_excel(dfs: Dict[str, pd.DataFrame], path: str): provide simple utilities 
#for structured file output and organized result storage.

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    """Save multiple DataFrames to Excel with different sheets"""
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------ Main Experiment Runner
#This function launches a QAOA experiment suite using fixed parameters tailored for a Kaggle environment, including qubit sizes, trial counts, and threading.
#It prints a clear configuration summary‚Äîproblem type, parameter grid, hardware description, and carbon accounting settings‚Äîbefore execution begins.
#These settings guide subsequent parallel runs, carbon estimation, and result aggregation.

def run_qaoa_experiment():
    """
    Main QAOA experiment runner with hardcoded parameters for Kaggle.
    Equivalent to: --problem maxcut --n_qubits 8,10,12,14 --repeats 2
                   --device-power-watts 65 --pue 1.2 --combine
    """
    # Experiment parameters
    problem_types = ["maxcut"]  # Focus on Max-Cut for demonstration
    n_qubits_list = [8, 10, 12, 14]  # Stay within safe statevector limits
    trials = 2   # trials per configuration
    workers = 4   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting QAOA experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Problem types: {problem_types}")
    print(f"   Qubit counts: {n_qubits_list}")
    print(f"   Trials per configuration: {trials}")
    print(f"   QAOA layers: 1")
    print(f"   Parameter grid: 5√ó5 (gamma √ó beta)")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for problem_type in problem_types:
        for n_qubits in n_qubits_list:
            for i in range(trials):
                seed = 1000 + 17 * n_qubits + 31 * len(problem_type) + i
                jobs.append((problem_type, n_qubits, seed))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        status = "‚úÖ" if row["success"] else "‚ùå"
        obj_val = f"{row['objective_value']:.3f}"
        print(f"  {status} {row['problem_type']}, n={row['n_qubits']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.6f}s, objective={obj_val})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:

            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed problem_type={job[0]}, n_qubits={job[1]}, seed={job[2]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("‚ùå All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1], job[2])
                _consume(result)
            except Exception as e:
                print(f"‚ùå Sequential fallback also failed for problem_type={job[0]}, n_qubits={job[1]}, seed={job[2]}: {e}")

    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

    # ------------********------------
    # ------------********------------ Performance Results ----------------

    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")

    # Only aggregate successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0:
        perf_agg = perf_df.groupby(["problem_type", "n_qubits"]).agg({
            "runtime_s": ["mean", "std"],
            "peak_mem_mb": ["mean", "std"],
            "objective_value": ["mean", "std"],
            "actual_cost": ["mean", "std"],
            "success": "mean",
            "gamma_candidates": "mean",
            "beta_candidates": "mean"
        }).reset_index()

        # Flatten column names
        perf_agg.columns = ['_'.join(col).strip('_') for col in perf_agg.columns.values]
    else:
        perf_agg = pd.DataFrame({"note": ["No successful experiments"]})

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

    # ------------********------------
    # ------------********------------ Scalability Results ----------------

    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")

    # Compute scaling coefficients only for successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0 and len(success_df["n_qubits"].unique()) >= 2:

        # Log-log fit for scaling analysis
        scaling_data = success_df.groupby("n_qubits")["runtime_s"].mean().reset_index()
        log_qubits = np.log(scaling_data["n_qubits"].values)
        log_time = np.log(scaling_data["runtime_s"].values + 1e-12)

        # Only compute if we have enough data and variation
        if len(log_qubits) >= 2 and np.std(log_qubits) > 1e-6 and np.std(log_time) > 1e-6:
            try:
                scaling_coeff = np.polyfit(log_qubits, log_time, 1)[0]
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [scaling_coeff],
                    "description": ["Exponent in time ~ qubits^exponent"]
                })
            except:
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [np.nan],
                    "description": ["Numerical issues in scaling analysis"]
                })
        else:
            scaling_summary = pd.DataFrame({
                "parameter": ["scaling_exponent"],
                "value": [np.nan],
                "description": ["Insufficient data variation for scaling analysis"]
            })
    else:
        scaling_summary = pd.DataFrame({
            "parameter": ["scaling_exponent"],
            "value": [np.nan],
            "description": ["No successful experiments for scaling analysis"]
        })

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_data": perf_df[["problem_type", "n_qubits", "runtime_s", "peak_mem_mb", "success"]],
            "scaling_analysis": scaling_summary
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

    # ------------********------------
   # ------------********------------ Reliability Results ----------------

    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    reliability_analysis = perf_df.groupby(["problem_type", "n_qubits"]).agg({
        "objective_value": ["mean", "std", "min", "max"],
        "runtime_s": ["mean", "std"],
        "success": "mean",
        "gamma_candidates": "mean",
        "beta_candidates": "mean"
    }).reset_index()
    reliability_analysis.columns = ['_'.join(col).strip('_') for col in reliability_analysis.columns.values]

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "reliability_analysis": reliability_analysis,
            "raw_results": perf_df[["problem_type", "n_qubits", "seed", "objective_value", "actual_cost", "success"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)
    
    # ------------********------------
    # ------------********------------Carbon Footprint Results ----------------

    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")

        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["problem_type", "n_qubits", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    success_count = perf_df["success"].sum()
    total_count = len(perf_df)
    success_rate = success_count / total_count if total_count > 0 else 0

    print("\n" + "="*60)
    print("üéâ QAOA EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {total_count}")
    print(f"‚úÖ Successful:      {success_count}")
    print(f"‚ùå Failed:          {total_count - success_count}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¢ Qubits tested:   {sorted(perf_df['n_qubits'].unique())}")
    print(f"üéØ Success rate:    {success_rate * 100:.1f}%")

    if success_count > 0:
        success_df = perf_df[perf_df["success"]]
        print(f"üìè Mean objective:  {success_df['objective_value'].mean():.3f}")
        print(f"‚è±Ô∏è  Mean runtime:    {success_df['runtime_s'].mean():.6f}s")
        print(f"üßÆ Param search:    {success_df['gamma_candidates'].iloc[0]}√ó{success_df['beta_candidates'].iloc[0]} grid")
    else:
        print(f"üìè Mean objective:  N/A (no successful runs)")
        print(f"‚è±Ô∏è  Mean runtime:    N/A (no successful runs)")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ============================================================
# ------------********------------   Main execution for Kaggle
# ============================================================
if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting QAOA Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_qaoa_experiment()

    if results_df is not None:

        # Display final results summary
        success_count = results_df["success"].sum()
        total_count = len(results_df)

        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {total_count}")
        print(f"Successful experiments: {success_count}")
        print(f"Failed experiments: {total_count - success_count}")
        print(f"Qubits tested: {sorted(results_df['n_qubits'].unique())}")
        print(f"Problem types: {sorted(results_df['problem_type'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {success_count / total_count * 100:.1f}%")

        if success_count > 0:
            success_df = results_df[results_df["success"]]
            print(f"Average objective: {success_df['objective_value'].mean():.3f}")
            print(f"Average actual cost: {success_df['actual_cost'].mean():.3f}")
        else:
            print(f"Average objective: N/A")
            print(f"Average actual cost: N/A")

        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):

            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\n‚úÖ All QAOA deliverables generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå QAOA benchmark failed - no results generated")

