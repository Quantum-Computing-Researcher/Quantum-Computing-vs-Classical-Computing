#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------

# quantum_pendulum_tomography_ibm_hardware_light_notebook.py
#
# Quantum State Tomography for Pendulum / single bosonic mode
#   - Ideal (noiseless) homodyne tomography (same as quantum_pendulum_tomography_noiseless.py)
#   - Plus a LIGHT IBM hardware component:
#       * For each steps value S (number of homodyne angles),
#         we run a tiny 1-qubit "quadrature-like" circuit on IBM hardware
#         at a subset of angles.
#       * We record hardware_runtime_sec and basic circuit metrics.
#       * We add hardware_runtime_sec into one performance row for that S.
#   - Carbon is computed from total runtime (ideal + hardware) with power=65 W, PUE=1.2
#
# Entry point in notebook:
#   results_tomo_ibm = run_pendulum_tomography_ibm_hardware_notebook(...)
#   results_tomo_ibm["perf_df"].head()
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------


# ------------********------------ Imports

import os
import time
import warnings
import tracemalloc
from typing import Dict, List, Tuple, Union

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2
from qiskit import QuantumCircuit
from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager

warnings.filterwarnings("ignore", category=UserWarning)

# ------------********------------
# ------------********------------ Shared helpers 
# ------------********------------

#This helper ensures that a target directory exists before writing any outputs.
#It creates the directory if needed and safely ignores the case where it already exists.

def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

#This function writes multiple pandas DataFrames into a single Excel workbook.
#If Excel export fails, it gracefully falls back to writing each table as a CSV file.

def write_excel(tables: Dict[str, pd.DataFrame], path: str) -> None:
    try:
        with pd.ExcelWriter(path, engine="openpyxl") as w:
            for k, df in tables.items():
                sheet_name = (k[:31] or "Sheet1")
                df.to_excel(w, index=False, sheet_name=sheet_name)
        print(f"[OK] Wrote Excel: {path}")
    except Exception as e:
        print(f"[WARN] Excel write failed ({e}); CSV fallback.")
        base = os.path.splitext(path)[0]
        for k, df in tables.items():
            csvp = f"{base}__{k}.csv"
            df.to_csv(csvp, index=False)
            print(f"[OK] {csvp}")

#This utility saves a Matplotlib figure with consistent layout and resolution.
#It closes the figure after saving to free resources and reports the output path.
def save_fig(fig: plt.Figure, path: str, dpi: int = 150) -> None:
    fig.tight_layout()
    fig.savefig(path, dpi=dpi, bbox_inches="tight")
    plt.close(fig)
    print(f"[OK] Saved plot: {path}")

#This function resolves a file path by checking user input and Desktop locations.
#It supports quoted paths, missing extensions, and a list of default fallback filenames.

def resolve_desktop_file(arg: Union[str, None], defaults: List[str]) -> Union[str, None]:
    desk = os.path.join(os.environ.get("USERPROFILE", ""), "Desktop")
    def exists(x: str) -> bool:
        return os.path.isfile(x)

    if arg:
        base = arg.strip().strip('"').strip("'")
        # 1) direct path
        if exists(base):
            return os.path.abspath(base)
        # 2) Desktop\base
        cand = os.path.join(desk, base)
        if exists(cand):
            return os.path.abspath(cand)
        # 3) try with extensions if no extension given
        root, ext = os.path.splitext(base)
        if not ext:
            for e in (".xlsx", ".xls", ".csv"):
                cand = os.path.join(desk, root + e)
                if exists(cand):
                    return os.path.abspath(cand)
        # 4) case-insensitive name match on Desktop
        tail = os.path.split(base)[1].lower()
        if os.path.isdir(desk):
            for fn in os.listdir(desk):
                if fn.lower() == tail:
                    cand = os.path.join(desk, fn)
                    if exists(cand):
                        return os.path.abspath(cand)
    # fallback: try defaults
    for c in defaults:
        if exists(c):
            return os.path.abspath(c)
    return None

# ------------********------------
# ------------********------------ Carbon helpers
# ------------********------------

def load_country_co2(path: str, year_select: str = "latest") -> pd.DataFrame:
    if path.lower().endswith(".csv"):
        df = pd.read_csv(path)
    else:
        df = pd.read_excel(path)
    df.columns = [str(c).strip().lower() for c in df.columns]
    cname = next((c for c in df.columns if "country" in c or c in ("name", "nation")), None)
    if not cname:
        raise ValueError("No country column in CO2 file.")
    yname = next((c for c in df.columns if "year" in c), None)

    cand = [c for c in df.columns if "intensity" in c] or \
           [c for c in df.columns if ("co2" in c and "kwh" in c)] or \
           [c for c in df.columns if (("kg" in c or "g" in c) and "kwh" in c)]
    if not cand:
        raise ValueError("No intensity column in CO2 file.")
    icol = cand[0]

    use = [cname, icol] + ([yname] if yname else [])
    out = df[use].dropna().copy()
    out.columns = ["country", "intensity_raw"] + (["year"] if yname else [])
    out["intensity_raw"] = pd.to_numeric(out["intensity_raw"], errors="coerce")
    out = out.dropna(subset=["intensity_raw"])

    if np.nanmedian(out["intensity_raw"]) > 10.0:
        out["kg_per_kwh"] = out["intensity_raw"] / 1000.0
    else:
        out["kg_per_kwh"] = out["intensity_raw"]

    out["country"] = out["country"].astype(str).str.strip().str.title()
    if yname and year_select == "latest":
        out = out.sort_values(["country", "year"]).groupby("country", as_index=False).tail(1)

    cols = ["country", "kg_per_kwh"] + (["year"] if yname else [])
    return out[cols].drop_duplicates()

def compute_emissions(perf_df: pd.DataFrame,
                      co2_df: pd.DataFrame,
                      power_watts: float,
                      pue: float) -> pd.DataFrame:
    perf = perf_df.copy()
    perf["energy_kwh"] = (perf["runtime_sec"] * power_watts) / 3.6e6
    perf["energy_kwh"] *= pue
    co2 = co2_df.rename(columns={"kg_per_kwh": "kgco2_per_kwh"}).copy()
    perf["_k"] = 1
    co2["_k"] = 1
    joined = perf.merge(co2, on="_k").drop(columns="_k")
    joined["emissions_kgCO2"] = joined["energy_kwh"] * joined["kgco2_per_kwh"]
    return joined

def worstcase_country_table(perf_df: pd.DataFrame,
                            co2_df: pd.DataFrame,
                            power_watts: float,
                            pue: float,
                            step_key: str = "steps",
                            dim_key: str = "grid_dim"):
    heavy_dim = int(perf_df[dim_key].max())
    heavy_S = int(perf_df[step_key].max())
    rt = float(perf_df[(perf_df[dim_key] == heavy_dim) &
                       (perf_df[step_key] == heavy_S)]["runtime_sec"].mean())
    kwh = rt * power_watts / 3.6e6 * pue
    tbl = co2_df.copy()
    if "year" not in tbl.columns:
        tbl["year"] = pd.NA
    out = pd.DataFrame({
        "Country": tbl["country"],
        "Year": tbl["year"],
        "Intensity": tbl["kg_per_kwh"],
        "kWh": kwh,
    })
    out["kgCO2e"] = out["Intensity"] * out["kWh"]
    out = out.sort_values("kgCO2e", ascending=False).reset_index(drop=True)
    meta = {
        "grid_dim": heavy_dim,
        "steps": heavy_S,
        "mean_runtime_s": rt,
        "kWh_used": kwh,
    }
    return out, meta

# ------------********------------
# ------------********------------ Tomography core
# ------------********------------
#This helper generates S evenly spaced angles in the interval [0, π).
#These angles are typically used as quadrature measurement directions.

def make_angles(S: int) -> np.ndarray:
    return np.linspace(0.0, np.pi, S, endpoint=False)


#This function computes the mean and variance of a quadrature for a squeezed Gaussian state.
#It accounts for displacement, squeezing strength, and squeezing angle.

# units: vacuum Var(X) = 0.5
def quad_mean_var(theta: float, x0: float, p0: float, r: float, phi: float):
    mu = x0 * np.cos(theta) + p0 * np.sin(theta)
    var = 0.5 * (
        np.exp(-2 * r) * np.cos(theta - phi) ** 2
        + np.exp(2 * r) * np.sin(theta - phi) ** 2
    )
    return mu, var

#This function draws random quadrature samples from a Gaussian distribution.
#The distribution parameters are determined by the quadrature angle and squeezing properties.

def sample_quadrature(theta: float, shots: int,
                      x0: float, p0: float,
                      r: float, phi: float,
                      rng: np.random.Generator) -> np.ndarray:
    mu, var = quad_mean_var(theta, x0, p0, r, phi)
    return rng.normal(loc=mu, scale=np.sqrt(var), size=shots)

#This function computes the analytic Wigner function of a displaced squeezed Gaussian state.
#It evaluates the distribution on a phase-space grid and enforces proper normalization.
def wigner_truth(grid_x: np.ndarray, grid_p: np.ndarray,
                 x0: float, p0: float, r: float, phi: float) -> np.ndarray:
    X, P = np.meshgrid(grid_x, grid_p, indexing="xy")
    c, s = np.cos(phi), np.sin(phi)
    Xp = c * X + s * P
    Pp = -s * X + c * P
    sigx2 = 0.5 * np.exp(-2 * r)
    sigp2 = 0.5 * np.exp(2 * r)
    X0 = c * x0 + s * p0
    P0 = -s * x0 + c * p0
    expo = -((Xp - X0) ** 2 / sigx2 + (Pp - P0) ** 2 / sigp2)
    W = (1.0 / np.pi) * np.exp(expo)
    dx = grid_x[1] - grid_x[0]
    dp = grid_p[1] - grid_p[0]
    sarea = np.sum(W) * dx * dp
    if sarea > 0:
        W /= sarea
    return W


#This helper applies a ramp filter in the frequency domain to a 1D density.
#It is commonly used in filtered backprojection for tomographic reconstruction.

def ramp_filter_1d(dens: np.ndarray, dx: float) -> np.ndarray:
    n = len(dens)
    F = np.fft.rfft(dens)
    freqs = np.fft.rfftfreq(n, d=dx)
    Ff = F * np.abs(freqs)
    back = np.fft.irfft(Ff, n=n)
    return back

#This function reconstructs a 2D phase-space distribution using filtered backprojection.
#It interpolates projected densities along rotated axes and normalizes the result.
def backproject(angles: np.ndarray,
                proj_dens_list: List[np.ndarray],
                t_centers: np.ndarray,
                grid_x: np.ndarray,
                grid_p: np.ndarray,
                use_filter: bool = True) -> np.ndarray:
    Gx, Gp = len(grid_x), len(grid_p)
    X, P = np.meshgrid(grid_x, grid_p, indexing="xy")
    recon = np.zeros((Gp, Gx), dtype=float)
    dt = t_centers[1] - t_centers[0]
    dtheta = (np.pi / len(angles)) if len(angles) > 0 else 0.0
    for th, dens in zip(angles, proj_dens_list):
        d = ramp_filter_1d(dens, dt) if use_filter else dens
        T = X * np.cos(th) + P * np.sin(th)
        vals = np.interp(T.ravel(), t_centers, d, left=0.0, right=0.0).reshape(T.shape)
        recon += vals * dtheta
    dx = grid_x[1] - grid_x[0]
    dp = grid_p[1] - grid_p[0]
    s = np.sum(recon) * dx * dp
    if s > 0:
        recon /= s
    return recon

#This routine runs a complete tomography experiment for a single parameter setting.
#It samples quadratures, reconstructs the Wigner function, and compares it to the analytic truth.
#Error metrics such as MSE and L1 distance are returned along with reconstruction data.

def run_once_tomography(
    S: int,
    shots: int,
    G: int,
    x0: float,
    p0: float,
    r: float,
    phi: float,
    rng: np.random.Generator,
    lim: float,
) -> dict:
    angles = make_angles(S)
    tmax = lim * np.max(np.abs(np.cos(angles))) + lim * np.max(np.abs(np.sin(angles)))
    B = max(256, 2 * G)
    t_edges = np.linspace(-tmax, tmax, B + 1)
    t_centers = 0.5 * (t_edges[:-1] + t_edges[1:])

    proj = []
    for th in angles:
        x = sample_quadrature(th, shots, x0, p0, r, phi, rng)
        hist, _ = np.histogram(x, bins=t_edges)
        dens = hist / (shots * (t_edges[1] - t_edges[0]))
        proj.append(dens)

    gx = np.linspace(-lim, lim, G)
    gp = np.linspace(-lim, lim, G)
    recon = backproject(angles, proj, t_centers, gx, gp, use_filter=True)
    truth = wigner_truth(gx, gp, x0, p0, r, phi)
    mse = float(np.mean((recon - truth) ** 2))
    l1 = float(np.sum(np.abs(recon - truth)) * (gx[1] - gx[0]) * (gp[1] - gp[0]))
    return dict(recon=recon, truth=truth, gx=gx, gp=gp,
                mse=mse, l1=l1, S=S, shots=shots, grid=G)

# =========================
# Ideal benchmark driver
# =========================

def run_tomography_benchmarks(
    S_list: List[int],
    repeats: int,
    shots_per_angle: int,
    grid: int,
    x0: float,
    p0: float,
    r: float,
    phi: float,
    lim: float,
    seed: int,
):
    rng = np.random.default_rng(seed)
    rows_perf = []
    rows_rel = []

    for S in sorted(S_list):
        for rep in range(repeats):
            tracemalloc.start()
            t0 = time.perf_counter()
            out = run_once_tomography(S, shots_per_angle, grid, x0, p0, r, phi, rng, lim)
            runtime = time.perf_counter() - t0
            _, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            if rep == 0:
                # Wigner plots (truth, recon, difference)
                fig, ax = plt.subplots(1, 3, figsize=(13, 4))
                im0 = ax[0].imshow(out["truth"],
                                   extent=[-lim, lim, -lim, lim],
                                   origin="lower")
                ax[0].set_title("Truth Wigner")
                im1 = ax[1].imshow(out["recon"],
                                   extent=[-lim, lim, -lim, lim],
                                   origin="lower")
                ax[1].set_title(f"Reconstruction (S={S})")
                im2 = ax[2].imshow(out["recon"] - out["truth"],
                                   extent=[-lim, lim, -lim, lim],
                                   origin="lower")
                ax[2].set_title("Difference")
                for a in ax:
                    a.set_xlabel("x")
                    a.set_ylabel("p")
                for im, a in zip([im0, im1, im2], ax):
                    plt.colorbar(im, ax=a)
                # save path is handled by caller (reliability dir)
                yield "wigner_plot", S, fig

            rows_perf.append({
                "grid_dim": grid,
                "steps": S,
                "repeat": rep,
                "runtime_sec": runtime,
                "peak_mem_mb": peak / (1024 ** 2),
            })
            rows_rel.append({
                "grid_dim": grid,
                "steps": S,
                "repeat": rep,
                "mse": out["mse"],
                "l1_distance": out["l1"],
            })

    perf_df = pd.DataFrame(rows_perf)
    rel_df = pd.DataFrame(rows_rel)
    yield "dataframes", perf_df, rel_df

# =========================
# IBM hardware tomography (LIGHT)
# =========================

def _parse_steps_arg(steps_arg) -> List[int]:
    if isinstance(steps_arg, str):
        parts = [p.strip() for p in steps_arg.replace(",", " ").split() if p.strip()]
        return sorted({int(p) for p in parts})
    # assume iterable
    return sorted({int(s) for s in steps_arg})

def build_quadrature_circuits_ibm(
    angles: np.ndarray,
    x0: float,
    p0: float,
    r: float,
    phi: float,
) -> List[QuantumCircuit]:
    """
    Super-light 1-qubit circuits to mimic 'quadrature-like'
    measurements at different angles.

    We approximate quadrature displacement by an RX/RY angle
    derived from the coherent displacement mean μ(θ).
    This is purely for benchmarking (depth, runtime), not for exact physics.
    """
    circuits = []
    for theta in angles:
        mu, _ = quad_mean_var(theta, x0, p0, r, phi)
        # Map mu ∈ R to a bounded rotation angle:
        alpha = np.clip(mu, -1.0, 1.0) * (np.pi / 2.0)
        qc = QuantumCircuit(1, 1)
        qc.ry(2.0 * alpha, 0)
        qc.rz(theta, 0)
        qc.measure(0, 0)
        circuits.append(qc)
    return circuits

def get_ibm_sampler(backend_name: str, shots: int):
    """
    Initialize QiskitRuntimeService, backend and SamplerV2 + pass manager.
    Assumes IBM Quantum account is already saved.
    """
    print("[ibm] Initializing QiskitRuntimeService...")
    service = QiskitRuntimeService()
    backend = service.backend(backend_name)
    print(f"[ibm] Using backend: {backend.name}")
    print(f"[ibm]   num_qubits={backend.num_qubits}, simulator={getattr(backend, 'simulator', False)}")
    print(f"[ibm]   shots per circuit: {shots}")
    pm = generate_preset_pass_manager(backend=backend, optimization_level=1)
    sampler = SamplerV2(mode=backend)
    return backend, sampler, pm

def hardware_tomography_for_S(
    S: int,
    x0: float,
    p0: float,
    r: float,
    phi: float,
    backend,
    sampler: SamplerV2,
    pass_manager,
    hw_shots_per_angle: int,
    hw_angles_max: int,
) -> dict:
    """
    Run a LIGHT tomography job for a given S on IBM hardware:

      - Choose up to hw_angles_max angles from {0, π/S, ..., (S-1)π/S}.
      - Build 1-qubit circuits for those angles.
      - Transpile via pass_manager, run with SamplerV2, record:
          * hardware_runtime_sec
          * average / max depth
          * total counts
    """
    if backend.num_qubits < 1:
        print("[ibm] Backend has no qubits? Skipping hardware.")
        return {
            "hardware_runtime_sec": 0.0,
            "hardware_depth_mean": 0.0,
            "hardware_depth_max": 0,
            "hardware_angles_used": 0,
            "hardware_shots": int(hw_shots_per_angle),
            "hardware_counts_total": 0,
        }

    angles_all = make_angles(S)
    k = min(len(angles_all), int(hw_angles_max))
    if k <= 0:
        return {
            "hardware_runtime_sec": 0.0,
            "hardware_depth_mean": 0.0,
            "hardware_depth_max": 0,
            "hardware_angles_used": 0,
            "hardware_shots": int(hw_shots_per_angle),
            "hardware_counts_total": 0,
        }

    idx = np.linspace(0, len(angles_all) - 1, num=k, dtype=int)
    angles_hw = angles_all[idx]

    logical_circs = build_quadrature_circuits_ibm(angles_hw, x0, p0, r, phi)
    isa_circs = pass_manager.run(logical_circs)
    depths = [c.depth() for c in isa_circs]

    print(f"[ibm-tomo] Submitting {len(isa_circs)} circuits for S={S} "
          f"(angles_used={len(angles_hw)}, shots={hw_shots_per_angle}) ...")

    t0_hw = time.perf_counter()
    job = sampler.run(isa_circs, shots=int(hw_shots_per_angle))
    primitive_result = job.result()
    hw_runtime = time.perf_counter() - t0_hw
    print(f"[ibm-tomo] Hardware job for S={S} finished in {hw_runtime:.3f} s.")

    total_counts = 0
    for res in primitive_result:
        joined = res.join_data()
        counts = joined.get_counts()
        total_counts += sum(counts.values())

    return {
        "hardware_runtime_sec": float(hw_runtime),
        "hardware_depth_mean": float(np.mean(depths) if depths else 0.0),
        "hardware_depth_max": int(max(depths) if depths else 0),
        "hardware_angles_used": int(len(angles_hw)),
        "hardware_shots": int(hw_shots_per_angle),
        "hardware_counts_total": int(total_counts),
    }

# =========================
# Notebook entry point
# =========================

def run_pendulum_tomography_ibm_hardware_notebook(
    # CLI-compatible arguments
    n_min: int = 2,    # placeholders (not used directly, kept for compatibility)
    n_max: int = 6,
    steps: Union[str, Tuple[int, ...]] = "8,16,32",
    repeats: int = 2,
    shots_per_angle: int = 1500,
    grid: int = 64,
    seed: int = 7,
    state: str = "coherent",
    x0: float = 0.8,
    p0: float = -0.5,
    squeeze_r: float = 0.0,
    squeeze_phi: float = 0.0,
    lim: float = 3.0,
    # carbon / IO
    excel_filename: str = "Filtered CO2 intensity 236 Countries.xlsx",
    device_power_watts: float = 65.0,
    pue: float = 1.2,
    year_select: str = "latest",
    outdir: Union[str, None] = None,
    # IBM hardware params
    backend_name: str = "ibm_torino",
    hw_shots_per_angle: int = 256,
    hw_angles_max: int = 8,
):
    """
    Quantum State Tomography for Pendulum (homodyne-style) with IBM hardware in the loop.

    - Ideal tomography (same physics as quantum_pendulum_tomography_noiseless.py)
    - IBM hardware used LIGHTLY:
        * For each steps value S, run a set of tiny 1-qubit circuits
          for up to hw_angles_max angles.
        * Add the measured hardware runtime to one performance run for that S.
    - Four benchmark folders created under outdir:
        Performance, Scalability, Reliability, Carbon.
    - Carbon uses device_power_watts (default 65.0) and PUE (default 1.2),
      combining ideal + hardware runtime.
    """
    # parse steps -> S_list
    S_list = _parse_steps_arg(steps)
    if not S_list:
        raise ValueError("steps must contain at least one positive integer.")

    print("[tomo-ibm-light] Quantum Pendulum Tomography (IBM hardware, light)")
    print("  steps S list:", S_list)
    print(f"  repeats={repeats}, shots_per_angle={shots_per_angle}, grid={grid}")
    print(f"  state={state}, x0={x0}, p0={p0}, squeeze_r={squeeze_r}, squeeze_phi={squeeze_phi}, lim={lim}")
    print(f"  backend={backend_name}, hw_shots_per_angle={hw_shots_per_angle}, hw_angles_max={hw_angles_max}")
    print(f"  excel_filename={excel_filename}")
    print(f"  device_power_watts={device_power_watts}, pue={pue}, year_select={year_select}")

    # choose squeezing parameters
    if state == "squeezed":
        r = squeeze_r
        phi = squeeze_phi
    else:
        r = 0.0
        phi = 0.0

    # outdir
    if outdir is None:
        root = os.getcwd()
    else:
        root = os.path.abspath(os.path.expandvars(outdir))

    perf_dir = os.path.join(root, "Performance")
    scal_dir = os.path.join(root, "Scalability")
    reli_dir = os.path.join(root, "Reliability")
    carb_dir = os.path.join(root, "Carbon")
    for d in (perf_dir, scal_dir, reli_dir, carb_dir):
        ensure_dir(d)

    print("[INFO] Output root:", root)
    print("       - Performance ->", perf_dir)
    print("       - Scalability ->", scal_dir)
    print("       - Reliability ->", reli_dir)
    print("       - Carbon      ->", carb_dir)

    # ---------------
    # Ideal benchmarks
    # ---------------
    print("[INFO] Running ideal (noiseless) tomography benchmarks...")
    t0_all = time.perf_counter()
    perf_df = None
    rel_df = None

    # run_tomography_benchmarks yields wigner plots and then final dataframes
    for tag, *payload in run_tomography_benchmarks(
        S_list=S_list,
        repeats=repeats,
        shots_per_angle=shots_per_angle,
        grid=grid,
        x0=x0,
        p0=p0,
        r=r,
        phi=phi,
        lim=lim,
        seed=seed,
    ):
        if tag == "wigner_plot":
            S_val, fig = payload
            save_fig(fig, os.path.join(reli_dir, f"wigner_maps_S{S_val}.png"))
        elif tag == "dataframes":
            perf_df, rel_df = payload

    if perf_df is None or rel_df is None:
        raise RuntimeError("Benchmark driver did not produce dataframes.")

    print(f"[INFO] Ideal benchmarks completed in {time.perf_counter() - t0_all:.2f} s")

    # ---------------
    # IBM hardware runs (light)
    # ---------------
    backend, sampler, pm = get_ibm_sampler(backend_name, hw_shots_per_angle)
    hw_metrics_by_S: Dict[int, dict] = {}

    for S in S_list:
        print(f"\n[ibm-tomo] Running hardware tomography for S={S}")
        hw_metrics = hardware_tomography_for_S(
            S=S,
            x0=x0,
            p0=p0,
            r=r,
            phi=phi,
            backend=backend,
            sampler=sampler,
            pass_manager=pm,
            hw_shots_per_angle=hw_shots_per_angle,
            hw_angles_max=hw_angles_max,
        )
        hw_metrics_by_S[S] = hw_metrics

    # merge hardware metrics into perf_df
    perf_df_ibm = perf_df.copy()
    perf_df_ibm["hardware_runtime_sec"] = 0.0
    perf_df_ibm["hardware_depth_mean"] = 0.0
    perf_df_ibm["hardware_depth_max"] = 0
    perf_df_ibm["hardware_angles_used"] = 0
    perf_df_ibm["hardware_shots"] = int(hw_shots_per_angle)
    perf_df_ibm["hardware_counts_total"] = 0

    for S in S_list:
        hw = hw_metrics_by_S.get(S, None)
        if hw is None:
            continue
        mask_S = (perf_df_ibm["steps"] == S)
        idx_S = perf_df_ibm[mask_S].sort_values("repeat").index.tolist()
        if not idx_S:
            continue
        first_idx = idx_S[0]

        perf_df_ibm.loc[first_idx, "hardware_runtime_sec"] = hw["hardware_runtime_sec"]
        perf_df_ibm.loc[first_idx, "hardware_depth_mean"] = hw["hardware_depth_mean"]
        perf_df_ibm.loc[first_idx, "hardware_depth_max"] = hw["hardware_depth_max"]
        perf_df_ibm.loc[first_idx, "hardware_angles_used"] = hw["hardware_angles_used"]
        perf_df_ibm.loc[first_idx, "hardware_shots"] = hw["hardware_shots"]
        perf_df_ibm.loc[first_idx, "hardware_counts_total"] = hw["hardware_counts_total"]
        # add hardware runtime into total runtime_sec for this run
        perf_df_ibm.loc[first_idx, "runtime_sec"] = (
            perf_df_ibm.loc[first_idx, "runtime_sec"] + hw["hardware_runtime_sec"]
        )

    # ---------------
    # PERFORMANCE
    # ---------------
    perf_sum = perf_df_ibm.groupby(["grid_dim", "steps"], as_index=False).agg(
        mean_runtime_sec=("runtime_sec", "mean"),
        std_runtime_sec=("runtime_sec", "std"),
        mean_hw_runtime_sec=("hardware_runtime_sec", "mean"),
        mean_peak_mem_mb=("peak_mem_mb", "mean"),
        std_peak_mem_mb=("peak_mem_mb", "std"),
    )

    perf_xlsx = os.path.join(perf_dir, "performance_ibm_hardware.xlsx")
    write_excel(
        {"raw": perf_df_ibm.sort_values(["steps", "repeat"]),
         "summary": perf_sum.sort_values(["grid_dim", "steps"])},
        perf_xlsx,
    )

    # performance plot: runtime vs steps
    fig, ax = plt.subplots(figsize=(7, 5))
    sfit = perf_df_ibm.groupby("steps", as_index=False)["runtime_sec"].mean()
    ax.plot(sfit["steps"], sfit["runtime_sec"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Steps S")
    ax.set_ylabel("Runtime (s)")
    ax.set_title("Scalability: runtime vs S (IBM hardware in loop)")
    save_fig(fig, os.path.join(scal_dir, "runtime_vs_steps_ibm_hardware.png"))

    # ---------------
    # SCALABILITY fit
    # ---------------
    xb = np.log(sfit["steps"].values)
    yb = np.log(sfit["runtime_sec"].values + 1e-12)
    beta, a0 = np.polyfit(xb, yb, 1)
    scal_xlsx = os.path.join(scal_dir, "scalability_ibm_hardware.xlsx")
    write_excel(
        {
            "scaling_fit": pd.DataFrame({
                "fit_parameter": ["beta_step_scaling"],
                "estimate": [beta],
                "note": [f"G={grid} (IBM hardware in total runtime)"],
            })
        },
        scal_xlsx,
    )

    # ---------------
    # RELIABILITY (same as ideal)
    # ---------------
    rel_sum = rel_df.groupby(["grid_dim", "steps"], as_index=False).agg(
        mean_mse=("mse", "mean"),
        std_mse=("mse", "std"),
        mean_l1=("l1_distance", "mean"),
        std_l1=("l1_distance", "std"),
    )

    reli_xlsx = os.path.join(reli_dir, "reliability_ibm_hardware.xlsx")
    write_excel(
        {"raw": rel_df.sort_values(["steps", "repeat"]),
         "summary": rel_sum.sort_values(["grid_dim", "steps"])},
        reli_xlsx,
    )

    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(rel_sum["steps"], rel_sum["mean_mse"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Steps S")
    ax.set_ylabel("Mean MSE (log scale)")
    ax.set_title("Reliability: Wigner MSE vs S (ideal)")
    save_fig(fig, os.path.join(reli_dir, "mse_vs_steps_ibm_hardware.png"))

    # ---------------
    # CARBON (performance runtime only, including hardware)
    # ---------------
    desktop = os.path.join(os.environ.get("USERPROFILE", ""), "Desktop")
    defaults = [
        os.path.join(desktop, "Filtered CO2 intensity 236 Countries.xlsx"),
        os.path.join(desktop, "Filtered CO2 intensity 236 Countries.csv"),
    ]
    co2_path = resolve_desktop_file(excel_filename, defaults)
    carbon_df = None
    carbon_meta_df = None

    if co2_path and os.path.isfile(co2_path):
        co2_df = load_country_co2(co2_path, year_select=year_select)
        carbon_df = compute_emissions(
            perf_df_ibm,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        wc_table, meta = worstcase_country_table(
            perf_df_ibm,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        carbon_meta_df = pd.DataFrame([meta])
        carb_xlsx = os.path.join(carb_dir, "carbon_footprint_ibm_hardware.xlsx")
        write_excel(
            {
                "emissions_all": carbon_df.sort_values(["steps"]),
                "worst_case_table": wc_table,
                "meta": carbon_meta_df,
            },
            carb_xlsx,
        )
        fig, ax = plt.subplots(figsize=(7, 5))
        ax.hist(wc_table["kgCO2e"], bins=30)
        ax.set_xlabel("kg CO2e")
        ax.set_title("Carbon distribution (worst-case, IBM hardware)")
        save_fig(fig, os.path.join(carb_dir, "carbon_distribution_ibm_hardware.png"))
    else:
        carb_xlsx = os.path.join(carb_dir, "carbon_placeholder_ibm_hardware.xlsx")
        write_excel(
            {"info": pd.DataFrame([{
                "note": "CO2 file not found; carbon footprint skipped.",
                "hint": "Place 'Filtered CO2 intensity 236 Countries.xlsx' on Desktop or pass full path.",
            }])},
            carb_xlsx,
        )

    print("\n[DONE] Quantum pendulum tomography (IBM hardware, light) complete.")
    print(f"Performance Excel:  {perf_xlsx}")
    print(f"Scalability Excel:  {scal_xlsx}")
    print(f"Reliability Excel:  {reli_xlsx}")
    print(f"Carbon Excel:       {os.path.join(carb_dir, 'carbon_footprint_ibm_hardware.xlsx')}")
    print("Folders under:", root)
    print("  - Performance")
    print("  - Scalability")
    print("  - Reliability")
    print("  - Carbon")

    return {
        "perf_df": perf_df_ibm,
        "rel_df": rel_df,
        "perf_dir": perf_dir,
        "scal_dir": scal_dir,
        "reli_dir": reli_dir,
        "carb_dir": carb_dir,
        "carbon_df": carbon_df,
        "carbon_summary": carbon_meta_df,
    }
