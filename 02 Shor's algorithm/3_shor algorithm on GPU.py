#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ----------------------------------------------------------------------------------------------------------------
# GPU-accelerated Empirical Evaluation: Shor's Algorithm (Noiseless) ‚Äì Quantum Baseline
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

"""2 Shor
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/2-shor.87e92f73-b338-4f52-a9f8-6df0b2c33312.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251121/auto/storage/goog4_request%26X-Goog-Date%3D20251121T142919Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e0f7b38adc978c4141f7114b99d652fbef75c5e2e98b6f8c620293c4abd071c2134db3430c0f43ba136547f82b0a02ab09d3c636a3c29469a06cc0a2308e4de9d71bb9f78a6aef3dc75924431d8ba9c0b1e92559f140f55a71fd789866a7769740c708d11ae96a23705292597df998a9897cc2202324305d90e78301740024bf6061e25cc9c352bf228046e33ead0250e32d2eef142d93e0e3c39e441ce387bcc6a2bd997c7d8efd24142277d76e7a25a1371af1d0e461855b0d9e36fd268fe2144321c2172895edb6d1ddd8dac263ce7e581e41ef299006bef722bd7d3fdb4f81b6447ee9a2919e909312d71a09e1913bf2b317e76cd9f0df3594890814df8
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()
# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
#  THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

i222070tabidahusmani_masters_path = kagglehub.dataset_download('i222070tabidahusmani/masters')
print('Data source import complete.')

# ------------********------------ Imports
#These imports provide core utilities for filesystem access, timing, memory profiling, warnings, and mathematical operations.
#They include dataclass support and rich typing annotations to structure experiment configurations and results.
#Threading and ThreadPoolExecutor enable concurrent execution with thread-safe coordination of parallel tasks.

import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================
# GPU Setup and Dependencies
# ============================================================
print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil", "scikit-learn"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

def to_numpy(x):
    """Safely convert CuPy array to NumPy array"""
    if HAS_GPU and isinstance(x, cp.ndarray):
        return cp.asnumpy(x)
    return x

def to_cupy(x):
    """Safely convert NumPy array to CuPy array"""
    if HAS_GPU and isinstance(x, np.ndarray):
        return cp.asarray(x)
    return x

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    N, seed = args
    with _gpu_lock:  # Ensure only one thread uses GPU at a time
        return _run_single_experiment(N, seed)

# ------------********------------
# ========================= Shor's Algorithm Core =========================
# ------------********------------

# Compute greatest common divisor using Euclidean algorithm
def gcd(a: int, b: int) -> int:
    
    while b != 0:
        a, b = b, a % b
    return a

def is_prime(n: int) -> bool:
    """Check if a number is prime"""
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False
    for i in range(3, int(math.sqrt(n)) + 1, 2):
        if n % i == 0:
            return False
    return True

def find_coprime(N: int, rng) -> int:
    """Find a number coprime to N"""
    for _ in range(100):  # Try up to 100 random numbers
        a = rng.randint(2, N - 1)
        if gcd(a, N) == 1:
            return a
    return 2  # Fallback to 2 if no coprime found quickly

def quantum_order_finding_simulation(a: int, N: int, shots: int = 1024) -> int:
    """
    # Simulate quantum order finding using GPU-accelerated computation.
    # This is a simplified simulation of the quantum period finding subroutine.
    """
    # For small numbers, we can compute the order classically
    # In a real quantum computer, this would use QFT and modular exponentiation

    order = 1
    current = a % N
    while current != 1 and order < N:
        current = (current * a) % N
        order += 1
    return order if current == 1 else 0

def shors_algorithm_factorization(N: int, shots: int = 1024, rng=None) -> Tuple[bool, List[int], str]:
   
   # Simplified GPU-accelerated Shor's algorithm implementation.
   # Uses classical simulation of quantum components.
   
    if rng is None:
        rng = np.random.RandomState()

    # Check for trivial cases
    if N < 2:
        return False, [], "N must be >= 2"
    if N == 2:
        return True, [2], "prime"
    if is_prime(N):
        return True, [N], "prime"

    # Check for even number
    if N % 2 == 0:
        return True, [2, N // 2], "even"

    # Try to find factors using simplified Shor's approach
    for attempt in range(10):  # Try up to 10 random bases
        a = find_coprime(N, rng)

        # Check if we get lucky with GCD
        g = gcd(a, N)
        if 1 < g < N:
            return True, sorted([g, N // g]), f"gcd_lucky (a={a})"

        # Simulate quantum order finding
        r = quantum_order_finding_simulation(a, N, shots)

        if r > 0 and r % 2 == 0:
            # Potential factor found
            x = pow(a, r // 2, N)
            if x != 1 and x != N - 1:
                p = gcd(x - 1, N)
                q = gcd(x + 1, N)
                if 1 < p < N and 1 < q < N and p * q == N:
                    return True, sorted([p, q]), f"shor_success (a={a}, r={r})"

    return False, [], "max_attempts_reached"

def _run_single_experiment(N: int, seed: int) -> dict:
    """
    Core experiment logic - run single Shor's Algorithm experiment.
    """
    # Fixed parameters
    shots = 1024  # Quantum measurement shots

    # Set random seed
    np.random.seed(seed)
    if HAS_GPU:
        cp.random.seed(seed)

    tracemalloc.start()
    t0 = time.perf_counter()

    try:
        # Run Shor's algorithm simulation
        success, factors, reason = shors_algorithm_factorization(N, shots, np.random.RandomState(seed))

        # Compute additional metrics
        if success and len(factors) == 2:
            factor1, factor2 = factors
            correctness = (factor1 * factor2 == N)
            factor_ratio = max(factor1, factor2) / min(factor1, factor2) if min(factor1, factor2) > 0 else float('inf')
        else:
            correctness = False
            factor_ratio = 0.0

        # Compute problem complexity metrics
        bits = N.bit_length()
        digits = len(str(N))

    except Exception as e:
        print(f"‚ùå Experiment failed for N={N}, seed={seed}: {e}")
        success = False
        factors = []
        reason = f"error: {e}"
        correctness = False
        factor_ratio = 0.0
        bits = N.bit_length()
        digits = len(str(N))

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "N": int(N),
        "bits": int(bits),
        "digits": int(digits),
        "seed": int(seed),
        "shots": int(shots),
        "success": bool(success),
        "correctness": bool(correctness),
        "factors": str(factors),
        "factor_ratio": float(factor_ratio),
        "reason": str(reason),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "device": get_device_name()
    }
# ------------********------------
# ============================== Carbon I/O =====
# ------------********------------

#d ef resolve_excel_path resolves a user-specified Excel filename to a valid absolute path, 
#  checking both the given path and a Desktop fallback.
def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

# def load_carbon_excel loads and cleans a carbon-intensity dataset by inferring key columns, 
#  normalizing units to kgCO‚ÇÇ/kWh, and optionally selecting the latest year.
def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)


#def compute_carbon converts total experiment runtime into energy use and per-country CO‚ÇÇ emissions, 
#  returning detailed results and a summary table.
def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------
# ============================== Plot Helpers =========
# ------------********------------

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    # Runtime vs N
    g = df.groupby("N")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g["N"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number to Factor (N)")
    plt.ylabel("Runtime (s)")
    plt.title(f"Performance: Runtime vs N\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_N.png"), **plt_kwargs)
    plt.close()

    # Memory usage vs N
    g2 = df.groupby("N")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["N"], g2["peak_mem_mb"], marker="s", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number to Factor (N)")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Performance: Memory vs N\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_memory_vs_N.png"), **plt_kwargs)
    plt.close()

    # Success rate vs N
    success_rates = df.groupby("N")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(success_rates["N"], success_rates["success"], marker="^", linewidth=2, markersize=8, color='purple')
    plt.xlabel("Number to Factor (N)")
    plt.ylabel("Success Rate")
    plt.title(f"Performance: Success Rate vs N\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_success_rate_vs_N.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_scalability(df: pd.DataFrame, outdir: str):
    # Runtime vs bits (log-log)
    g = df.groupby("bits")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.loglog(g["bits"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number of Bits")
    plt.ylabel("Runtime (s)")
    plt.title(f"Scalability: Runtime vs Bits (log-log)\n{get_device_name()}")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_bits.png"), **plt_kwargs)
    plt.close()

    # Memory vs bits
    g2 = df.groupby("bits")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.semilogy(g2["bits"], g2["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Bits")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Scalability: Memory vs Bits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_bits.png"), **plt_kwargs)
    plt.close()

    # Factor ratio vs N (shows factorization difficulty)
    success_df = df[df["success"]]
    if len(success_df) > 0:
        plt.figure(figsize=(8, 6))
        plt.plot(success_df["N"], success_df["factor_ratio"], marker="s", linewidth=2, markersize=8, color='orange')
        plt.xlabel("Number to Factor (N)")
        plt.ylabel("Factor Ratio (larger/smaller)")
        plt.title(f"Scalability: Factor Ratio vs N\n{get_device_name()}")
        plt.grid(True)
        plt.savefig(os.path.join(outdir, "scal_factor_ratio_vs_N.png"), **plt_kwargs)
        plt.close()

# ------------********------------
def plot_reliability(df: pd.DataFrame, outdir: str):
    # Success rate distribution by N
    success_rates = df.groupby("N")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["N"].astype(str), success_rates["success"])
    plt.xlabel("Number to Factor (N)")
    plt.ylabel("Success Rate")
    plt.title(f"Reliability: Success Rate by N\n{get_device_name()}")
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "rel_success_rate_by_N.png"), **plt_kwargs)
    plt.close()

    # Runtime distribution by N
    data = [df[df["N"] == n]["runtime_s"].values for n in sorted(df["N"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"N={n}" for n in sorted(df["N"].unique())])
    plt.yscale("log")
    plt.xlabel("Number to Factor (N)")
    plt.ylabel("Runtime (s, log scale)")
    plt.title(f"Reliability: Runtime Distribution by N\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_runtime_distribution.png"), **plt_kwargs)
    plt.close()

    # Success heatmap
    pivot_data = df.pivot_table(values="success", index="bits", columns="digits", aggfunc="mean")
    plt.figure(figsize=(10, 6))
    plt.imshow(pivot_data.values, aspect="auto", cmap="viridis", origin="lower")
    plt.colorbar(label="Success Rate")
    plt.xticks(range(len(pivot_data.columns)), pivot_data.columns)
    plt.yticks(range(len(pivot_data.index)), pivot_data.index)
    plt.xlabel("Number of Digits")
    plt.ylabel("Number of Bits")
    plt.title(f"Reliability: Success Rate Heatmap\n{get_device_name()}")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "rel_success_heatmap.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------
# ============================== Utility Functions =====
# ------------********------------

def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    """Save multiple DataFrames to Excel with different sheets"""
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------
# ============================== Main Experiment Runner ==========
# ------------********------------

def run_shors_algorithm_experiment():
    """
    Main Shor's Algorithm experiment runner with hardcoded parameters for Kaggle.
    Equivalent to: --Ns 15,21,33,35,39 --repeats 2
                   --device-power-watts 65 --pue 1.2 --combine
    """
    # Experiment parameters
    N_list = [15, 21, 33, 35, 39]
    trials = 2   # trials per N
    workers = 4   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting Shor's Algorithm experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Numbers to factor: {N_list}")
    print(f"   Trials per number: {trials}")
    print(f"   Quantum shots: 1024")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for N in N_list:
        for i in range(trials):
            seed = 1000 + 17 * N + i
            jobs.append((N, seed))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        status = "‚úÖ" if row["success"] else "‚ùå"
        factors_str = row["factors"] if row["success"] else "failed"
        print(f"  {status} N={row['N']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.6f}s, factors={factors_str})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed N={job[0]}, seed={job[1]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("‚ùå All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1])
                _consume(result)
            except Exception as e:
                print(f"‚ùå Sequential fallback also failed for N={job[0]}, seed={job[1]}: {e}")

    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------
    # ---------------- Performance Results ----------------
# ------------********------------

    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")

    # Only aggregate successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0:
        perf_agg = perf_df.groupby("N").agg({
            "runtime_s": ["mean", "std"],
            "peak_mem_mb": ["mean", "std"],
            "success": "mean",
            "correctness": "mean",
            "factor_ratio": ["mean", "std"]
        }).reset_index()
        # Flatten column names
        perf_agg.columns = ['_'.join(col).strip('_') for col in perf_agg.columns.values]
    else:
        perf_agg = pd.DataFrame({"note": ["No successful experiments"]})

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------
    # ---------------- Scalability Results ----------------
# ------------********------------

    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")

    # Compute scaling coefficients only for successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0 and len(success_df["bits"].unique()) >= 2:
        # Log-log fit for scaling analysis
        scaling_data = success_df.groupby("bits")["runtime_s"].mean().reset_index()
        log_bits = np.log(scaling_data["bits"].values)
        log_time = np.log(scaling_data["runtime_s"].values + 1e-12)

        # Only compute if we have enough data and variation
        if len(log_bits) >= 2 and np.std(log_bits) > 1e-6 and np.std(log_time) > 1e-6:
            try:
                scaling_coeff = np.polyfit(log_bits, log_time, 1)[0]
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [scaling_coeff],
                    "description": ["Exponent in time ~ bits^exponent"]
                })
            except:
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [np.nan],
                    "description": ["Numerical issues in scaling analysis"]
                })
        else:
            scaling_summary = pd.DataFrame({
                "parameter": ["scaling_exponent"],
                "value": [np.nan],
                "description": ["Insufficient data variation for scaling analysis"]
            })
    else:
        scaling_summary = pd.DataFrame({
            "parameter": ["scaling_exponent"],
            "value": [np.nan],
            "description": ["No successful experiments for scaling analysis"]
        })

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_data": perf_df[["N", "bits", "digits", "runtime_s", "peak_mem_mb", "success"]],
            "scaling_analysis": scaling_summary
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------
    # ---------------- Reliability Results ----------------
# ------------********------------

    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    reliability_analysis = perf_df.groupby("N").agg({
        "success": ["mean", "count"],
        "correctness": "mean",
        "runtime_s": ["mean", "std", "min", "max"],
        "factor_ratio": ["mean", "std"]
    }).reset_index()
    reliability_analysis.columns = ['_'.join(col).strip('_') for col in reliability_analysis.columns.values]

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "reliability_analysis": reliability_analysis,
            "raw_results": perf_df[["N", "bits", "digits", "seed", "success", "correctness", "factors", "reason"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------
    # ---------------- Carbon Footprint Results ----------------
# ------------********------------

    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["N", "bits", "digits", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    success_count = perf_df["success"].sum()
    total_count = len(perf_df)
    success_rate = success_count / total_count if total_count > 0 else 0

    print("\n" + "="*60)
    print("üéâ SHOR'S ALGORITHM EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {total_count}")
    print(f"‚úÖ Successful:      {success_count}")
    print(f"‚ùå Failed:          {total_count - success_count}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¢ Numbers tested:  {sorted(perf_df['N'].unique())}")
    print(f"üéØ Success rate:    {success_rate * 100:.1f}%")

    if success_count > 0:
        success_df = perf_df[perf_df["success"]]
        print(f"üìè Mean factor ratio: {success_df['factor_ratio'].mean():.2f}")
        print(f"‚è±Ô∏è  Mean runtime:     {success_df['runtime_s'].mean():.6f}s")
    else:
        print(f"üìè Mean factor ratio: N/A (no successful runs)")
        print(f"‚è±Ô∏è  Mean runtime:     N/A (no successful runs)")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ============================================================
# Main execution for Kaggle
# ============================================================

if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting Shor's Algorithm Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_shors_algorithm_experiment()

    if results_df is not None:
        # Display final results summary
        success_count = results_df["success"].sum()
        total_count = len(results_df)

        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {total_count}")
        print(f"Successful experiments: {success_count}")
        print(f"Failed experiments: {total_count - success_count}")
        print(f"Numbers tested: {sorted(results_df['N'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {success_count / total_count * 100:.1f}%")

        if success_count > 0:
            success_df = results_df[results_df["success"]]
            print(f"Average factor ratio: {success_df['factor_ratio'].mean():.2f}")
            print(f"Average correctness: {success_df['correctness'].mean() * 100:.1f}%")
        else:
            print(f"Average factor ratio: N/A")
            print(f"Average correctness: N/A")

        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\n‚úÖ All Shor's Algorithm deliverables generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå Shor's Algorithm benchmark failed - no results generated")