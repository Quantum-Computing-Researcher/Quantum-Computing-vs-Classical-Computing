#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ----------------------------------------------------------------------------------------------------------------
# quantum_grover_ibm_hardware_notebook.py
# Quantum Grover benchmark on real IBM hardware (I have used lightweight version to minimize hardware resource' usage).
#
#  - Uses a standard Grover circuit that marks the |0...0> basis state.
#  - For each n in [n_min, n_max], N = 2^n search space.
#  - Grover iterations R ~ pi/(4 sqrt(K/N)), with K=1.
#  - On hardware we CAP the iteration count at max_iterations_hw to keep
#    circuits shallow and resources low.
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

# ------------********------------ Imports
#These imports set up timing, memory profiling, warnings control, 
#    and numerical/data-processing utilities for the benchmarks.
#Matplotlib is configured for headless plot generation, while NumPy 
#     and pandas support computation and result aggregation.
#Qiskit Runtime components enable construction and execution of quantum circuits 
#      on IBM backends, with warnings suppressed for clean output.
import os
import time
import math
import tracemalloc
import warnings
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2
from qiskit import QuantumCircuit
from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager

warnings.filterwarnings("ignore", category=UserWarning)


# =========================
# ------------********------------ Filesystem / utils
# =========================

def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def write_excel(tables: Dict[str, pd.DataFrame], path: str) -> None:
    # Writing multiple DataFrames to one Excel; fall back to CSV if needed.
    try:
        with pd.ExcelWriter(path, engine="openpyxl") as writer:
            for name, df in tables.items():
                sheet_name = (name[:31] or "Sheet1")
                df.to_excel(writer, index=False, sheet_name=sheet_name)
        print(f"[OK] Wrote Excel: {path}")
    except Exception as e:
        print(f"[WARN] Excel write failed ({e}); writing CSVs instead.")
        base = os.path.splitext(path)[0]
        for name, df in tables.items():
            csv_path = f"{base}__{name}.csv"
            df.to_csv(csv_path, index=False)
            print(f"[OK] Wrote CSV fallback: {csv_path}")


def save_fig(fig: plt.Figure, path: str, dpi: int = 150) -> None:
    fig.tight_layout()
    fig.savefig(path, dpi=dpi, bbox_inches="tight")
    plt.close(fig)
    print(f"[OK] Saved plot: {path}")


def resolve_desktop_file(user_arg: Optional[str],
                         default_basenames: List[str]) -> Optional[str]:
    #  Resolve CO2 Excel from Desktop or explicit path.
    desktop = os.path.join(os.environ.get("USERPROFILE", ""), "Desktop")

    def exists(p: str) -> bool:
        return os.path.isfile(p)

    if user_arg:
        base = user_arg.strip().strip('"').strip("'")
        # 1) direct path
        if exists(base):
            return os.path.abspath(base)
        # 2) Desktop\base
        cand = os.path.join(desktop, base)
        if exists(cand):
            return os.path.abspath(cand)
        # 3) add extensions if missing
        root, ext = os.path.splitext(base)
        if not ext:
            for e in (".xlsx", ".xls", ".csv"):
                cand = os.path.join(desktop, root + e)
                if exists(cand):
                    return os.path.abspath(cand)
        # 4) case-insensitive match on Desktop
        tail = os.path.basename(base).lower()
        if os.path.isdir(desktop):
            for fname in os.listdir(desktop):
                if fname.lower() == tail:
                    cand = os.path.join(desktop, fname)
                    if exists(cand):
                        return os.path.abspath(cand)
        return None

    # No user_arg: try defaults as basenames on Desktop
    for b in default_basenames:
        for ext in ("", ".xlsx", ".xls", ".csv"):
            cand = os.path.join(desktop, b + ext)
            if exists(cand):
                return os.path.abspath(cand)
    return None


# =========================
# ------------********------------ Quantum Grover core (theory)
# =========================

def dims_from_n_range(n_min: int, n_max: int) -> List[int]:
    #  Return list of N = 2^n for n_min <= n <= n_max.
    if n_min < 1:
        n_min = 1
    if n_max < n_min:
        return []
    return [2 ** n for n in range(n_min, n_max + 1)]


def grover_iterations(N: int, marked_count: int) -> Tuple[int, float, float]:
    
   # Compute Grover iteration count and theoretical success probability.
   # We treat K = marked_count solutions in an N-dimensional space.
   #   sin^2 θ = K/N
   #   p_success_theory = sin^2((2R+1) θ)  after R iterations
   #   R* ≈ pi/(4θ) - 0.5   (near-optimal)
   # Returns:
   #   R_opt (int), p_success_theory (float) for R_opt, R_star (float).
    
    K = max(1, min(marked_count, N))
    sin2_theta = K / float(N)
    sin2_theta = min(max(sin2_theta, 0.0), 1.0)
    if sin2_theta <= 0.0:
        return 0, 0.0, 0.0
    theta = math.asin(math.sqrt(sin2_theta))
    R_star = math.pi / (4.0 * theta) - 0.5
    R_opt = max(1, int(round(R_star)))
    p_theory_opt = math.sin((2 * R_opt + 1) * theta) ** 2
    p_theory_opt = min(max(p_theory_opt, 0.0), 1.0)
    return R_opt, p_theory_opt, R_star


# =========================
# Quantum Grover circuit for IBM hardware
# (oracle marks |0...0>, single solution)
# =========================

def multi_controlled_Z(qc: QuantumCircuit, qubits: List[int]) -> None:
   
    # Apply a multi-controlled Z on the given qubits (phase flip on |1...1>).
    # Implementation: H on last qubit, MCX on all except last -> last,then H on last qubit.
    
    if len(qubits) == 1:
        qc.z(qubits[0])
        return
    controls = qubits[:-1]
    target = qubits[-1]
    qc.h(target)
    qc.mcx(controls, target)
    qc.h(target)


def oracle_mark_zero(qc: QuantumCircuit, qubits: List[int]) -> None:
    
    #Oracle that phase-flips |0...0> on the given qubits.
    #Implementation:
    #  - X on all qubits to map |0...0> -> |1...1>
    #  - multi-controlled Z on |1...1>
    #  - X on all qubits
    
    qc.x(qubits)
    multi_controlled_Z(qc, qubits)
    qc.x(qubits)


def diffusion_operator(qc: QuantumCircuit, qubits: List[int]) -> None:
    
    #Grover diffusion operator (reflection about uniform state):
      #  D = 2|s><s| - I.

    # Implementation (standard):
      #  - H on all qubits
      #  - X on all qubits
      # - multi-controlled Z on |1...1>
      # - X on all qubits
      # - H on all qubits
    
    qc.h(qubits)
    qc.x(qubits)
    multi_controlled_Z(qc, qubits)
    qc.x(qubits)
    qc.h(qubits)


def build_grover_circuit(n_qubits: int, R: int) -> QuantumCircuit:
    """
    Build a Grover circuit that:
      - starts in uniform superposition over N=2^n basis states,
      - applies R Grover iterations (oracle for |0...0> + diffusion),
      - measures all data qubits.
    """
    qc = QuantumCircuit(n_qubits, n_qubits)
    q = list(range(n_qubits))

    # Uniform superposition
    qc.h(q)

    # Grover iterations
    for _ in range(R):
        oracle_mark_zero(qc, q)
        diffusion_operator(qc, q)

    # Measure all qubits
    qc.measure(q, q)
    return qc


# =========================
# ------------********------------ IBM Runtime helpers
# =========================

def get_ibm_sampler(backend_name: str, default_shots: int):
    #Initialize QiskitRuntimeService and SamplerV2 for given backend.
    #Assumes IBM account is already saved locally.
    
    print("[ibm] Initializing QiskitRuntimeService ...")
    service = QiskitRuntimeService()
    backend = service.backend(backend_name)
    print(f"[ibm] Using backend: {backend.name}")
    print(f"[ibm]   num_qubits={backend.num_qubits}, simulator={getattr(backend, 'simulator', False)}")
    print(f"[ibm]   default shots: {default_shots}")
    pm = generate_preset_pass_manager(backend=backend, optimization_level=1)
    sampler = SamplerV2(mode=backend)
    return backend, sampler, pm


  
    #Run a single Grover search on IBM hardware, with:
    #  - n_bits qubits,
    #  - oracle marking |0...0> (implicitly K=1),
     # - R_opt from theory, but capped at max_iterations_hw,
    #  - shots measurements.
    
def run_single_quantum_grover_ibm(
    n_bits: int,
    marked_count: int,
    acc_tol: float,
    seed: int,
    backend,
    sampler: SamplerV2,
    pass_manager,
    shots: int,
    max_iterations_hw: int,
) -> Dict:

    N = 2 ** n_bits
    rng = np.random.default_rng(seed)

    # We only support K=1 on hardware (oracle for |0...0>)
    if marked_count != 1:
        print(f"[WARN] marked_count={marked_count} > 1 not supported on hardware; using 1.")
    K_effective = 1

    # Theoretical Grover parameters (for K=1)
    R_opt, p_th_opt, R_star = grover_iterations(N, K_effective)
    R_hw = max(1, min(R_opt, max_iterations_hw))

    # Adjust theoretical success probability to actual R_hw
    sin2_theta = K_effective / float(N)
    theta = math.asin(math.sqrt(sin2_theta))
    p_th_hw = math.sin((2 * R_hw + 1) * theta) ** 2
    p_th_hw = min(max(p_th_hw, 0.0), 1.0)

    # Build and transpile circuit
    logical_circ = build_grover_circuit(n_bits, R_hw)
    isa_circ = pass_manager.run(logical_circ)
    depth = isa_circ.depth()

    # Measure hardware runtime + host memory
    tracemalloc.start()
    t0 = time.perf_counter()
    job = sampler.run([isa_circ], shots=int(shots))
    primitive_result = job.result()
    hw_runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Extract counts from SamplerV2 result
    # primitive_result is iterable over pub results
    res0 = primitive_result[0]
    joined = res0.join_data()
    counts = joined.get_counts()
    total_counts = sum(counts.values())
    success_str = "0" * n_bits  # all zeros -> |0...0>
    success_counts = counts.get(success_str, 0)
    p_hw = float(success_counts) / float(total_counts) if total_counts > 0 else 0.0

    p_error_hw = float(p_hw - p_th_hw)
    success_flag = int(p_hw >= acc_tol)

    return {
        "N": int(N),
        "n_bits": int(n_bits),
        "marked_count": int(K_effective),
        "seed": int(seed),
        "runtime_sec": float(hw_runtime),
        "peak_mem_mb": peak / (1024 ** 2),
        "iterations_opt": int(R_opt),
        "iterations_hw": int(R_hw),
        "queries_opt": float(R_star),
        "p_success_theory_opt": float(p_th_opt),
        "p_success_theory_hw": float(p_th_hw),
        "p_success_hw": float(p_hw),
        "p_error_hw": p_error_hw,
        "shots": int(shots),
        "circuit_depth": int(depth),
        "acc_tol": float(acc_tol),
        "success": int(success_flag),
    }


def run_quantum_grover_ibm_benchmarks(
    n_min: int,
    n_max: int,
    trials: int,
    marked_count: int,
    acc_tol: float,
    shots: int,
    seed: int,
    backend,
    sampler: SamplerV2,
    pass_manager,
    max_iterations_hw: int,
) -> pd.DataFrame:
    # Run IBM hardware Grover for N=2^n, n_min..n_max (subject to backend.num_qubits).
    Ns = dims_from_n_range(n_min, n_max)
    rows = []
    for N in Ns:
        n_bits = int(round(math.log2(N)))
        if n_bits > backend.num_qubits:
            print(f"[WARN] Skipping N={N} (n_bits={n_bits}) > backend.num_qubits={backend.num_qubits}")
            continue
        for r in range(trials):
            s = seed + N * 1000 + r * 17
            res = run_single_quantum_grover_ibm(
                n_bits=n_bits,
                marked_count=marked_count,
                acc_tol=acc_tol,
                seed=s,
                backend=backend,
                sampler=sampler,
                pass_manager=pass_manager,
                shots=shots,
                max_iterations_hw=max_iterations_hw,
            )
            res["trial"] = r
            rows.append(res)
            print(f"[run] N={N} trial={r} it_hw={res['iterations_hw']} "
                  f"p_hw={res['p_success_hw']:.4f} runtime={res['runtime_sec']:.3f}s depth={res['circuit_depth']}")
    if not rows:
        return pd.DataFrame(columns=[
            "N","n_bits","marked_count","seed","runtime_sec","peak_mem_mb",
            "iterations_opt","iterations_hw","queries_opt",
            "p_success_theory_opt","p_success_theory_hw",
            "p_success_hw","p_error_hw","shots","circuit_depth",
            "acc_tol","success","trial",
        ])
    return pd.DataFrame(rows)


# =========================
# ------------********------------ Carbon helpers
# =========================

def load_country_co2(path: str, year_select: str = "latest") -> pd.DataFrame:
    # Load country-level CO2 intensity (kg CO2 / kWh); keep latest year per country.
    if path.lower().endswith(".csv"):
        df = pd.read_csv(path)
    else:
        df = pd.read_excel(path)

    df.columns = [str(c).strip().lower() for c in df.columns]

    cname = next((c for c in df.columns if "country" in c or c in ("name","nation")), None)
    if cname is None:
        raise ValueError("No 'country' column found in CO2 file.")

    yname = next((c for c in df.columns if "year" in c), None)

    cand = [c for c in df.columns if "intensity" in c]
    if not cand:
        cand = [c for c in df.columns if ("co2" in c and "kwh" in c)]
    if not cand:
        cand = [c for c in df.columns if (("kg" in c or "g" in c) and "kwh" in c)]
    if not cand:
        raise ValueError("No intensity column found in CO2 file.")
    icol = cand[0]

    cols = [cname, icol] + ([yname] if yname else [])
    out = df[cols].dropna().copy()
    out.columns = ["country","intensity_raw"] + (["year"] if yname else [])
    out["intensity_raw"] = pd.to_numeric(out["intensity_raw"], errors="coerce")
    out = out.dropna(subset=["intensity_raw"])

    med = float(np.nanmedian(out["intensity_raw"].values))
    if med > 10.0:
        out["kg_per_kwh"] = out["intensity_raw"] / 1000.0
    else:
        out["kg_per_kwh"] = out["intensity_raw"]

    out["country"] = out["country"].astype(str).str.strip().str.title()
    if yname and year_select == "latest":
        out = out.sort_values(["country","year"]).groupby("country", as_index=False).tail(1)

    cols_out = ["country","kg_per_kwh"] + (["year"] if yname else [])
    return out[cols_out].drop_duplicates()


def compute_emissions(perf_df: pd.DataFrame,
                      co2_df: pd.DataFrame,
                      power_watts: float,
                      pue: float) -> pd.DataFrame:
    """
    Compute carbon emissions for each run:
      E_kWh = runtime_sec * power_watts / 3.6e6 * pue
      emissions = E_kWh * kg_per_kWh
    """
    perf = perf_df.copy()
    perf["energy_kwh"] = (perf["runtime_sec"] * power_watts) / 3.6e6
    perf["energy_kwh"] *= pue
    co2 = co2_df.rename(columns={"kg_per_kwh": "kgco2_per_kwh"}).copy()
    perf["_k"] = 1
    co2["_k"] = 1
    joined = perf.merge(co2, on="_k").drop(columns="_k")
    joined["emissions_kgCO2"] = joined["energy_kwh"] * joined["kgco2_per_kwh"]
    return joined


def build_worstcase_country_table(perf_df: pd.DataFrame,
                                  co2_df: pd.DataFrame,
                                  power_watts: float,
                                  pue: float) -> Tuple[pd.DataFrame, dict]:
   
    #Use the largest N and its mean runtime as a worst-case scenario,
    #then compute per-country emissions for that scenario.
   
    N_heavy = int(perf_df["N"].max())
    mask = perf_df["N"] == N_heavy
    mean_runtime_s = float(perf_df.loc[mask, "runtime_sec"].mean())
    energy_kwh = mean_runtime_s * power_watts / 3.6e6 * pue

    tbl = co2_df.copy()
    if "year" not in tbl.columns:
        tbl["year"] = pd.NA

    out = pd.DataFrame({
        "Country": tbl["country"].astype(str),
        "Year": tbl["year"],
        "Intensity": tbl["kg_per_kwh"],
        "kWh": energy_kwh,
    })
    out["kgCO2e"] = out["Intensity"] * out["kWh"]
    out = out.sort_values("kgCO2e", ascending=False).reset_index(drop=True)

    meta = {
        "N_heavy": N_heavy,
        "n_bits_heavy": int(round(math.log2(N_heavy))) if N_heavy > 0 else 0,
        "mean_runtime_s": mean_runtime_s,
        "power_watts": power_watts,
        "pue": pue,
        "kWh_used": energy_kwh,
    }
    return out, meta


def plot_carbon_distribution(worst_table: pd.DataFrame, out_path: str) -> None:
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.hist(worst_table["kgCO2e"].values, bins=30)
    ax.set_xlabel("kg CO2e")
    ax.set_title("Carbon: Emission distribution (worst-case, Grover IBM)")
    save_fig(fig, out_path)


def plot_carbon_topN(worst_table: pd.DataFrame, N: int, out_path: str) -> None:
    s = worst_table.sort_values("kgCO2e", ascending=False).head(N)
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(s["Country"], s["kgCO2e"])
    ax.invert_yaxis()
    ax.set_xlabel("kg CO2e")
    ax.set_title(f"Carbon: Top {N} countries (worst-case, Grover IBM)")
    save_fig(fig, out_path)


def plot_carbon_median_vs_dimension(carbon_df: pd.DataFrame, out_path: str) -> None:
    med = carbon_df.groupby("N")["emissions_kgCO2"].median().reset_index()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(med["N"], med["emissions_kgCO2"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Median emissions (kg CO2e)")
    ax.set_title("Median emissions vs N (IBM Grover)")
    save_fig(fig, out_path)


# =========================
# ------------********------------ Plot helpers: performance / reliability / scalability
# =========================

def plot_runtime_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["runtime_sec"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Runtime (s)")
    ax.set_title("Performance: runtime vs N (Grover on IBM hardware)")
    save_fig(fig, out_path)


def plot_memory_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["peak_mem_mb"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["peak_mem_mb"], marker="s")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Peak memory (MB)")
    ax.set_title("Performance: memory vs N (Grover on IBM hardware)")
    save_fig(fig, out_path)


def plot_iterations_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["iterations_hw"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["iterations_hw"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Mean Grover iterations (hardware)")
    ax.set_title("Grover iterations vs N (IBM hardware, capped)")
    save_fig(fig, out_path)


def plot_p_success_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False).agg(
        p_theory=("p_success_theory_hw","mean"),
        p_hw=("p_success_hw","mean"),
    )
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["p_theory"], marker="o", label="theory (R_hw)")
    ax.plot(sub["N"], sub["p_hw"], marker="s", label="hardware")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Success probability")
    ax.set_title("Reliability: success probability (theory vs IBM hardware)")
    ax.legend()
    save_fig(fig, out_path)


def plot_p_error_hist(perf_df: pd.DataFrame, out_path: str) -> None:
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.hist(perf_df["p_error_hw"].values, bins=30)
    ax.set_xlabel("p_success_hw - p_success_theory_hw")
    ax.set_ylabel("Count")
    ax.set_title("Reliability: distribution of p_success error (IBM vs theory)")
    save_fig(fig, out_path)


def plot_scaling_runtime_vs_N_loglog(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["runtime_sec"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Runtime (s)")
    ax.set_title("Scalability: runtime vs N (log-log, Grover IBM)")
    save_fig(fig, out_path)


# =========================
# ------------********------------ Notebook entry point
# =========================

def run_quantum_grover_ibm_hardware_notebook(
    # same "CLI" parameters as noiseless version
    n_min: int = 2,
    n_max: int = 6,
    trials: int = 5,
    marked_count: int = 1,
    acc_tol: float = 0.9,
    shots: int = 1024,
    # carbon & CO2
    excel_filename: str = "Filtered CO2 intensity 236 Countries.xlsx",
    device_power_watts: float = 65.0,
    pue: float = 1.2,
    year_select: str = "latest",
    outdir: Optional[str] = None,
    # random seed
    seed: int = 7,
    # IBM hardware-specific
    backend_name: str = "ibm_torino",
    max_iterations_hw: int = 8,   # cap Grover iterations for resource control
):

    """
    Quantum Grover benchmark on real IBM hardware (lightweight).

    Same 'CLI-style' arguments as noiseless version:
      - n_min, n_max: qubit range, search size N=2^n.
      - trials: runs per N (different seeds).
      - marked_count: #marked items (for theory only; hardware oracle is |0...0>, i.e. K=1).
      - acc_tol: threshold for success (p_success_hw >= acc_tol).
      - shots: measurements per circuit (hardware).
      - excel_filename, device_power_watts (65 W), pue (1.2),
        year_select, outdir, seed.

      - backend_name: IBM backend to use.
      - max_iterations_hw: cap on Grover iterations (to keep circuits shallow).
    """


    Ns = dims_from_n_range(n_min, n_max)
    if not Ns:
        raise ValueError("No valid powers of two in the requested n-range.")

    if marked_count != 1:
        print(f"[WARN] marked_count={marked_count} != 1; on hardware we implement only one marked state |0...0>.")

    if shots <= 0:
        shots = 1024
        print(f"[WARN] shots <= 0, setting shots={shots} for hardware.")

    print("[quantum-grover-ibm] Benchmark settings")
    print("  n_min:", n_min, "n_max:", n_max)
    print("  N values:", Ns)
    print("  trials:", trials, "marked_count:", marked_count)
    print("  acc_tol:", acc_tol, "shots:", shots)
    print("  backend:", backend_name, "max_iterations_hw:", max_iterations_hw)
    print("  excel_filename:", excel_filename)
    print("  device_power_watts:", device_power_watts, "PUE:", pue)
    print("  year_select:", year_select)

    # Output root
    if outdir is None:
        root = os.getcwd()
    else:
        root = os.path.abspath(os.path.expandvars(outdir))

    perf_dir = os.path.join(root, "Performance")
    scal_dir = os.path.join(root, "Scalability")
    reli_dir = os.path.join(root, "Reliability")
    carb_dir = os.path.join(root, "Carbon")
    for d in (perf_dir, scal_dir, reli_dir, carb_dir):
        ensure_dir(d)

    print("[INFO] Output root:", root)
    print("       - Performance ->", perf_dir)
    print("       - Scalability ->", scal_dir)
    print("       - Reliability ->", reli_dir)
    print("       - Carbon      ->", carb_dir)

    # IBM runtime objects
    backend, sampler, pm = get_ibm_sampler(backend_name, default_shots=shots)

    # -----------------------
    # ------------********------------ PERFORMANCE + RELIABILITY data
    # -----------------------
    print("[INFO] Running Grover circuits on IBM hardware ...")
    t0_all = time.perf_counter()
    perf_df = run_quantum_grover_ibm_benchmarks(
        n_min=n_min,
        n_max=n_max,
        trials=trials,
        marked_count=marked_count,
        acc_tol=acc_tol,
        shots=shots,
        seed=seed,
        backend=backend,
        sampler=sampler,
        pass_manager=pm,
        max_iterations_hw=max_iterations_hw,
    )
    print(f"[INFO] Hardware benchmarks completed in {time.perf_counter() - t0_all:.2f} s")

    if perf_df.empty:
        print("[WARN] No data collected (maybe n range exceeds backend qubits?).")
        return {
            "perf_df": perf_df,
            "perf_dir": perf_dir,
            "scal_dir": scal_dir,
            "reli_dir": reli_dir,
            "carb_dir": carb_dir,
            "carbon_df": None,
            "carbon_summary": None,
        }

    # PERFORMANCE summary
    perf_summary = perf_df.groupby("N", as_index=False).agg(
        n_bits=("n_bits","first"),
        marked_count=("marked_count","max"),
        trials=("trial","nunique"),
        mean_runtime_sec=("runtime_sec","mean"),
        std_runtime_sec=("runtime_sec","std"),
        mean_peak_mem_mb=("peak_mem_mb","mean"),
        std_peak_mem_mb=("peak_mem_mb","std"),
        mean_iterations_hw=("iterations_hw","mean"),
        std_iterations_hw=("iterations_hw","std"),
        mean_p_success_theory_hw=("p_success_theory_hw","mean"),
        mean_p_success_hw=("p_success_hw","mean"),
        success_rate=("success","mean"),
        mean_depth=("circuit_depth","mean"),
    )

    perf_xlsx = os.path.join(perf_dir, "performance_quantum_grover_ibm_hardware.xlsx")
    write_excel(
        {
            "raw_runs": perf_df.sort_values(["N","trial"]),
            "summary": perf_summary.sort_values("N"),
        },
        perf_xlsx,
    )
    plot_runtime_vs_N(perf_df, os.path.join(perf_dir, "runtime_vs_N_ibm.png"))
    plot_memory_vs_N(perf_df, os.path.join(perf_dir, "memory_vs_N_ibm.png"))
    plot_iterations_vs_N(perf_df, os.path.join(perf_dir, "iterations_vs_N_ibm.png"))

    # -----------------------
    # ------------********------------ RELIABILITY
    # -----------------------
    reli_xlsx = os.path.join(reli_dir, "reliability_quantum_grover_ibm_hardware.xlsx")
    write_excel(
        {
            "runs": perf_df[[
                "N","n_bits","trial",
                "iterations_opt","iterations_hw","queries_opt",
                "p_success_theory_opt","p_success_theory_hw",
                "p_success_hw","p_error_hw","shots",
                "circuit_depth","success",
            ]].sort_values(["N","trial"]),
        },
        reli_xlsx,
    )
    plot_p_success_vs_N(perf_df, os.path.join(reli_dir, "p_success_vs_N_ibm.png"))
    plot_p_error_hist(perf_df, os.path.join(reli_dir, "p_error_hist_ibm.png"))

    # -----------------------
    # ------------********------------ SCALABILITY
    # -----------------------
    scal_xlsx = os.path.join(scal_dir, "scalability_quantum_grover_ibm_hardware.xlsx")

    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    x = np.log(sub["N"].values + 1e-16)
    y = np.log(sub["runtime_sec"].values + 1e-16)
    alpha, _ = np.polyfit(x, y, 1)
    scaling_fit = pd.DataFrame({
        "fit_parameter": ["alpha_dim_scaling"],
        "estimate": [alpha],
        "note": ["runtime ~ N^alpha (Grover on IBM hardware)"],
    })

    write_excel(
        {
            "perf_raw": perf_df.sort_values(["N","trial"]),
            "scaling_fit": scaling_fit,
        },
        scal_xlsx,
    )
    plot_scaling_runtime_vs_N_loglog(perf_df, os.path.join(scal_dir, "runtime_vs_N_loglog_ibm.png"))

    # -----------------------
    # ------------********------------ CARBON
    # -----------------------
    desktop = os.path.join(os.environ.get("USERPROFILE",""), "Desktop")
    defaults = [
        "Filtered CO2 intensity 236 Countries",
        "countries_co2",
        "co2_countries",
    ]
    co2_path = resolve_desktop_file(excel_filename, defaults)

    carbon_df = None
    carbon_meta_df = None

    if co2_path and os.path.isfile(co2_path):
        print(f"[INFO] Using CO2 file: {co2_path}")
        co2_df = load_country_co2(co2_path, year_select=year_select)
        carbon_df = compute_emissions(
            perf_df,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        worst_table, meta = build_worstcase_country_table(
            perf_df,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        carbon_meta_df = pd.DataFrame([meta])
        carb_xlsx = os.path.join(carb_dir, "carbon_quantum_grover_ibm_hardware.xlsx")
        write_excel(
            {
                "co2_by_country": (co2_df.rename(columns={"kg_per_kwh":"kgco2_per_kwh"})
                                   if "kg_per_kwh" in co2_df.columns else co2_df),
                "emissions_all": carbon_df.sort_values(["N","country"]),
                "worst_case_table": worst_table,
                "worst_case_metadata": carbon_meta_df,
            },
            carb_xlsx,
        )
        plot_carbon_distribution(worst_table, os.path.join(carb_dir, "carbon_distribution_ibm.png"))
        plot_carbon_topN(worst_table, 15, os.path.join(carb_dir, "carbon_top15_ibm.png"))
        plot_carbon_median_vs_dimension(carbon_df, os.path.join(carb_dir, "median_emissions_vs_N_ibm.png"))
    else:
        print("[WARN] CO2 file not found; writing placeholder carbon workbook.")
        carb_xlsx = os.path.join(carb_dir, "carbon_placeholder_quantum_grover_ibm_hardware.xlsx")
        placeholder = pd.DataFrame({
            "note": ["CO2 file not provided or not found on Desktop."],
            "hint": ["Place 'Filtered CO2 intensity 236 Countries.xlsx' on Desktop "
                     "or pass full path via excel_filename."],
        })
        write_excel({"info": placeholder}, carb_xlsx)

    print("\n[DONE] Quantum Grover IBM hardware benchmark complete.")
    print(f"Performance Excel:  {perf_xlsx}")
    print(f"Scalability Excel:  {scal_xlsx}")
    print(f"Reliability Excel:  {reli_xlsx}")
    print(f"Carbon Excel:       {os.path.join(carb_dir, 'carbon_quantum_grover_ibm_hardware.xlsx')}")
    print("Folders under:", root)
    print("  - Performance")
    print("  - Scalability")
    print("  - Reliability")
    print("  - Carbon")

    return {
        "perf_df": perf_df,
        "perf_dir": perf_dir,
        "scal_dir": scal_dir,
        "reli_dir": reli_dir,
        "carb_dir": carb_dir,
        "carbon_df": carbon_df,
        "carbon_summary": carbon_meta_df,
    }
