#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ----------------------------------------------------------------------------------------------------------------
# quantum_grover_noiseless_notebook.py
#
# Noiseless quantum Grover benchmark:
#   - Ideal statevector simulation (no hardware, no noise).
#   - Search space N = 2^n, with 'marked_count' marked items.
#   - Grover iterations R ~ pi/(4*sqrt(K/N)).
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

# ------------********------------ Imports
import os
import time
import math
import tracemalloc
import warnings
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore", category=UserWarning)


# =========================
# ------------********------------  Filesystem / utils
# =========================

#def ensure_dir creates a directory if it does not already exist, safely handling repeated calls.
def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


#def write_excel writes multiple DataFrames into a single Excel workbook with one sheet per table,
#    falling back to CSV files on failure.
def write_excel(tables: Dict[str, pd.DataFrame], path: str) -> None:
    try:
        with pd.ExcelWriter(path, engine="openpyxl") as writer:
            for name, df in tables.items():
                sheet_name = (name[:31] or "Sheet1")
                df.to_excel(writer, index=False, sheet_name=sheet_name)
        print(f"[OK] Wrote Excel: {path}")
    except Exception as e:
        print(f"[WARN] Excel write failed ({e}); writing CSVs instead.")
        base = os.path.splitext(path)[0]
        for name, df in tables.items():
            csv_path = f"{base}__{name}.csv"
            df.to_csv(csv_path, index=False)
            print(f"[OK] Wrote CSV fallback: {csv_path}")


#def save_fig finalizes layout, saves a Matplotlib figure to disk with consistent formatting, and closes it to free resources.
def save_fig(fig: plt.Figure, path: str, dpi: int = 150) -> None:
    fig.tight_layout()
    fig.savefig(path, dpi=dpi, bbox_inches="tight")
    plt.close(fig)
    print(f"[OK] Saved plot: {path}")


def resolve_desktop_file(user_arg: Optional[str],
                         default_basenames: List[str]) -> Optional[str]:
    # Resolve CO2 Excel from Desktop or given path.
    desktop = os.path.join(os.environ.get("USERPROFILE", ""), "Desktop")

    def exists(p: str) -> bool:
        return os.path.isfile(p)

    if user_arg:
        base = user_arg.strip().strip('"').strip("'")
        # 1) direct path
        if exists(base):
            return os.path.abspath(base)
        # 2) Desktop\base
        cand = os.path.join(desktop, base)
        if exists(cand):
            return os.path.abspath(cand)
        # 3) if no extension: try common ones on Desktop
        root, ext = os.path.splitext(base)
        if not ext:
            for e in (".xlsx", ".xls", ".csv"):
                cand = os.path.join(desktop, root + e)
                if exists(cand):
                    return os.path.abspath(cand)
        # 4) case-insensitive match on Desktop
        tail = os.path.basename(base).lower()
        if os.path.isdir(desktop):
            for fname in os.listdir(desktop):
                if fname.lower() == tail:
                    cand = os.path.join(desktop, fname)
                    if exists(cand):
                        return os.path.abspath(cand)
        return None

    # No user_arg: try defaults as basenames on Desktop
    for b in default_basenames:
        for ext in ("", ".xlsx", ".xls", ".csv"):
            cand = os.path.join(desktop, b + ext)
            if exists(cand):
                return os.path.abspath(cand)
    return None


# =========================
# ------------********------------ Quantum Grover core (noiseless statevector)
# =========================

#def dims_from_n_range generates problem sizes as powers of two for a given bit-length range.
#It clamps the lower bound to at least 1 and returns an empty list if the range is invalid.
#The output is a list of N = 2^n values for n between n_min and n_max inclusive.

def dims_from_n_range(n_min: int, n_max: int) -> List[int]:
    if n_min < 1:
        n_min = 1
    if n_max < n_min:
        return []
    return [2 ** n for n in range(n_min, n_max + 1)]


def grover_iterations(N: int, marked_count: int) -> Tuple[int, float, float]:
  
    # Compute Grover iteration count and theoretical success probability.
    #
    # sin^2 θ = K/N, where K = marked_count.
    # Then after R iterations:
    #  p_success_theory = sin^2((2R+1) θ)
    # A near-optimal iteration count is:
    #  R* ≈ pi/(4θ) - 0.5
    # We use R = max(1, round(R*)).
    # Returns: R (int), p_success_theory, R_star (float, 'queries_theory')
    
    K = max(1, min(marked_count, N))
    sin2_theta = K / float(N)
    # just in case numerical issues:
    sin2_theta = min(max(sin2_theta, 0.0), 1.0)
    if sin2_theta <= 0.0:
        return 0, 0.0, 0.0
    theta = math.asin(math.sqrt(sin2_theta))
    R_star = math.pi / (4.0 * theta) - 0.5
    R = max(1, int(round(R_star)))
    p_theory = math.sin((2 * R + 1) * theta) ** 2
    # Clip just in case of floating rounding.
    p_theory = min(max(p_theory, 0.0), 1.0)
    return R, p_theory, R_star


def run_single_quantum_grover(
    N: int,
    marked_count: int,
    acc_tol: float,
    seed: int,
    shots: int = 0,
) -> Dict:
   
    #Single noiseless quantum Grover run with full statevector simulation.
    #Steps:
    #  - Choose K = marked_count random marked indices.
    #  - Initialize |ψ0> = uniform superposition over N basis states.
    #  - Compute optimal iteration count R and theoretical p_success.
    #  - For each iteration:
    #      * Oracle: phase-flip marked amplitudes: a_i -> -a_i for i in marked set
    #      * Diffusion: reflection around mean (D = 2|s⟩⟨s| - I)
    #  - Compute simulated success probability p_sim = sum_{i in marked} |a_i|^2.
    #  - If shots > 0, sample measured success frequency from Binomial(shots, p_sim).
    
    rng = np.random.default_rng(seed)

    K = max(1, min(marked_count, N))
    # choose marked indices
    marked = rng.choice(N, size=K, replace=False)
    marked = np.array(marked, dtype=np.int64)
    marked_set = set(int(i) for i in marked)

    # Grover parameters
    R, p_theory, R_star = grover_iterations(N, K)

    # Initialize uniform state
    amp = np.ones(N, dtype=np.complex128) / math.sqrt(N)

    # Measure runtime & memory for Grover iterations
    tracemalloc.start()
    t0 = time.perf_counter()

    for _ in range(R):
        # Oracle: phase-flip marked states
        amp[marked] *= -1.0
        # Diffusion: reflection about uniform state |s>
        mean_amp = amp.mean()
        amp = 2.0 * mean_amp - amp
        # re-normalize (optional, for numerical stability)
        norm = np.linalg.norm(amp)
        if norm > 0:
            amp /= norm

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Success probability from statevector
    probs = np.abs(amp) ** 2
    p_sim = float(probs[list(marked_set)].sum())
    p_sim = min(max(p_sim, 0.0), 1.0)

    # Optional measurement sampling
    if shots and shots > 0:
        k_succ = rng.binomial(shots, p_sim)
        p_meas = float(k_succ) / float(shots)
    else:
        p_meas = float("nan")

    # Metrics
    p_error_sim = float(p_sim - p_theory)
    p_error_meas = float(p_meas - p_theory) if not math.isnan(p_meas) else float("nan")
    success = int(p_sim >= acc_tol)

    return {
        "N": int(N),
        "n_bits": int(round(math.log2(N))) if N > 0 else 0,
        "marked_count": int(K),
        "seed": int(seed),
        "runtime_sec": float(runtime),
        "peak_mem_mb": peak / (1024 ** 2),
        "iterations": int(R),
        "queries": int(R),             # treat 1 oracle call per Grover iteration
        "queries_theory": float(R_star),
        "queries_error": float(R - R_star),
        "p_success_theory": float(p_theory),
        "p_success_sim": float(p_sim),
        "p_success_meas": float(p_meas),
        "p_error_sim": p_error_sim,
        "p_error_meas": p_error_meas,
        "acc_tol": float(acc_tol),
        "success": success,
    }

# def run_quantum_grover_benchmarks executes repeated quantum Grover search experiments 
#     over problem sizes N = 2^n for a given range.
#It runs multiple trials per size, collects iteration counts, success probabilities, runtimes, and accuracy metrics, and logs progress.
# All per-trial results are aggregated into a pandas DataFrame and returned for further analysis.

def run_quantum_grover_benchmarks(
    n_min: int,
    n_max: int,
    trials: int,
    marked_count: int,
    acc_tol: float,
    shots: int,
    seed: int,
) -> pd.DataFrame:
    
    Ns = dims_from_n_range(n_min, n_max)
    rows = []
    for N in Ns:
        for r in range(trials):
            s = seed + N * 1000 + r * 17
            res = run_single_quantum_grover(
                N=N,
                marked_count=marked_count,
                acc_tol=acc_tol,
                seed=s,
                shots=shots,
            )
            res["trial"] = r
            rows.append(res)
            print(f"[run] N={N} trial={r} iter={res['iterations']} "
                  f"p_sim={res['p_success_sim']:.4f} runtime={res['runtime_sec']:.6f}s")
    if not rows:
        return pd.DataFrame(columns=[
            "N","n_bits","marked_count","seed","runtime_sec","peak_mem_mb",
            "iterations","queries","queries_theory","queries_error",
            "p_success_theory","p_success_sim","p_success_meas",
            "p_error_sim","p_error_meas","acc_tol","success","trial",
        ])
    return pd.DataFrame(rows)


# =========================
# ------------********------------ Carbon helpers
# =========================

def load_country_co2(path: str, year_select: str = "latest") -> pd.DataFrame:
    # Load country-level CO2 intensity (kg CO2 / kWh); keep latest year per country
    if path.lower().endswith(".csv"):
        df = pd.read_csv(path)
    else:
        df = pd.read_excel(path)

    df.columns = [str(c).strip().lower() for c in df.columns]

    cname = next((c for c in df.columns if "country" in c or c in ("name","nation")), None)
    if cname is None:
        raise ValueError("No 'country' column found in CO2 file.")

    yname = next((c for c in df.columns if "year" in c), None)

    cand = [c for c in df.columns if "intensity" in c]
    if not cand:
        cand = [c for c in df.columns if ("co2" in c and "kwh" in c)]
    if not cand:
        cand = [c for c in df.columns if (("kg" in c or "g" in c) and "kwh" in c)]
    if not cand:
        raise ValueError("No intensity column found in CO2 file.")
    icol = cand[0]

    cols = [cname, icol] + ([yname] if yname else [])
    out = df[cols].dropna().copy()
    out.columns = ["country","intensity_raw"] + (["year"] if yname else [])
    out["intensity_raw"] = pd.to_numeric(out["intensity_raw"], errors="coerce")
    out = out.dropna(subset=["intensity_raw"])

    med = float(np.nanmedian(out["intensity_raw"].values))
    if med > 10.0:
        out["kg_per_kwh"] = out["intensity_raw"] / 1000.0
    else:
        out["kg_per_kwh"] = out["intensity_raw"]

    out["country"] = out["country"].astype(str).str.strip().str.title()
    if yname and year_select == "latest":
        out = out.sort_values(["country","year"]).groupby("country", as_index=False).tail(1)

    cols_out = ["country","kg_per_kwh"] + (["year"] if yname else [])
    return out[cols_out].drop_duplicates()


def compute_emissions(perf_df: pd.DataFrame,
                      co2_df: pd.DataFrame,
                      power_watts: float,
                      pue: float) -> pd.DataFrame:
   
    #  Compute carbon emissions for each run:
    #  E_kWh = runtime_sec * power_watts / 3.6e6 * pue
    #  emissions = E_kWh * kg_per_kWh
    
    perf = perf_df.copy()
    perf["energy_kwh"] = (perf["runtime_sec"] * power_watts) / 3.6e6
    perf["energy_kwh"] *= pue
    co2 = co2_df.rename(columns={"kg_per_kwh": "kgco2_per_kwh"}).copy()
    perf["_k"] = 1
    co2["_k"] = 1
    joined = perf.merge(co2, on="_k").drop(columns="_k")
    joined["emissions_kgCO2"] = joined["energy_kwh"] * joined["kgco2_per_kwh"]
    return joined


def build_worstcase_country_table(perf_df: pd.DataFrame,
                                  co2_df: pd.DataFrame,
                                  power_watts: float,
                                  pue: float) -> Tuple[pd.DataFrame, dict]:
    
    #Use the largest N and its mean runtime as a worst-case scenario,
    #then compute per-country emissions for that scenario.
    
    N_heavy = int(perf_df["N"].max())
    mask = perf_df["N"] == N_heavy
    mean_runtime_s = float(perf_df.loc[mask, "runtime_sec"].mean())
    energy_kwh = mean_runtime_s * power_watts / 3.6e6 * pue

    tbl = co2_df.copy()
    if "year" not in tbl.columns:
        tbl["year"] = pd.NA

    out = pd.DataFrame({
        "Country": tbl["country"].astype(str),
        "Year": tbl["year"],
        "Intensity": tbl["kg_per_kwh"],
        "kWh": energy_kwh,
    })
    out["kgCO2e"] = out["Intensity"] * out["kWh"]
    out = out.sort_values("kgCO2e", ascending=False).reset_index(drop=True)

    meta = {
        "N_heavy": N_heavy,
        "n_bits_heavy": int(round(math.log2(N_heavy))) if N_heavy > 0 else 0,
        "mean_runtime_s": mean_runtime_s,
        "power_watts": power_watts,
        "pue": pue,
        "kWh_used": energy_kwh,
    }
    return out, meta


def plot_carbon_distribution(worst_table: pd.DataFrame, out_path: str) -> None:
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.hist(worst_table["kgCO2e"].values, bins=30)
    ax.set_xlabel("kg CO2e")
    ax.set_title("Carbon: Emission distribution (worst-case, quantum Grover)")
    save_fig(fig, out_path)


def plot_carbon_topN(worst_table: pd.DataFrame, N: int, out_path: str) -> None:
    s = worst_table.sort_values("kgCO2e", ascending=False).head(N)
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(s["Country"], s["kgCO2e"])
    ax.invert_yaxis()
    ax.set_xlabel("kg CO2e")
    ax.set_title(f"Carbon: Top {N} countries (worst-case, quantum Grover)")
    save_fig(fig, out_path)


def plot_carbon_median_vs_dimension(carbon_df: pd.DataFrame, out_path: str) -> None:
    med = carbon_df.groupby("N")["emissions_kgCO2"].median().reset_index()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(med["N"], med["emissions_kgCO2"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Median emissions (kg CO2e)")
    ax.set_title("Median emissions vs N (quantum Grover, noiseless)")
    save_fig(fig, out_path)


# =========================
# ------------********------------ Plot helpers: performance / reliability / scalability
# =========================

def plot_runtime_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["runtime_sec"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Runtime (s)")
    ax.set_title("Performance: runtime vs N (quantum Grover, noiseless)")
    save_fig(fig, out_path)


def plot_memory_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["peak_mem_mb"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["peak_mem_mb"], marker="s")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Peak memory (MB)")
    ax.set_title("Performance: memory vs N (quantum Grover, noiseless)")
    save_fig(fig, out_path)


def plot_iterations_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["iterations"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["iterations"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Mean Grover iterations")
    ax.set_title("Quantum Grover iterations vs N (expected ~ sqrt(N))")
    save_fig(fig, out_path)


def plot_p_success_vs_N(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False).agg(
        p_theory=("p_success_theory","mean"),
        p_sim=("p_success_sim","mean"),
    )
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["p_theory"], marker="o", label="theory")
    ax.plot(sub["N"], sub["p_sim"], marker="s", label="simulated")
    ax.set_xscale("log", base=2)
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Success probability")
    ax.set_title("Reliability: p_success (theory vs simulated)")
    ax.legend()
    save_fig(fig, out_path)


def plot_p_error_hist(perf_df: pd.DataFrame, out_path: str) -> None:
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.hist(perf_df["p_error_sim"].values, bins=30)
    ax.set_xlabel("p_success_sim - p_success_theory")
    ax.set_ylabel("Count")
    ax.set_title("Reliability: distribution of p_success error (sim vs theory)")
    save_fig(fig, out_path)


def plot_scaling_runtime_vs_N_loglog(perf_df: pd.DataFrame, out_path: str) -> None:
    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(sub["N"], sub["runtime_sec"], marker="o")
    ax.set_xscale("log", base=2)
    ax.set_yscale("log")
    ax.set_xlabel("Search space size N")
    ax.set_ylabel("Runtime (s)")
    ax.set_title("Scalability: runtime vs N (log-log, quantum Grover)")
    save_fig(fig, out_path)


# =========================
# ------------********------------ Notebook entry point
# =========================

   
    # Noiseless quantum Grover benchmark:
    #
    #  - For each n in [n_min, n_max], set N=2^n.
    #  - For each N, run 'trials' Grover searches with 'marked_count' solutions.
    #  - Ideal statevector simulation: no noise, no hardware.
    #  - Compute:
    #      * Performance: runtime, memory, iterations, queries.
    #      * Reliability: difference between simulated and theoretical success probabilities.
    #      * Scalability: log-log fit of runtime vs N.
    #      * Carbon: emissions by country using CO2 intensity, power (65 W), PUE (1.2).
    #
    #  Parameters:
    #    n_min, n_max: number of qubits range (search space size N=2^n).
    #    trials: repetitions per N (different seeds).
    #    marked_count: number of "marked" items in the search space.
    #    acc_tol: threshold for success based on p_success_sim (default 0.9).
    #    shots: if > 0, also simulate measurement sampling.
    #    excel_filename: CO2 Excel 
    #    device_power_watts: device power for carbon (default 65 W).
    #    pue: Power Usage Effectiveness (default 1.2).
    #    year_select: "latest" record per country in CO2 file.
    #    outdir: root output directory; inside it we create:
    #            Performance, Scalability, Reliability, Carbon.
   

def run_quantum_grover_noiseless_notebook(
    # problem size & repetitions
    n_min: int = 8,
    n_max: int = 16,
    trials: int = 10,
    marked_count: int = 1,
    acc_tol: float = 0.9,
    shots: int = 0,
    # carbon & CO2
    excel_filename: str = "Filtered CO2 intensity 236 Countries.xlsx",
    device_power_watts: float = 65.0,
    pue: float = 1.2,
    year_select: str = "latest",
    outdir: Optional[str] = None,
    # random seed
    seed: int = 7,
):

    Ns = dims_from_n_range(n_min, n_max)
    if not Ns:
        raise ValueError("No valid powers of two in the requested n-range.")

    print("[quantum-grover-noiseless] Benchmark settings")
    print("  n_min:", n_min, "n_max:", n_max)
    print("  N values:", Ns)
    print("  trials:", trials, "marked_count:", marked_count)
    print("  acc_tol:", acc_tol, "shots:", shots)
    print("  excel_filename:", excel_filename)
    print("  device_power_watts:", device_power_watts, "PUE:", pue)
    print("  year_select:", year_select)

    # Output root
    if outdir is None:
        root = os.getcwd()
    else:
        root = os.path.abspath(os.path.expandvars(outdir))

    perf_dir = os.path.join(root, "Performance")
    scal_dir = os.path.join(root, "Scalability")
    reli_dir = os.path.join(root, "Reliability")
    carb_dir = os.path.join(root, "Carbon")
    for d in (perf_dir, scal_dir, reli_dir, carb_dir):
        ensure_dir(d)

    print("[INFO] Output root:", root)
    print("       - Performance ->", perf_dir)
    print("       - Scalability ->", scal_dir)
    print("       - Reliability ->", reli_dir)
    print("       - Carbon      ->", carb_dir)

    # -----------------------
    # ------------********------------ PERFORMANCE + RELIABILITY data
    # -----------------------
    print("[INFO] Running noiseless quantum Grover benchmarks ...")
    t0_all = time.perf_counter()
    perf_df = run_quantum_grover_benchmarks(
        n_min=n_min,
        n_max=n_max,
        trials=trials,
        marked_count=marked_count,
        acc_tol=acc_tol,
        shots=shots,
        seed=seed,
    )
    print(f"[INFO] Benchmarks completed in {time.perf_counter() - t0_all:.2f} s")

    # PERFORMANCE summary
    perf_summary = perf_df.groupby("N", as_index=False).agg(
        n_bits=("n_bits","first"),
        marked_count=("marked_count","max"),
        trials=("trial","nunique"),
        mean_runtime_sec=("runtime_sec","mean"),
        std_runtime_sec=("runtime_sec","std"),
        mean_peak_mem_mb=("peak_mem_mb","mean"),
        std_peak_mem_mb=("peak_mem_mb","std"),
        mean_iterations=("iterations","mean"),
        std_iterations=("iterations","std"),
        mean_queries=("queries","mean"),
        mean_p_success_theory=("p_success_theory","mean"),
        mean_p_success_sim=("p_success_sim","mean"),
        success_rate=("success","mean"),
    )

    perf_xlsx = os.path.join(perf_dir, "performance_quantum_grover_noiseless.xlsx")
    write_excel(
        {
            "raw_runs": perf_df.sort_values(["N","trial"]),
            "summary": perf_summary.sort_values("N"),
        },
        perf_xlsx,
    )
    plot_runtime_vs_N(perf_df, os.path.join(perf_dir, "runtime_vs_N.png"))
    plot_memory_vs_N(perf_df, os.path.join(perf_dir, "memory_vs_N.png"))
    plot_iterations_vs_N(perf_df, os.path.join(perf_dir, "iterations_vs_N.png"))

    # -----------------------
    # ------------********------------ RELIABILITY
    # -----------------------
    reli_xlsx = os.path.join(reli_dir, "reliability_quantum_grover_noiseless.xlsx")
    write_excel(
        {
            "runs": perf_df[[
                "N","n_bits","trial",
                "iterations","queries","queries_theory","queries_error",
                "p_success_theory","p_success_sim","p_success_meas",
                "p_error_sim","p_error_meas","success",
            ]].sort_values(["N","trial"]),
        },
        reli_xlsx,
    )
    plot_p_success_vs_N(perf_df, os.path.join(reli_dir, "p_success_vs_N.png"))
    plot_p_error_hist(perf_df, os.path.join(reli_dir, "p_error_hist.png"))

    # -----------------------
    # ------------********------------ SCALABILITY
    # -----------------------
    scal_xlsx = os.path.join(scal_dir, "scalability_quantum_grover_noiseless.xlsx")

    sub = perf_df.groupby("N", as_index=False)["runtime_sec"].mean()
    x = np.log(sub["N"].values + 1e-16)
    y = np.log(sub["runtime_sec"].values + 1e-16)
    alpha, _ = np.polyfit(x, y, 1)
    scaling_fit = pd.DataFrame({
        "fit_parameter": ["alpha_dim_scaling"],
        "estimate": [alpha],
        "note": ["runtime ~ N^alpha (noiseless quantum Grover)"],
    })

    write_excel(
        {
            "perf_raw": perf_df.sort_values(["N","trial"]),
            "scaling_fit": scaling_fit,
        },
        scal_xlsx,
    )
    plot_scaling_runtime_vs_N_loglog(perf_df, os.path.join(scal_dir, "runtime_vs_N_loglog.png"))

    # -----------------------
    # ------------********------------ CARBON Footprints
    # -----------------------
    desktop = os.path.join(os.environ.get("USERPROFILE",""), "Desktop")
    defaults = [
        "Filtered CO2 intensity 236 Countries",
        "countries_co2",
        "co2_countries",
    ]
    co2_path = resolve_desktop_file(excel_filename, defaults)

    carbon_df = None
    carbon_meta_df = None

    if co2_path and os.path.isfile(co2_path):
        print(f"[INFO] Using CO2 file: {co2_path}")
        co2_df = load_country_co2(co2_path, year_select=year_select)
        carbon_df = compute_emissions(
            perf_df,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        worst_table, meta = build_worstcase_country_table(
            perf_df,
            co2_df,
            power_watts=device_power_watts,
            pue=pue,
        )
        carbon_meta_df = pd.DataFrame([meta])
        carb_xlsx = os.path.join(carb_dir, "carbon_quantum_grover_noiseless.xlsx")
        write_excel(
            {
                "co2_by_country": (co2_df.rename(columns={"kg_per_kwh":"kgco2_per_kwh"})
                                   if "kg_per_kwh" in co2_df.columns else co2_df),
                "emissions_all": carbon_df.sort_values(["N","country"]),
                "worst_case_table": worst_table,
                "worst_case_metadata": carbon_meta_df,
            },
            carb_xlsx,
        )
        plot_carbon_distribution(worst_table, os.path.join(carb_dir, "carbon_distribution.png"))
        plot_carbon_topN(worst_table, 15, os.path.join(carb_dir, "carbon_top15.png"))
        plot_carbon_median_vs_dimension(carbon_df, os.path.join(carb_dir, "median_emissions_vs_N.png"))
    else:
        print("[WARN] CO2 file not found; writing placeholder carbon workbook.")
        carb_xlsx = os.path.join(carb_dir, "carbon_placeholder_quantum_grover_noiseless.xlsx")
        placeholder = pd.DataFrame({
            "note": ["CO2 file not provided or not found on Desktop."],
            "hint": ["Place 'Filtered CO2 intensity 236 Countries.xlsx' on Desktop "
                     "or pass full path via excel_filename."],
        })
        write_excel({"info": placeholder}, carb_xlsx)

    print("\n[DONE] Noiseless quantum Grover benchmark complete.")
    print(f"Performance Excel:  {perf_xlsx}")
    print(f"Scalability Excel:  {scal_xlsx}")
    print(f"Reliability Excel:  {reli_xlsx}")
    print(f"Carbon Excel:       {os.path.join(carb_dir, 'carbon_quantum_grover_noiseless.xlsx')}")
    print("Folders under:", root)
    print("  - Performance")
    print("  - Scalability")
    print("  - Reliability")
    print("  - Carbon")

    return {
        "perf_df": perf_df,
        "perf_dir": perf_dir,
        "scal_dir": scal_dir,
        "reli_dir": reli_dir,
        "carb_dir": carb_dir,
        "carbon_df": carbon_df,
        "carbon_summary": carbon_meta_df,
    }
