#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------
# GPU-accelerated Empirical Evaluation: Quantum Walk 1D
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

"""8 Quantum Walk on GPU

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/8-quantum-walk.61ebac5b-1dc7-4221-b9ab-b078e75e0a8d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251117/auto/storage/goog4_request%26X-Goog-Date%3D20251117T140900Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3fb47420c4a3d7010cc89efa11065af4cd67aaebb80c73b248acad17e889c445ed60bcb24b16094b81b4ddaf64e3264a8266a1b96d79901daf5d03ba285797e788c9b7370741038c6b80ca4c2f45997ea2bf927afdf5ac495ab77027824c7e78c731bf3f38d45c74ad1b5bc795c370be1b3c49a1091c6f090b89d59cbc98c85c886a8ecacc0c2dc8f8f4991d39b061f9479f0e0c04db2cd09b8775b46770194768bfc53604d93dcd5d88daada3ba4361ee3f7598346ff334544d0b2b990e60b7a26bb1500664b2219049ba8c8030af20c499ac8e2f8d7233137af9cfb9de2215c0dab5a2658ab445121f334dd703d5a21889604a84b4459d4534fc8967c6d432

"""


"""
GPU-accelerated Empirical Evaluation: Quantum Walk 1D (Noiseless) ‚Äì Quantum Baseline

This script mirrors the classical baseline's parameters, metrics, and evaluation strategy.
It produces four benchmarks:
  1) Performance     ‚Äì runtime, memory, accuracy (TV distance vs dense-unitary reference)
  2) Scalability     ‚Äì runtime & memory vs problem size
  3) Reliability     ‚Äì stability of accuracy across seeds
  4) Carbon footprint‚Äì CO‚ÇÇ based solely on Performance runtimes

Folders created in the current directory:
  Performance/, Scalability/, Reliability/, Carbon footprints/<outdir>/
"""


# ------------********------------ Imports

import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------********------------
# GPU Setup and Dependencies
# ------------********------------

print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
#This section of code defines GPU utility helpers for identifying the active compute device.
#It provides a safe cleanup routine to release GPU memory without raising errors.
#It initializes a global lock to ensure thread-safe access to GPU resources.

def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

#def run_experiment_thread_safe(args): wraps a single experiment run with 
#       a mutex to ensure thread-safe GPU usage.
#It prevents concurrent threads from accessing GPU resources at the same time.
#It delegates the actual experiment execution to the underlying worker function.

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    N, seed = args
    with _gpu_lock:  # Ensure only one thread uses GPU at a time
        return _run_single_experiment(N, seed)

# ------------********------------
# ========================= Quantum walk core
# ------------********------------

#def hadamard_coin(): constructs and returns the 2√ó2 Hadamard coin matrix.

def hadamard_coin():
    inv = 1.0 / math.sqrt(2.0)
    return np.array([[inv, inv],
                     [inv, -inv]], dtype=np.complex128)

#def initial_coin_vec_from_seed() selects a deterministic, normalized initial coin state based on the given seed.
#It constructs several canonical coin states using basis vectors and the Hadamard transform.
#It ensures reproducible but varied initial conditions across multiple trials.

def initial_coin_vec_from_seed(seed: int) -> np.ndarray:
    """
    Deterministic set of normalized coin states; seed picks one.
    Keeps 'trials' meaningful while staying noiseless.
    """
    H = hadamard_coin()
    L = np.array([1.0, 0.0], dtype=np.complex128)
    R = np.array([0.0, 1.0], dtype=np.complex128)
    plus  = H @ R           # (|L>+|R>)/sqrt2
    minus = H @ L           # (|L>-|R>)/sqrt2
    ip    = (L + 1j*R) / math.sqrt(2.0)
    im    = (L - 1j*R) / math.sqrt(2.0)
    bank = [L, R, plus, minus, ip, im]
    return bank[seed % len(bank)]

#def apply_coin_all_sites function applies the coin operator to the coin subspace at every lattice site.
#It updates the quantum walk state in place 
#      using explicit component-wise operations.
#It avoids additional memory allocations by 
#      reusing temporary copies of coin amplitudes.

def apply_coin_all_sites(psi: np.ndarray, H: np.ndarray):
    """
    psi shape: (N, 2) complex; apply 2x2 coin to coin subspace at each site.
    Row-wise multiply by H^T (since rows are site states, columns coin components).
    """
    # psi[:, :] = psi @ H.T  (explicit to avoid extra allocs)
    left  = psi[:, 0].copy()
    right = psi[:, 1].copy()
    psi[:, 0] = H[0,0]*left + H[1,0]*right
    psi[:, 1] = H[0,1]*left + H[1,1]*right



#def shift_reflecting function performs a reflecting boundary shift for a coined quantum walk.
#It moves left and right coin components inward while reflecting amplitudes at the edges.

def shift_reflecting(psi: np.ndarray):
    """
    Reflecting hard-wall shift (unitary permutation):
      interior: |L,i> -> |L,i-1>, |R,i> -> |R,i+1>
      left wall:  |L,0>   -> |R,0>
      right wall: |R,N-1> -> |L,N-1>
    """
    N = psi.shape[0]
    new = np.zeros_like(psi)
    # interior left
    if N > 1:
        new[0:N-1, 0] += psi[1:N, 0]      # from i=1..N-1 (L) -> i-1
        new[1:N,   1] += psi[0:N-1, 1]    # from i=0..N-2 (R) -> i+1
    # reflections
    new[0, 1]     += psi[0, 0]            # L at 0 -> R at 0
    new[N-1, 0]   += psi[N-1, 1]          # R at N-1 -> L at N-1
    return new

#evolve_iterative function evolves a coined quantum walk state iteratively for a fixed number of steps.
def evolve_iterative(N: int, T: int, seed: int):
    """
    Efficient iterative evolution: (H‚äóI) then reflecting shift, T times.
    Returns final psi (N,2).
    """
    H = hadamard_coin()
    psi = np.zeros((N, 2), dtype=np.complex128)
    pos0 = N // 2
    psi[pos0, :] = initial_coin_vec_from_seed(seed)

    for _ in range(T):
        apply_coin_all_sites(psi, H)
        psi = shift_reflecting(psi)
    return psi

#def build_dense_U constructs the coined quantum-walk step operator U by combining a block-diagonal Hadamard coin C with a reflecting shift S.

def build_dense_U(N: int) -> np.ndarray:
    """
    Build global step operator U = S @ (C), where
      C = ‚äï_{i=0}^{N-1} H (block-diagonal Hadamard),
      S = reflecting shift permutation in the {|L,i>, |R,i>} basis.
    Basis ordering: [(L,0),(R,0),(L,1),(R,1),..., (L,N-1),(R,N-1)]
    """
    H = hadamard_coin()
    d = 2 * N
    C = np.zeros((d, d), dtype=np.complex128)
    for i in range(N):
        C[2*i:2*i+2, 2*i:2*i+2] = H

    S = np.zeros((d, d), dtype=np.complex128)

    def idx(i, coin):  # coin 0=L, 1=R
        return 2*i + coin

    # interior shifts
    for i in range(1, N):      # L,i -> L,i-1
        S[idx(i-1, 0), idx(i, 0)] = 1.0
    for i in range(0, N-1):    # R,i -> R,i+1
        S[idx(i+1, 1), idx(i, 1)] = 1.0

    # reflections
    S[idx(0,   1), idx(0,   0)] = 1.0      # L,0 -> R,0
    S[idx(N-1, 0), idx(N-1, 1)] = 1.0      # R,N-1 -> L,N-1

    U = S @ C
    return U

#def evolve_dense_reference () function simulates T time-steps of a coined quantum walk by 
#    repeatedly applying the dense unitary U to the state vector psi.
#It initializes psi at the middle position with a seed-determined coin superposition 
#    and keeps the full 2N-dimensional complex state.
#After evolution, it reshapes the final state into (N, 2) so 
#    each site stores its [L, R] coin amplitudes.

def evolve_dense_reference(N: int, T: int, seed: int, U: np.ndarray):
    """
    Reference evolution using dense unitary U: psi <- U^T psi.
    """
    d = 2 * N
    psi = np.zeros(d, dtype=np.complex128)
    pos0 = N // 2
    coin = initial_coin_vec_from_seed(seed)   # [L,R]
    psi[2*pos0:2*pos0+2] = coin
    for _ in range(T):
        psi = U @ psi
    # reshape to (N,2)
    return psi.reshape(N, 2)

def position_marginal(psi: np.ndarray) -> np.ndarray:
    """Return p[i] = |psi_L(i)|^2 + |psi_R(i)|^2."""
    return (np.abs(psi[:, 0])**2 + np.abs(psi[:, 1])**2).astype(np.float64)

# ------------********------------
# ========================= Metrics
# ------------********------------

#def tv_distance computes the total variation distance between two probability distributions 
#   as 0.5 * sum(|p‚àíq|), giving a scalar divergence measure.

def tv_distance(p: np.ndarray, q: np.ndarray) -> float:
    return 0.5 * float(np.abs(p - q).sum())


#def _run_single_experiment runs one quantum-walk trial: 
#     builds a dense reference unitary, evolves both iterative and dense versions, and compares their position marginals.
#It profiles runtime and peak memory via tracemalloc, performs GPU cleanup, 
#     and returns a metrics dictionary including TV distance, correctness vs tolerance, and device info.

def _run_single_experiment(N: int, seed: int) -> dict:
    """
    Core experiment logic - run single quantum walk experiment.
    """
    T = 1000  # Fixed number of steps
    tv_tol = 1e-12  # Fixed tolerance

    # Build dense unitary matrix for reference
    U = build_dense_U(N)

    tracemalloc.start()
    t0 = time.perf_counter()

    # Run both iterative and dense evolution
    psi_iter = evolve_iterative(N, T, seed)
    psi_ref  = evolve_dense_reference(N, T, seed, U)

    # Calculate position marginals
    p_iter = position_marginal(psi_iter)
    p_ref  = position_marginal(psi_ref)

    # Calculate TV distance
    tv = tv_distance(p_iter, p_ref)

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "n": N,
        "steps": T,
        "seed": seed,
        "tv_distance": float(tv),
        "correct": bool(tv <= tv_tol),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "device": get_device_name()
    }

# ------------********------------
# ============================== Carbon I/O
# ------------********------------

def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, we can convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------
# ============================== Plot Helpers ==============================
# ------------********------------

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

# ------------********------------
def plot_performance(df: pd.DataFrame, outdir: str):
    g = df.groupby("n")["runtime_s"].mean().reset_index()
    plt.figure()
    plt.plot(g["n"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Performance: Runtime vs Lattice Size\n{get_device_name()}")
    plt.xlabel("Lattice Size N (positions)")
    plt.ylabel("Runtime (s)")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_n.png"), **plt_kwargs)
    plt.close()

    g2 = df.groupby("n")["tv_distance"].mean().reset_index()
    plt.figure()
    plt.plot(g2["n"], g2["tv_distance"], marker="s", linewidth=2, markersize=8)
    plt.title(f"Performance: Mean TV Distance vs N\n{get_device_name()}")
    plt.xlabel("Lattice Size N (positions)")
    plt.ylabel("TV Distance")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_tv_vs_n.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_scalability(df: pd.DataFrame, outdir: str):
    g = df.groupby("n")["peak_mem_mb"].mean().reset_index()
    plt.figure()
    plt.plot(g["n"], g["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.title(f"Scalability: Memory vs Lattice Size\n{get_device_name()}")
    plt.xlabel("Lattice Size N (positions)")
    plt.ylabel("Memory (MB)")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_n.png"), **plt_kwargs)
    plt.close()

    g2 = df.groupby("n")["runtime_s"].mean().reset_index()
    plt.figure()
    plt.loglog(g2["n"], g2["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Scalability: Runtime (log-log)\n{get_device_name()}")
    plt.xlabel("Lattice Size N")
    plt.ylabel("Runtime (s)")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_loglog_runtime.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_reliability(df: pd.DataFrame, outdir: str):
    data = [df[df["n"] == n]["tv_distance"].values for n in sorted(df["n"].unique())]
    plt.figure()
    _boxplot_with_labels(data, labels=sorted(df["n"].unique()))
    plt.title(f"Reliability: TV Distance Distribution by N\n{get_device_name()}")
    plt.xlabel("Lattice Size N (positions)")
    plt.ylabel("TV Distance")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_tv_boxplot.png"), **plt_kwargs)
    plt.close()

    g = df.groupby("n")["correct"].mean().reset_index()
    plt.figure()
    plt.plot(g["n"], g["correct"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Reliability: Success Rate vs N\n{get_device_name()}")
    plt.xlabel("Lattice Size N")
    plt.ylabel("Success Rate")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_success_vs_n.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------
# ============================== Utility Functions
# ------------********------------

#def ensure_dir ensures a target directory path exists by 
# creating it (and any missing parents) without error if already present.

def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)

#def to_excel writes a collection of named pandas DataFrames to a single Excel workbook, placing each DataFrame on its own sheet.
#It uses openpyxl via ExcelWriter and truncates sheet names to 31 characters to satisfy Excel‚Äôs sheet-name length limit.

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    """Save multiple DataFrames to Excel with different sheets"""
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------
# ============================== Main Experiment Runner code
# ------------********------------

def run_quantum_walk_experiment():
    """
    Main Quantum Walk experiment runner with hardcoded parameters for Kaggle.
    Equivalent to: --sizes 21 --steps 1000 --trials 10 --method quantum_walk
                   --device-power-watts 65 --pue 1.2 --combine
    """
    # Experiment parameters
    sizes = [21]  # Lattice sizes (positions)
    trials = 10   # trials per size
    steps = 1000  # time steps
    method = "quantum_walk"
    workers = 4   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting Quantum Walk experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Lattice sizes: {sizes}")
    print(f"   Time steps: {steps}")
    print(f"   Trials per size: {trials}")
    print(f"   Method: {method}")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for N in sizes:
        for i in range(trials):
            jobs.append((N, 1000 + 17 * N + i))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        print(f"  ‚úÖ N={row['n']}, seed={row['seed']} (runtime={row['runtime_s']:.6f}s, TV={row['tv_distance']:.2e}, correct={row['correct']})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed N={job[0]}, seed={job[1]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("‚ùå All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1])
                _consume(result)
            except Exception as e:
                print(f"‚ùå Sequential fallback also failed for N={job[0]}, seed={job[1]}: {e}")

    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------
    # ---------------- Performance Results ----------------

    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")
    perf_agg = perf_df.groupby("n").agg({
        "runtime_s": "mean",
        "tv_distance": "mean",
        "peak_mem_mb": "mean",
        "correct": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------
    # ---------------- Scalability Results ----------------

    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")
    scal_agg = perf_df.groupby("n").agg({
        "runtime_s": "mean",
        "peak_mem_mb": "mean",
        "tv_distance": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "from_perf_runs": perf_df[["n", "seed", "runtime_s", "peak_mem_mb", "tv_distance"]],
            "aggregated": scal_agg
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------
    # ---------------- Reliability Results ----------------

    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "correctness_analysis": perf_df[["n", "seed", "tv_distance", "correct"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------
    # ---------------- Carbon Footprint Results ----------------

    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["n", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    print("\n" + "="*60)
    print("üéâ QUANTUM WALK EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {len(perf_df)}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¨ Lattice sizes:   {sorted(perf_df['n'].unique())}")
    print(f"üéØ Success rate:    {perf_df['correct'].mean() * 100:.1f}%")
    print(f"üìè Mean TV distance: {perf_df['tv_distance'].mean():.2e}")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ============================================================
#                       Main execution for Kaggle
# ============================================================

#This main block runs the full quantum-walk benchmark in a Kaggle environment, 
#         starting with a GPU cleanup and a clear console header.
#It executes run_quantum_walk_experiment(), then prints a summary of outcomes 
#         (counts, runtimes, success rate, TV distance, and device used).
#If results exist, it walks the /kaggle/working directory to display only 
#     relevant output folders and generated .xlsx/.png deliverables; 
#     otherwise it reports failure.

if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting Quantum Walk Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_quantum_walk_experiment()

    if results_df is not None:
        # Display final results summary
        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {len(results_df)}")
        print(f"Lattice sizes tested: {sorted(results_df['n'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {results_df['correct'].mean() * 100:.1f}%")
        print(f"Average TV distance: {results_df['tv_distance'].mean():.2e}")
        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\n‚úÖ All Quantum Walk deliverables generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå Quantum Walk benchmark failed - no results generated")

