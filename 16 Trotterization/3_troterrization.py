#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------

# GPU-accelerated Quantum Hamiltonian Simulation: Trotterization (Noiseless) - COMPLETELY FIXED VERSION
# Completely rewritten gate application functions to fix "repeated axis in transpose" error.

"""15 Troterrization
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/15-troterrization.5dd13e07-b211-4704-8048-68539607c185.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251121/auto/storage/goog4_request%26X-Goog-Date%3D20251121T115704Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0947502232832dd8160682a0fd6d2f5ca4da57f6d16224495d3f175e2ae9efa394bd0e633478fd125f46efd44932ed33b6c6a810f6863db98fd66e6cc81cd959bfcca82dbd411a507b5a78a80ed6677456fe93c072b68b031ea2324d42c01a95a35a8e5f60bcb984359953d09fbb26993db2138ca32f767c7e17c251a9a53cbe31a83ba46651ea0092ef8f0d67a4f0f1f29e71b77a32c7c64a37d79a944d5f08f5ee61f7ea6cc53877b5321b5ff473697220b3fb65a251eb909a4af9b8cf87d56811cbab93e3ea807235e34f4fb29f24f484ef117a362d66271ca8b60400ed4b846de2e94b80345e59554c65142a9a2805da45fa5e70e16f5812e641839073cf
"""

"""GPU Quantum Hamiltonian Simulation - Trotterization (COMPLETELY FIXED)
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/10-qnn.e7383569-e5d3-4bf6-9e71-ed3b260100dc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251117/auto/storage/goog4_request%26X-Goog-Date%3D20251117T180825Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D56d2a9911953f68cfe48fdecffdb1582ff47311d0bf8587d25ecfa284199db3cbc6e7486ad3e5046833b8d7d65da13286a5586bb7ad1f6be5bed801823859ee02369f0850b8857fef8c571715d6daf45c8cba197040d0f62689aa02ec4d4852d619b84a0380514c11d746bb4032ae1d1ebf4579a2dba9b99ea951ae533ab484e34b5bf4dc9a57b758519711ab569913ee37c742a90d51ffc0aaf6d25fc90a2b4afc273ad0da63ec387120cba9906d469a7e9879d937df5e4c39a59a10adda227b97f64d71b78840f44d2dc40e8c5af0beeb6995110e59f53890d6d9adf91414dde4b0c45b9a64d476ddd293378aa0ca01b42e9b53665f334616a86b1b7a3ae2a
"""

# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------


# ------------********------------ Imports

import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================
# GPU Setup and Dependencies
# ============================================================
print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    n_qubits, trotter_steps, seed = args
    with _gpu_lock:  # Ensure only one thread uses GPU at a time
        return _run_single_experiment(n_qubits, trotter_steps, seed)


# ------------********------------
# ------------********------------ Quantum Hamiltonian Simulation Core
# ------------********------------

#This function defines the four standard Pauli matrices used in quantum mechanics.
#Each matrix is constructed explicitly as a 2√ó2 complex NumPy array.
#The identity and X, Y, Z operators are returned together for convenient reuse.

def paulis():
    """Return Pauli matrices I, X, Y, Z"""
    I = cp.array([[1, 0], [0, 1]], dtype=cp.complex128)
    X = cp.array([[0, 1], [1, 0]], dtype=cp.complex128)
    Y = cp.array([[0, -1j], [1j, 0]], dtype=cp.complex128)
    Z = cp.array([[1, 0], [0, -1]], dtype=cp.complex128)
    return I, X, Y, Z

#This following function computes the matrix exponential of a Hermitian operator using eigendecomposition.
#Eigenvalues are exponentiated to form phase factors, ensuring numerical stability.
#The result reconstructs the unitary evolution operator via the eigenbasis.

def expm_hermitian(H: cp.ndarray, dt: float) -> cp.ndarray:
    if HAS_GPU:
        # CuPy eigh
        w, V = cp.linalg.eigh(H)
        phase = cp.exp(-1j * dt * w)
        return (V * phase) @ V.conj().T
    else:
        # NumPy fallback
        w, V = np.linalg.eigh(H.get() if hasattr(H, 'get') else H)
        phase = np.exp(-1j * dt * w)
        return cp.asarray((V * phase) @ V.conj().T)

def two_qubit_heisenberg_gate(J: float, dt: float) -> cp.ndarray:

   #  U_2q = exp(-i dt J (XX + YY + ZZ)) on a 2-qubit subspace.
    _, X, Y, Z = paulis()
    H_pair = J * (cp.kron(X, X) + cp.kron(Y, Y) + cp.kron(Z, Z))
    return expm_hermitian(H_pair, dt)

def one_qubit_z_field_gate(h: float, dt: float) -> cp.ndarray:
    """U_1q = exp(-i dt h Z)."""
    _, _, _, Z = paulis()
    return expm_hermitian(h * Z, dt)


    # Apply 1-qubit unitary U to qubit i (0-indexed), statevector length 2^n.
    # Completely rewritten to avoid axis conflicts.
def apply_1q(U: cp.ndarray, psi: cp.ndarray, n: int, i: int) -> cp.ndarray:
   
    # Reshape to tensor
    psi_tensor = psi.reshape([2] * n)

    # Create new axis order: target qubit first, then all others in order
    axes_order = [i] + [j for j in range(n) if j != i]

    # Move target qubit to first position
    psi_moved = cp.transpose(psi_tensor, axes_order)

    # Apply unitary to first dimension
    original_shape = psi_moved.shape
    psi_transformed = (U @ psi_moved.reshape(2, -1)).reshape(original_shape)

    # Create inverse axis order to restore original positions
    inverse_order = [0] * n
    for new_pos, old_pos in enumerate(axes_order):
        inverse_order[old_pos] = new_pos

    # Restore original axis order
    psi_restored = cp.transpose(psi_transformed, inverse_order)

    return psi_restored.reshape(-1)

    
    # Apply 2-qubit unitary U to neighboring qubits (i, i+1).
    # Completely rewritten to avoid axis conflicts.
def apply_2q_nn(U: cp.ndarray, psi: cp.ndarray, n: int, i: int) -> cp.ndarray:

    # Reshape to tensor
    psi_tensor = psi.reshape([2] * n)

    # Create new axis order: target qubits first, then all others in order
    axes_order = [i, i+1] + [j for j in range(n) if j not in [i, i+1]]

    # Move target qubits to first two positions
    psi_moved = cp.transpose(psi_tensor, axes_order)

    # Apply unitary to first two dimensions
    original_shape = psi_moved.shape
    psi_transformed = (U @ psi_moved.reshape(4, -1)).reshape(original_shape)

    # Create inverse axis order to restore original positions
    inverse_order = [0] * n
    for new_pos, old_pos in enumerate(axes_order):
        inverse_order[old_pos] = new_pos

    # Restore original axis order
    psi_restored = cp.transpose(psi_transformed, inverse_order)

    return psi_restored.reshape(-1)

def build_full_hamiltonian(n: int, J: float, h: float) -> cp.ndarray:
    """Dense 2^n x 2^n H used only to compute the exact evolution for error."""
    I, X, Y, Z = paulis()

    def single_qubit_op(i: int, op: cp.ndarray) -> cp.ndarray:
        mats = [I] * n
        mats[i] = op
        out = mats[0]
        for m in mats[1:]:
            out = cp.kron(out, m)
        return out

    def two_qubit_op(i: int, j: int, op_i: cp.ndarray, op_j: cp.ndarray) -> cp.ndarray:
        mats = [I] * n
        mats[i] = op_i
        mats[j] = op_j
        out = mats[0]
        for m in mats[1:]:
            out = cp.kron(out, m)
        return out

    H = cp.zeros((2**n, 2**n), dtype=cp.complex128)
    for i in range(n - 1):
        H += J * (two_qubit_op(i, i + 1, X, X) +
                  two_qubit_op(i, i + 1, Y, Y) +
                  two_qubit_op(i, i + 1, Z, Z))
    for i in range(n):
        H += h * single_qubit_op(i, Z)
    return H

def simulate_trotter_ideal(n: int, J: float, h: float, m_steps: int, T: float, psi0: cp.ndarray) -> cp.ndarray:
    """First-order Trotter using gate application on a statevector (ideal/noiseless)."""
    dt = T / m_steps
    U2 = two_qubit_heisenberg_gate(J, dt)     # shared across all pairs
    U1 = one_qubit_z_field_gate(h, dt)        # shared across all qubits

    psi = psi0.copy()
    for step in range(m_steps):
        # even bonds: (0,1), (2,3), ...
        for i in range(0, n - 1, 2):
            psi = apply_2q_nn(U2, psi, n, i)
        # odd bonds: (1,2), (3,4), ...
        for i in range(1, n - 1, 2):
            psi = apply_2q_nn(U2, psi, n, i)
        # on-site Z fields
        for i in range(n):
            psi = apply_1q(U1, psi, n, i)
    return psi

def simulate_exact_dense(n: int, J: float, h: float, T: float, psi0: cp.ndarray) -> cp.ndarray:
    """Exact evolution via dense expm, used only to quantify error."""
    H = build_full_hamiltonian(n, J, h)
    U = expm_hermitian(H, T)
    return U @ psi0

def l2_error(a: cp.ndarray, b: cp.ndarray) -> float:
    """Compute L2 error between two statevectors"""
    if HAS_GPU:
        return float(cp.linalg.norm(a - b))
    else:
        return float(np.linalg.norm(a - b))

def _run_single_experiment(n_qubits: int, trotter_steps: int, seed: int) -> dict:
    """
    Core experiment logic - run single quantum Hamiltonian simulation experiment.
    """
    # Fixed parameters
    J = 1.0
    h = 0.5
    T = 1.0

    # Initialize random state
    rng = np.random.default_rng(seed)
    dim = 2 ** n_qubits

    # Generate random normalized statevector
    psi0_real = rng.standard_normal(dim)
    psi0_imag = rng.standard_normal(dim)
    psi0 = psi0_real + 1j * psi0_imag
    psi0 = psi0 / np.linalg.norm(psi0)

    # Convert to GPU array if available
    psi0_gpu = cp.asarray(psi0)

    tracemalloc.start()
    t0 = time.perf_counter()

    # Run Trotter simulation
    try:
        psi_trotter = simulate_trotter_ideal(n_qubits, J, h, trotter_steps, T, psi0_gpu)

        # For reliability, compute exact solution and error
        error = 0.0
        if n_qubits <= 8:  # Limit exact computation to avoid memory issues
            try:
                psi_exact = simulate_exact_dense(n_qubits, J, h, T, psi0_gpu)
                error = l2_error(psi_trotter, psi_exact)
            except Exception as e:
                print(f"‚ö†Ô∏è  Exact computation failed for n={n_qubits}: {e}")
                error = -1.0  # Mark as failed

        success = error >= 0 and (error < 0.1 if n_qubits <= 8 else True)

    except Exception as e:
        print(f"‚ùå Trotter simulation failed for n={n_qubits}, steps={trotter_steps}: {e}")
        error = -1.0
        success = False
        psi_trotter = psi0_gpu  # Fallback

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "n_qubits": int(n_qubits),
        "trotter_steps": int(trotter_steps),
        "dimension": int(dim),
        "seed": int(seed),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "error": float(error),
        "success": bool(success),
        "device": get_device_name()
    }

# ============================== Carbon I/O ==============================

def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ============================== Plot Helpers ==============================

#These plotting functions visualize performance and accuracy of Trotterized simulations.
#They generate log-scaled runtime, memory, and error trends across increasing Trotter steps and qubit counts.
#A heatmap summarizes error behavior over the full parameter grid for quick comparison.
#All figures are saved using a shared helper to ensure consistent layout and formatting.

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    """Plot performance metrics"""
    # Runtime vs qubits for different steps
    plt.figure(figsize=(12, 8))
    for steps in sorted(df['trotter_steps'].unique()):
        sub_df = df[df['trotter_steps'] == steps]
        if len(sub_df) > 0:
            agg_df = sub_df.groupby('n_qubits')['runtime_s'].mean().reset_index()
            plt.plot(agg_df['n_qubits'], agg_df['runtime_s'], marker='o', linewidth=2, label=f'Steps={steps}')

    plt.title(f"Performance: Runtime vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits")
    plt.ylabel("Runtime (s)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Memory vs qubits
    plt.figure(figsize=(12, 8))
    for steps in sorted(df['trotter_steps'].unique()):
        sub_df = df[df['trotter_steps'] == steps]
        if len(sub_df) > 0:
            agg_df = sub_df.groupby('n_qubits')['peak_mem_mb'].mean().reset_index()
            plt.plot(agg_df['n_qubits'], agg_df['peak_mem_mb'], marker='s', linewidth=2, label=f'Steps={steps}')

    plt.title(f"Performance: Memory vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits")
    plt.ylabel("Memory (MB)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_memory_vs_qubits.png"), **plt_kwargs)
    plt.close()

def plot_scalability(df: pd.DataFrame, outdir: str):
    """Plot scalability metrics"""
    # Runtime scaling with dimension
    df['dimension'] = 2 ** df['n_qubits']

    plt.figure(figsize=(10, 6))
    plt.loglog(df['dimension'], df['runtime_s'], 'bo', alpha=0.7, markersize=8)
    plt.title(f"Scalability: Runtime vs Hilbert Space Dimension\n{get_device_name()}")
    plt.xlabel("Hilbert Space Dimension (2^n)")
    plt.ylabel("Runtime (s)")
    plt.grid(True, which="both", alpha=0.3)
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_dimension.png"), **plt_kwargs)
    plt.close()

    # Memory scaling with dimension
    plt.figure(figsize=(10, 6))
    plt.loglog(df['dimension'], df['peak_mem_mb'], 'go', alpha=0.7, markersize=8)
    plt.title(f"Scalability: Memory vs Hilbert Space Dimension\n{get_device_name()}")
    plt.xlabel("Hilbert Space Dimension (2^n)")
    plt.ylabel("Memory (MB)")
    plt.grid(True, which="both", alpha=0.3)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_dimension.png"), **plt_kwargs)
    plt.close()

def plot_reliability(df: pd.DataFrame, outdir: str):
    """Plot reliability metrics"""
    # Error distribution by qubit count (for small n where exact computation is feasible)
    small_n_df = df[(df['n_qubits'] <= 8) & (df['error'] >= 0)]
    if len(small_n_df) > 0:
        data = [small_n_df[small_n_df["n_qubits"] == n]["error"].values
                for n in sorted(small_n_df["n_qubits"].unique())]
        plt.figure(figsize=(10, 6))
        _boxplot_with_labels(data, labels=sorted(small_n_df["n_qubits"].unique()))
        plt.title(f"Reliability: Error Distribution by Qubit Count\n{get_device_name()}")
        plt.xlabel("Number of Qubits")
        plt.ylabel("L2 Error")
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(outdir, "rel_error_distribution.png"), **plt_kwargs)
        plt.close()

    # Success rate
    success_rates = df.groupby('n_qubits')['success'].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(success_rates['n_qubits'], success_rates['success'], marker='o', linewidth=2, markersize=8)
    plt.title(f"Reliability: Success Rate vs Qubit Count\n{get_device_name()}")
    plt.xlabel("Number of Qubits")
    plt.ylabel("Success Rate")
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1.1)
    plt.savefig(os.path.join(outdir, "rel_success_rate.png"), **plt_kwargs)
    plt.close()

def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********----------
# ------------********---------- Utility Functions ==============================
# ------------********----------

#These following utility functions handle filesystem setup, robust result export, and figure saving.
#   Excel output is with a CSV fallback for error resilience.
#       Directory creation and plotting helpers ensure clean output generation and resource cleanup.

def ensure_dir(d: str):
    os.makedirs(d, exist_ok=True)

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********----------
# ------------********---------- Main Experiment Runner ==============================
# ------------********----------
def run_quantum_hamiltonian_experiment():
    
    # Experiment parameters
    n_min = 2
    n_max = 6
    trotter_steps_list = [1, 2, 4, 8, 16]
    trials = 3      # trials per configuration
    J = 1.0
    h = 0.5
    T = 1.0
    method = "quantum_hamiltonian"
    workers = 4     # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting Quantum Hamiltonian Simulation experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Qubits: {n_min} to {n_max}")
    print(f"   Trotter steps: {trotter_steps_list}")
    print(f"   Trials per config: {trials}")
    print(f"   J (Heisenberg): {J}")
    print(f"   h (field): {h}")
    print(f"   T (time): {T}")
    print(f"   Method: {method}")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for n in range(n_min, n_max + 1):
        for steps in trotter_steps_list:
            for i in range(trials):
                seed = 1000 + 17 * n + 31 * steps + i
                jobs.append((n, steps, seed))

    all_rows = []
# ------------********----------# ------------********----------# ------------********----------
    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        error_str = f"{row['error']:.6f}" if row['error'] >= 0 else "N/A"
        print(f"  ‚úÖ n={row['n_qubits']}, steps={row['trotter_steps']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.6f}s, error={error_str}, success={row['success']})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed n={job[0]}, steps={job[1]}, seed={job[2]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            try:
                result = run_experiment_thread_safe(job)
                _consume(result)
            except Exception as e:
                print(f"‚ùå Job failed n={job[0]}, steps={job[1]}, seed={job[2]}: {e}")

    # Check if we have any successful results
    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------------------- Performance Results ----------------
    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")

    # Aggregate with proper column handling
    perf_agg = perf_df.groupby(["n_qubits", "trotter_steps"]).agg({
        "runtime_s": ["mean", "std"],
        "peak_mem_mb": ["mean", "std"],
        "error": "mean",
        "success": "mean"
    }).round(6)
    perf_agg.columns = ['_'.join(col).strip('_') for col in perf_agg.columns.values]
    perf_agg = perf_agg.reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------------------- Scalability Results ----------------
    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")
    scal_agg = perf_df.groupby("n_qubits").agg({
        "runtime_s": "mean",
        "peak_mem_mb": "mean",
        "dimension": "first"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "from_perf_runs": perf_df[["n_qubits", "trotter_steps", "runtime_s", "peak_mem_mb", "dimension"]],
            "aggregated": scal_agg
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------------------- Reliability Results ----------------
    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "error_analysis": perf_df[["n_qubits", "trotter_steps", "seed", "error", "success"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********-------------------------- Carbon Footprint Results ----------------
    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["n_qubits", "trotter_steps", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

# ------------********---------- Final summary

    print("\n" + "="*60)
    print("üéâ QUANTUM HAMILTONIAN SIMULATION EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {len(perf_df)}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¨ Qubit range:     {perf_df['n_qubits'].min()}-{perf_df['n_qubits'].max()}")
    print(f"üî¨ Trotter steps:   {sorted(perf_df['trotter_steps'].unique())}")
    print(f"üéØ Success rate:    {perf_df['success'].mean() * 100:.1f}%")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ------------********----------
# ------------********---------- Main execution for Kaggle
# ------------********----------
if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting Quantum Hamiltonian Simulation Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_quantum_hamiltonian_experiment()

    if results_df is not None:
        # Display final results summary
        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {len(results_df)}")
        print(f"Qubit range: {results_df['n_qubits'].min()}-{results_df['n_qubits'].max()}")
        print(f"Trotter steps tested: {sorted(results_df['trotter_steps'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {results_df['success'].mean() * 100:.1f}%")
        print(f"Max Hilbert dimension: {2**results_df['n_qubits'].max():,}")
        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\n‚úÖ All Quantum Hamiltonian Simulation deliverables generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå Quantum Hamiltonian Simulation benchmark failed - no results generated")

