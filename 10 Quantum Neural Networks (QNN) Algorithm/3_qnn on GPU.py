#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------
# GPU-accelerated Empirical Evaluation: QNN for XOR (Noiseless) â€“ Quantum Baseline
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 
# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------

"""10 QNN
Automatically generated by Colab.

Original file is located at

    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/10-qnn.e7383569-e5d3-4bf6-9e71-ed3b260100dc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251117/auto/storage/goog4_request%26X-Goog-Date%3D20251117T180825Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D56d2a9911953f68cfe48fdecffdb1582ff47311d0bf8587d25ecfa284199db3cbc6e7486ad3e5046833b8d7d65da13286a5586bb7ad1f6be5bed801823859ee02369f0850b8857fef8c571715d6daf45c8cba197040d0f62689aa02ec4d4852d619b84a0380514c11d746bb4032ae1d1ebf4579a2dba9b99ea951ae533ab484e34b5bf4dc9a57b758519711ab569913ee37c742a90d51ffc0aaf6d25fc90a2b4afc273ad0da63ec387120cba9906d469a7e9879d937df5e4c39a59a10adda227b97f64d71b78840f44d2dc40e8c5af0beeb6995110e59f53890d6d9adf91414dde4b0c45b9a64d476ddd293378aa0ca01b42e9b53665f334616a86b1b7a3ae2a
"""

# ------------********------------ Imports
#These imports provide core utilities for filesystem handling, timing, memory profiling, warnings, and mathematical operations.
#They include dataclass support and rich typing annotations to structure experiment configurations and results.
#Threading and ThreadPoolExecutor enable concurrent execution with thread-level synchronization where needed.

import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


# ------------********------------ 
# GPU Setup and Dependencies
# ------------********------------ 
#This block checks for GPU availability by attempting to import and initialize CuPy on CUDA device 0.
#If successful, it validates basic GPU functionality and reports device properties; otherwise it falls back to NumPy.
#The HAS_GPU flag and cp alias allow downstream code to transparently use either GPU or CPU backends.

print("ğŸ” Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"âœ… GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("âŒ CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"âš ï¸  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("ğŸ”„ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    H, seed = args
    with _gpu_lock:  # Ensure only one thread uses GPU at a time
        return _run_single_experiment(H, seed)

# ------------********------------ 
# ========================= QNN XOR Core
# ------------********------------ 
#def make_xor_dataset generates the XOR classification dataset with optional Gaussian-noise augmentation around each corner point.
#It supports reproducible sampling via a random seed, clips augmented samples 
#   to a bounded range, and returns stacked feature/label arrays.
#I2 defines the 2Ã—2 complex identity matrix, used as a basic building block for single- and two-qubit operators.

def make_xor_dataset(augment_per_corner: int = 0, noise_std: float = 0.05, seed: int | None = None):
    rng = np.random.default_rng(seed)
    X_base = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float64)
    y_base = np.array([[0],[1],[1],[0]], dtype=np.float64)
    if augment_per_corner <= 0:
        return X_base, y_base
    X_list, y_list = [], []
    for i in range(4):
        X_list.append(X_base[i:i+1]); y_list.append(y_base[i:i+1])
        noise = rng.normal(0.0, noise_std, size=(augment_per_corner, 2))
        X_aug = np.clip(X_base[i] + noise, -0.25, 1.25)
        y_aug = np.repeat(y_base[i:i+1], augment_per_corner, axis=0)
        X_list.append(X_aug); y_list.append(y_aug)
    X = np.vstack(X_list); y = np.vstack(y_list)
    return X, y

I2 = np.eye(2, dtype=np.complex128)


#def Ry and def Rz construct single-qubit rotation matrices about the Y and Z axes using half-angle trigonometric definitions.
#They return complex-valued 2Ã—2 unitaries suitable for quantum circuit simulation.

def Ry(theta):
    c = math.cos(theta/2.0); s = math.sin(theta/2.0)
    return np.array([[c, -s],[s, c]], dtype=np.complex128)

def Rz(theta):
    c = math.cos(theta/2.0); s = math.sin(theta/2.0)
    return np.array([[c-1j*s, 0],[0, c+1j*s]], dtype=np.complex128)

#CNOT_01 defines a two-qubit controlled-NOT gate matrix with qubit 0 as control and qubit 1 as target.
CNOT_01 = np.array([
    [1,0,0,0],
    [0,1,0,0],
    [0,0,0,1],
    [0,0,1,0]
], dtype=np.complex128)

#def layer_unitary builds a two-qubit variational layer by composing single-qubit Y-rotations, a fixed CNOT entangler, and Z-rotations.
def layer_unitary(params_layer):
    """Return 4Ã—4 layer unitary for [ry0, ry1, rz0, rz1] with order: RY0, RY1, CNOT, RZ0, RZ1."""
    ry0, ry1, rz0, rz1 = params_layer
    U = np.kron(Ry(ry0), I2)
    U = np.kron(I2, Ry(ry1)) @ U
    U = CNOT_01 @ U
    U = np.kron(Rz(rz0), I2) @ U
    U = np.kron(I2, Rz(rz1)) @ U
    return U

#def circuit_unitary multiplies all layers into one 4Ã—4 unitary, applying newer layers first to match circuit execution order.
def circuit_unitary(params):
    """Compose all H layers to a single 4Ã—4 unitary (left-multiply newest layer)."""
    U = np.eye(4, dtype=np.complex128)
    for layer in params:
        U = layer_unitary(layer) @ U
    return U

#def encode_states_matrix encodes classical 2D inputs into quantum states via Y-rotations on a |00âŸ© reference, returning a matrix of input kets.
def encode_states_matrix(X):
    """Return matrix S_in of shape (4, N): each column is |Ïˆ_in(x)âŸ© = (IâŠ—RY(Ï€x2)) (RY(Ï€x1)âŠ—I) |00âŸ©."""
    N = X.shape[0]
    S = np.zeros((4, N), dtype=np.complex128)
    ket00 = np.zeros(4, dtype=np.complex128); ket00[0] = 1.0
    for i in range(N):
        x1, x2 = float(X[i,0]), float(X[i,1])
        Uenc = np.kron(Ry(math.pi*x1), I2)
        Uenc = np.kron(I2, Ry(math.pi*x2)) @ Uenc
        S[:, i] = Uenc @ ket00
    return S

#def probs_from_unitary applies a two-qubit unitary to all encoded input states and computes the probability of measuring qubit 1 in |1âŸ©.
def probs_from_unitary(U, S_in):
    """Vectorized: S_out = U @ S_in; return p (N,) with p_i = P(q1=1)."""
    S_out = U @ S_in  # (4,N)
    p = (np.abs(S_out[1,:])**2 + np.abs(S_out[3,:])**2).real
    return p

#def bce_loss evaluates binary cross-entropy loss with clipping for numerical stability between predicted probabilities and true labels.
def bce_loss(p, y):
    p = np.asarray(p).reshape(-1)
    y = np.asarray(y).reshape(-1)
    p = np.clip(p, 1e-12, 1-1e-12)
    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))

#def accuracy_from_probs thresholds probabilities at 0.5 to compute classification accuracy against ground-truth labels.
def accuracy_from_probs(p, y):
    p = np.asarray(p).reshape(-1)
    y = np.asarray(y).reshape(-1)
    pred = (p >= 0.5).astype(np.int32)
    return float(np.mean(pred == (y >= 0.5)))


#class Adam implements the Adam optimization algorithm with first- and second-moment estimates for adaptive learning rates.
#It maintains running averages of gradients, applies bias correction each step, and updates parameters in-place.
#The step method advances the timestep and returns the updated parameter array.

class Adam:
    def __init__(self, shape, lr=0.05, beta1=0.9, beta2=0.999, eps=1e-8):
        self.m = np.zeros(shape, dtype=np.float64)
        self.v = np.zeros(shape, dtype=np.float64)
        self.lr = lr; self.b1 = beta1; self.b2 = beta2; self.eps = eps; self.t = 0
    def step(self, params, grad):
        self.t += 1
        self.m = self.b1*self.m + (1-self.b1)*grad
        self.v = self.b2*self.v + (1-self.b2)*(grad*grad)
        m_hat = self.m / (1 - self.b1**self.t)
        v_hat = self.v / (1 - self.b2**self.t)
        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        return params

def spsa_epoch(params, S_in, y, a0, c0, k, alpha=0.602, gamma=0.101, rng=None):
    """
    One SPSA iteration:
      a_k = a0 / (k+1)^alpha, c_k = c0 / (k+1)^gamma
      gÌ‚ = (L(Î¸+c_kÎ”) - L(Î¸-c_kÎ”)) / (2 c_k) * Î”^{-1}
    Returns (loss, grad_est, a_k).
    """
    if rng is None: rng = np.random.default_rng()
    ak = a0 / ((k+1)**alpha)
    ck = c0 / ((k+1)**gamma)
    delta = rng.choice([-1.0, 1.0], size=params.shape)
    thetap = params + ck*delta
    thetam = params - ck*delta
    Up = circuit_unitary(thetap)
    Um = circuit_unitary(thetam)
    p_plus  = probs_from_unitary(Up, S_in)
    p_minus = probs_from_unitary(Um, S_in)
    Lp = bce_loss(p_plus, y)
    Lm = bce_loss(p_minus, y)
    ghat = ((Lp - Lm) / (2.0*ck)) * (1.0 / delta)
    return float((Lp+Lm)/2.0), ghat, ak

def _run_single_experiment(H: int, seed: int) -> dict:
    """
    Core experiment logic - run single QNN XOR training experiment.
    """
    # Fixed parameters matching the command line
    epochs = 600
    augment = 16
    noise_std = 0.05
    lr = 0.02
    acc_tol = 0.99
    spsa_a0 = 0.05
    spsa_c0 = 0.05
    optimizer = "spsa"

    # dataset + pre-encode inputs once
    X, y = make_xor_dataset(augment_per_corner=augment, noise_std=noise_std, seed=seed)
    y = y.astype(np.float64).reshape(-1,1)
    S_in = encode_states_matrix(X)  # (4,N)

    # initialize parameters (H x 4)
    rng = np.random.default_rng(seed)
    params = rng.normal(0.0, 0.15, size=(H, 4)).astype(np.float64)

    # train
    tracemalloc.start()
    t0 = time.perf_counter()

    opt = Adam(params.shape, lr=lr)

    last_loss = None
    if optimizer.lower() == "spsa":
        # Fast path: 2 circuit evals/epoch (independent of #params)
        for k in range(epochs):
            loss, ghat, ak = spsa_epoch(params, S_in, y, a0=spsa_a0, c0=spsa_c0, k=k, rng=rng)
            params = opt.step(params, ak * ghat)   # apply SPSA step-size schedule
            last_loss = loss
    else:
        # Exact parameter-shift fallback (slower but precise)
        shift = math.pi/2.0
        for _ in range(epochs):
            U = circuit_unitary(params)
            p = probs_from_unitary(U, S_in); last_loss = bce_loss(p, y)
            # dL/dp
            p_clip = np.clip(p, 1e-12, 1-1e-12)
            dLdp = (p_clip - y.reshape(-1)) / (p_clip*(1-p_clip))
            # Parameter-shift gradient (clean loop)
            grad = np.zeros_like(params)
            for l in range(H):
                for k in range(4):
                    params_plus  = params.copy(); params_plus[l, k]  += shift
                    params_minus = params.copy(); params_minus[l, k] -= shift
                    p_plus  = probs_from_unitary(circuit_unitary(params_plus),  S_in)
                    p_minus = probs_from_unitary(circuit_unitary(params_minus), S_in)
                    dp = 0.5 * (p_plus - p_minus)
                    grad[l, k] = float(np.mean(dLdp * dp))
            params = opt.step(params, grad)

    # final metrics
    U_final = circuit_unitary(params)
    p_final = probs_from_unitary(U_final, S_in)
    acc = accuracy_from_probs(p_final, y)

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "H": int(H),
        "epochs": int(epochs),
        "augment_per_corner": int(augment),
        "seed": int(seed),
        "final_loss": float(last_loss if last_loss is not None else bce_loss(p_final, y)),
        "accuracy": float(acc),
        "correct": bool(acc >= acc_tol),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "n_samples": int(X.shape[0]),
        "n_params": int(H*4),
        "device": get_device_name()
    }

# ------------********------------ 
# ============================== Carbon I/O
# ------------********------------ 
#def resolve_excel_path resolves a user-specified Excel filename to an existing absolute path, 
#   checking the given path and falling back to the Desktop if needed.
#def load_carbon_excel loads a carbon-intensity dataset, infers country/year/intensity columns, 
#     fixes units to kgCO2/kWh when necessary, and optionally selects the latest year.
#def compute_carbon converts total experiment runtime into energy use and per-country COâ‚‚ emissions, 
#     returning detailed country-wise results and a summary table.

def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------ 
# ============================== Plot Helpers
# ------------********------------ 

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    g = df.groupby("H")["runtime_s"].mean().reset_index()
    plt.figure()
    plt.plot(g["H"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Performance: Runtime vs Layers\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Runtime (s)")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_H.png"), **plt_kwargs)
    plt.close()

    g2 = df.groupby("H")["accuracy"].mean().reset_index()
    plt.figure()
    plt.plot(g2["H"], g2["accuracy"], marker="s", linewidth=2, markersize=8)
    plt.title(f"Performance: Mean Accuracy vs H\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_accuracy_vs_H.png"), **plt_kwargs)
    plt.close()

def plot_scalability(df: pd.DataFrame, outdir: str):
    g = df.groupby("H")["peak_mem_mb"].mean().reset_index()
    plt.figure()
    plt.plot(g["H"], g["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.title(f"Scalability: Memory vs Layers\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Memory (MB)")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_H.png"), **plt_kwargs)
    plt.close()

    g2 = df.groupby("H")["runtime_s"].mean().reset_index()
    plt.figure()
    plt.loglog(g2["H"], g2["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Scalability: Runtime (log-log)\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Runtime (s)")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_loglog_runtime.png"), **plt_kwargs)
    plt.close()

def plot_reliability(df: pd.DataFrame, outdir: str):
    data = [df[df["H"] == H]["accuracy"].values for H in sorted(df["H"].unique())]
    plt.figure()
    _boxplot_with_labels(data, labels=sorted(df["H"].unique()))
    plt.title(f"Reliability: Accuracy Distribution by H\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_accuracy_boxplot.png"), **plt_kwargs)
    plt.close()

    g = df.groupby("H")["correct"].mean().reset_index()
    plt.figure()
    plt.plot(g["H"], g["correct"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Reliability: Success Rate vs H\n{get_device_name()}")
    plt.xlabel("Layers H")
    plt.ylabel("Success Rate")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_success_vs_H.png"), **plt_kwargs)
    plt.close()

def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
   
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------ 
# ============================== Utility Functions
# ------------********------------ 
#def ensure_dir guarantees that a directory path exists by creating it (and any missing parents) if necessary.
#It uses os.makedirs with exist_ok=True to avoid errors when the directory is already present.
#def to_excel saves multiple pandas DataFrames into a single Excel file, writing each DataFrame to its own named sheet 
#    (truncated to Excelâ€™s 31-char limit).

def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    """Save multiple DataFrames to Excel with different sheets"""
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------ 
# ============================== Main Experiment Runner 
# ------------********------------ 

def run_qnn_xor_experiment():
    """
    Main QNN XOR experiment runner with hardcoded parameters for Kaggle.
    Equivalent to: --sizes 2 4 --steps 600 --trials 5 --augment-per-corner 16 --noise-std 0.05
                   --optimizer spsa --spsa-a0 0.05 --spsa-c0 0.05 --lr 0.02 --acc-tol 0.99
                   --device-power-watts 65 --pue 1.2 --combine
    """
    # Experiment parameters
    sizes = [2, 4]  # Number of ansatz layers
    trials = 5      # trials per size
    epochs = 600    # training epochs
    augment = 16    # data augmentation
    noise_std = 0.05
    lr = 0.02
    acc_tol = 0.99
    spsa_a0 = 0.05
    spsa_c0 = 0.05
    optimizer = "spsa"
    method = "qnn_xor"
    workers = 4     # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"ğŸš€ Starting QNN XOR experiments on {get_device_name()}")
    print(f"ğŸ”§ Configuration:")
    print(f"   Layers: {sizes}")
    print(f"   Epochs: {epochs}")
    print(f"   Trials per size: {trials}")
    print(f"   Augmentation: {augment} per corner")
    print(f"   Noise std: {noise_std}")
    print(f"   Learning rate: {lr}")
    print(f"   Accuracy tolerance: {acc_tol}")
    print(f"   Optimizer: {optimizer}")
    print(f"   SPSA a0: {spsa_a0}")
    print(f"   SPSA c0: {spsa_c0}")
    print(f"   Method: {method}")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for H in sizes:
        for i in range(trials):
            jobs.append((H, 1000 + 17 * H + i))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        print(f"  âœ… H={row['H']}, seed={row['seed']} (runtime={row['runtime_s']:.6f}s, accuracy={row['accuracy']:.3f}, correct={row['correct']})")

    # Execute jobs with thread-based parallelization
    print(f"ğŸ”€ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"âŒ Job failed H={job[0]}, seed={job[1]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("âŒ All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1])
                _consume(result)
            except Exception as e:
                print(f"âŒ Sequential fallback also failed for H={job[0]}, seed={job[1]}: {e}")

    if not all_rows:
        print("ğŸ’¥ CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------ 
    # ---------------- Performance Results ----------------
    print("ğŸ“Š Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")
    perf_agg = perf_df.groupby("H").agg({
        "runtime_s": "mean",
        "accuracy": "mean",
        "peak_mem_mb": "mean",
        "correct": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------ 
    # ---------------- Scalability Results ----------------
    print("ğŸ“ˆ Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")
    scal_agg = perf_df.groupby("H").agg({
        "runtime_s": "mean",
        "peak_mem_mb": "mean",
        "accuracy": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "from_perf_runs": perf_df[["H", "seed", "runtime_s", "peak_mem_mb", "accuracy"]],
            "aggregated": scal_agg
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------ 
    # ---------------- Reliability Results ----------------
    print("ğŸ¯ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "correctness_analysis": perf_df[["H", "seed", "accuracy", "correct"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------ 
    # ---------------- Carbon Footprint Results ----------------
    print("ğŸŒ Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"âœ… Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"âœ… Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"âŒ ERROR loading carbon data: {e}")
        print("ğŸ’¡ Using sample carbon data as fallback...")
        
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["H", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"âœ… Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    print("\n" + "="*60)
    print("ğŸ‰ QNN XOR EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"ğŸ“ Performance:     {perf_excel}")
    print(f"ğŸ“ Scalability:     {scal_excel}")
    print(f"ğŸ“ Reliability:     {rel_excel}")
    print(f"ğŸ“ Carbon:          {carb_excel}")
    print(f"ğŸ–¥ï¸  Device:          {get_device_name()}")
    print(f"ğŸ“Š Total runs:      {len(perf_df)}")
    print(f"âš¡ Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"ğŸ”¬ Layer sizes:     {sorted(perf_df['H'].unique())}")
    print(f"ğŸ¯ Success rate:    {perf_df['correct'].mean() * 100:.1f}%")
    print(f"ğŸ“ Mean accuracy:   {perf_df['accuracy'].mean():.3f}")

    print(f"\nğŸ“‚ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   â€¢ {d}")

    return perf_df

# ============================================================
# Main execution for Kaggle
# ============================================================
#This main block launches the full QNN XOR benchmark in a Kaggle environment after performing GPU memory cleanup.
#It runs the experiment pipeline, prints a concise summary of runtimes, success rates, accuracies, and the compute device used.
#On success, it enumerates generated result files across output directories and confirms all deliverables; otherwise it reports failure.

if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("ğŸš€ Starting QNN XOR Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_qnn_xor_experiment()

    if results_df is not None:
        # Display final results summary
        print("\nğŸ“Š FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {len(results_df)}")
        print(f"Layer sizes tested: {sorted(results_df['H'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {results_df['correct'].mean() * 100:.1f}%")
        print(f"Average accuracy: {results_df['accuracy'].mean():.3f}")
        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nğŸ“ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}ğŸ“ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "ğŸ“Š" if file.endswith('.xlsx') else "ğŸ–¼ï¸"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\nâœ… All QNN XOR deliverables generated successfully!")
        print("   â€¢ Performance/performance_results.xlsx + perf_*.png")
        print("   â€¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   â€¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   â€¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("âŒ QNN XOR benchmark failed - no results generated")

