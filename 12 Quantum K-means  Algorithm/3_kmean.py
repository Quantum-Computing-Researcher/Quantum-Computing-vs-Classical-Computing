#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------
# GPU-accelerated Empirical Evaluation: Quantum K-Means (Noiseless) ‚Äì Quantum Baseline
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------


"""12 KMean
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/12-kmean.0f63720f-1b8b-45d8-b18e-059a5e56cb64.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251122/auto/storage/goog4_request%26X-Goog-Date%3D20251122T181149Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dad46935d8269aa4d0ce6193fd75fee043d677f439a9f091452fe51a347cd5ecef765834846e1faac2484d1b60c05129b35c71f3ebe95a984ed589426b15ca99331e99a7e0253d2c99913817e456e716538e55db3a7e29a6fcbb33bb0bdd5412e7cd01d182e79490af24c95bf12250c886b4ea7fa59498a0abdbff05005eb30041e4d0b5a7a2c07ee4bf4218d0477517d092c96806716bf81cd8604a22cca743d9b4bb932a55a7d97b8230217af27f2eac6ed7942debcdab859ffd62b46642768f22732fb9a83e109663112d124f0be02e02aba7ad6726a6a34f791863fe8dcfd8838ad7cc1b166281a618dad127fc56cdd7549c703c22afc5b479ba42458f322
"""


# ------------********------------ Imports
#These imports provide core utilities for filesystem handling, timing, memory profiling, warnings, and mathematical operations.
#They include dataclass support, rich typing annotations, 
#  and itertools for structured experiment logic and combinatorics.
#Threading and ThreadPoolExecutor enable concurrent execution with thread-safe coordination where needed.

import os, sys, time, tracemalloc, pathlib, warnings, math, itertools
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------********------------
# GPU Setup and Dependencies
# ------------********------------
print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp

    # Test if GPU is available and working
    with cp.cuda.Device(0):

        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil", "scikit-learn", "qiskit"])


# ------------********------------
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# Qiskit for quantum simulation
from qiskit import QuantumCircuit
from qiskit.quantum_info import Statevector

# GPU utility functions
def get_device_name() -> str:
    #Get current device name
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    #Safely cleanup GPU memory
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

def to_numpy(x):
    """Safely convert CuPy array to NumPy array"""
    if HAS_GPU and isinstance(x, cp.ndarray):
        return cp.asnumpy(x)
    return x

def to_cupy(x):
    # Safely convert NumPy array to CuPy array
    if HAS_GPU and isinstance(x, np.ndarray):
        return cp.asarray(x)
    return x

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
   
    # Thread-safe GPU experiment runner.
    # Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    
    k_clusters, seed = args
    with _gpu_lock:
        return _run_single_experiment(k_clusters, seed)

# ------------********------------ Quantum K-Means Core

# Fixed 5-point dataset for consistent benchmarking
DATA = np.array([
    [1.0, 1.0],
    [2.0, 1.0],
    [4.0, 3.0],
    [5.0, 4.0],
    [6.0, 5.0],
], dtype=np.float64)

def build_feature_map(x: np.ndarray) -> QuantumCircuit:
    # Quantum feature map with RZ‚ÄìRX‚ÄìRZZ encoding"""
    x = np.asarray(x, dtype=float)
    q = x.shape[0]  # Number of features (2D data)
    qc = QuantumCircuit(q)

    # Initial Hadamard layers
    for i in range(q):
        qc.h(i)

    # Feature encoding
    for i in range(q):
        theta = math.pi * x[i]
        qc.rz(theta, i)
        qc.rx(theta, i)

    # Entangling layers
    for i in range(q):
        for j in range(i+1, q):
            qc.rzz(math.pi * x[i] * x[j], i, j)

    return qc


#  Compute quantum state fidelity |‚ü®œà(x)|œà(z)‚ü©|¬≤ using statevector simulation
def quantum_fidelity(x: np.ndarray, z: np.ndarray) -> float:
    
    qx = build_feature_map(x)
    qz = build_feature_map(z)
    sv_x = Statevector.from_instruction(qx)
    sv_z = Statevector.from_instruction(qz)
    overlap = sv_x.data.conj().dot(sv_z.data)
    return float(np.abs(overlap)**2)

# Define quantum distance as sqrt(1 - fidelity)
def quantum_distance(x: np.ndarray, z: np.ndarray) -> float:
    return math.sqrt(max(0.0, 1.0 - quantum_fidelity(x, z)))

#Compute quantum distance matrix between two sets of points
def pairwise_qdist(A: np.ndarray, B: np.ndarray) -> np.ndarray:  
    n1, n2 = A.shape[0], B.shape[0]
    D = np.zeros((n1, n2))
    for i in range(n1):
        for j in range(n2):
            D[i, j] = quantum_distance(A[i], B[j])
    return D

def quantum_kmeans_labels(X: np.ndarray, K: int, max_iter: int = 20, seed: int = 0):
    """Quantum-inspired K-Means clustering with quantum distances"""
    rng = np.random.default_rng(seed)
    n = X.shape[0]
    K_eff = max(1, min(K, n))

    # Initialize centroids
    idx0 = rng.choice(n, size=K_eff, replace=False)
    centers = X[idx0].copy()
    labels = np.full(n, -1, dtype=np.int32)

    # K-means iteration
    for iteration in range(max_iter):
        # Assign labels based on quantum distances
        d = pairwise_qdist(X, centers)
        new_labels = np.argmin(d, axis=1)

        # Check convergence
        if np.array_equal(new_labels, labels):
            labels = new_labels
            break

        labels = new_labels

        # Update centroids
        for k in range(K_eff):
            members = (labels == k)
            if np.any(members):
                centers[k] = X[members].mean(axis=0)

    return labels, K_eff

#  Compute sum of squared quantum distances to cluster centroids
def compute_inertia_q(X: np.ndarray, labels: np.ndarray, K_eff: int) -> float:
    total = 0.0
    for k in range(K_eff):
        members = X[labels == k]
        if len(members) == 0:
            continue
        center = members.mean(axis=0)
        for x in members:
            total += quantum_distance(x, center)**2
    return total

def clustering_accuracy_vs_reference(labels: np.ndarray, ref_labels: np.ndarray, K_eff: int) -> float:
    """Compute clustering accuracy vs reference (handling label permutation)"""
    labels = np.asarray(labels)
    ref_labels = np.asarray(ref_labels)
    n = labels.shape[0]
    if n == 0:
        return 0.0

    best = 0.0
    for perm in itertools.permutations(range(K_eff)):
        perm = np.array(perm, dtype=np.int32)
        mapped = perm[labels]
        acc = float(np.mean(mapped == ref_labels))
        if acc > best:
            best = acc
    return best


    """
    Core experiment logic - run single Quantum K-Means experiment.
    """
def _run_single_experiment(k_clusters: int, seed: int) -> dict:

    # Fixed parameters
    max_iter = 20
    acc_tol = 0.5

    # Set random seed
    np.random.seed(seed)
    if HAS_GPU:
        cp.random.seed(seed)

    tracemalloc.start()
    t0 = time.perf_counter()

    try:
        # Generate reference clustering (seed=0 for consistency)
        ref_labels, ref_K_eff = quantum_kmeans_labels(DATA, k_clusters, max_iter=max_iter, seed=0)

        # Run quantum K-means
        labels, K_eff = quantum_kmeans_labels(DATA, k_clusters, max_iter=max_iter, seed=seed)

        # Compute metrics
        inertia = compute_inertia_q(DATA, labels, K_eff)
        accuracy = clustering_accuracy_vs_reference(labels, ref_labels, K_eff)
        success = bool(accuracy >= acc_tol)

    except Exception as e:
        print(f"‚ùå Experiment failed for K={k_clusters}, seed={seed}: {e}")
        K_eff = min(k_clusters, DATA.shape[0])
        accuracy = 0.0
        inertia = float('inf')
        success = False

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "k_clusters": int(k_clusters),
        "k_effective": int(K_eff),
        "seed": int(seed),
        "max_iter": int(max_iter),
        "accuracy": float(accuracy),
        "inertia": float(inertia),
        "success": bool(success),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "device": get_device_name()
    }
# ------------********------------
# ============================== Carbon I/O 
# ------------********------------
# to resolve parh
def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary



# ------------********------------
# ============================== Plot Helpers
# ------------********------------

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

# ------------********------------
def plot_performance(df: pd.DataFrame, outdir: str):
    # Runtime vs cluster count
    g = df.groupby("k_clusters")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g["k_clusters"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Runtime (s)")
    plt.title(f"Performance: Runtime vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_k.png"), **plt_kwargs)
    plt.close()

    # Memory usage vs cluster count
    g2 = df.groupby("k_clusters")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["k_clusters"], g2["peak_mem_mb"], marker="s", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Performance: Memory vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_memory_vs_k.png"), **plt_kwargs)
    plt.close()

    # Accuracy vs cluster count
    g3 = df.groupby("k_clusters")["accuracy"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g3["k_clusters"], g3["accuracy"], marker="^", linewidth=2, markersize=8, color='purple')
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Clustering Accuracy")
    plt.title(f"Performance: Accuracy vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.ylim(0, 1.05)
    plt.savefig(os.path.join(outdir, "perf_accuracy_vs_k.png"), **plt_kwargs)
    plt.close()

    # Inertia vs cluster count
    g4 = df.groupby("k_clusters")["inertia"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g4["k_clusters"], g4["inertia"], marker="d", linewidth=2, markersize=8, color='orange')
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Quantum Inertia")
    plt.title(f"Performance: Inertia vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_inertia_vs_k.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_scalability(df: pd.DataFrame, outdir: str):
    # Runtime vs cluster count (log-log)
    g = df.groupby("k_clusters")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.loglog(g["k_clusters"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Runtime (s)")
    plt.title(f"Scalability: Runtime vs Cluster Count (log-log)\n{get_device_name()}")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_k.png"), **plt_kwargs)
    plt.close()

    # Memory vs cluster count
    g2 = df.groupby("k_clusters")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.semilogy(g2["k_clusters"], g2["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Scalability: Memory vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_k.png"), **plt_kwargs)
    plt.close()

    # Accuracy scaling
    g3 = df.groupby("k_clusters")["accuracy"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.plot(g3["k_clusters"], g3["accuracy"], marker="s", linewidth=2, markersize=8, color='red')
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Clustering Accuracy")
    plt.title(f"Scalability: Accuracy vs Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.ylim(0, 1.05)
    plt.savefig(os.path.join(outdir, "scal_accuracy_vs_k.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_reliability(df: pd.DataFrame, outdir: str):
    # Accuracy distribution by cluster count
    data = [df[df["k_clusters"] == k]["accuracy"].values for k in sorted(df["k_clusters"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"K={k}" for k in sorted(df["k_clusters"].unique())])
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Clustering Accuracy")
    plt.title(f"Reliability: Accuracy Distribution by Cluster Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_accuracy_distribution.png"), **plt_kwargs)
    plt.close()

    # Success rate by cluster count
    success_rates = df.groupby("k_clusters")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["k_clusters"].astype(str), success_rates["success"])
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Success Rate")
    plt.title(f"Reliability: Success Rate by Cluster Count\n{get_device_name()}")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "rel_success_rate.png"), **plt_kwargs)
    plt.close()

    # Runtime distribution
    data = [df[df["k_clusters"] == k]["runtime_s"].values for k in sorted(df["k_clusters"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"K={k}" for k in sorted(df["k_clusters"].unique())])
    plt.yscale("log")
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Runtime (s, log scale)")
    plt.title(f"Reliability: Runtime Distribution\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_runtime_distribution.png"), **plt_kwargs)
    plt.close()

    # Inertia distribution
    data = [df[df["k_clusters"] == k]["inertia"].values for k in sorted(df["k_clusters"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"K={k}" for k in sorted(df["k_clusters"].unique())])
    plt.xlabel("Number of Clusters (K)")
    plt.ylabel("Quantum Inertia")
    plt.title(f"Reliability: Inertia Distribution\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_inertia_distribution.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------
# ============================== Utility Functions
# ------------********------------

def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)


"""Save multiple DataFrames to Excel with different sheets"""
def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------
# ============================== Main Experiment Runner
# ------------********------------

def run_quantum_kmeans_experiment():
    
    #Main Quantum K-Means experiment runner with hardcoded parameters for Kaggle.
    #   
    #  --sizes 2,3,4 
    #  --trials 10 
    #  --steps 20
    #  --device-power-watts 65
    #  --pue 1.2 
    #  --combine

    # Experiment parameters
    k_clusters_list = [2, 3, 4]
    trials = 10   # trials per cluster count
    workers = 4   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting Quantum K-Means experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Cluster counts: {k_clusters_list}")
    print(f"   Trials per cluster count: {trials}")
    print(f"   Dataset: 5-point 2D (fixed)")
    print(f"   Max iterations: 20")
    print(f"   Accuracy tolerance: 0.5")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for k_clusters in k_clusters_list:
        for i in range(trials):
            seed = 1000 + 17 * k_clusters + i
            jobs.append((k_clusters, seed))

    all_rows = []

    def _consume(row):
        #Process completed job results

        all_rows.append(row)
        status = "‚úÖ" if row["success"] else "‚ùå"
        accuracy = f"{row['accuracy']:.3f}"
        print(f"  {status} K={row['k_clusters']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.3f}s, accuracy={accuracy})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed K={job[0]}, seed={job[1]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("‚ùå All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1])
                _consume(result)
            except Exception as e:
                print(f"‚ùå Sequential fallback also failed for K={job[0]}, seed={job[1]}: {e}")

    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------
    # ---------------- Performance Results ----------------
# ------------********------------

    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")

    # Only aggregate successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0:
        perf_agg = perf_df.groupby("k_clusters").agg({
            "runtime_s": ["mean", "std"],
            "peak_mem_mb": ["mean", "std"],
            "accuracy": ["mean", "std"],
            "inertia": ["mean", "std"],
            "success": "mean"
        }).reset_index()
        # Flatten column names
        perf_agg.columns = ['_'.join(col).strip('_') for col in perf_agg.columns.values]
    else:
        perf_agg = pd.DataFrame({"note": ["No successful experiments"]})

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------
    # ---------------- Scalability Results ----------------
# ------------********------------

    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")

    # Compute scaling coefficients only for successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0 and len(success_df["k_clusters"].unique()) >= 2:
        # Log-log fit for scaling analysis
        scaling_data = success_df.groupby("k_clusters")["runtime_s"].mean().reset_index()
        log_k = np.log(scaling_data["k_clusters"].values)
        log_time = np.log(scaling_data["runtime_s"].values + 1e-12)

        # Only compute if we have enough data and variation
        if len(log_k) >= 2 and np.std(log_k) > 1e-6 and np.std(log_time) > 1e-6:
            try:
                scaling_coeff = np.polyfit(log_k, log_time, 1)[0]
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [scaling_coeff],
                    "description": ["Exponent in time ~ K^exponent"]
                })
            except:
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [np.nan],
                    "description": ["Numerical issues in scaling analysis"]
                })
        else:
            scaling_summary = pd.DataFrame({
                "parameter": ["scaling_exponent"],
                "value": [np.nan],
                "description": ["Insufficient data variation for scaling analysis"]
            })
    else:
        scaling_summary = pd.DataFrame({
            "parameter": ["scaling_exponent"],
            "value": [np.nan],
            "description": ["No successful experiments for scaling analysis"]
        })

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_data": perf_df[["k_clusters", "k_effective", "runtime_s", "peak_mem_mb", "success"]],
            "scaling_analysis": scaling_summary
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------
    # ---------------- Reliability Results ----------------
# ------------********------------

    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    reliability_analysis = perf_df.groupby("k_clusters").agg({
        "accuracy": ["mean", "std", "min", "max"],
        "inertia": ["mean", "std"],
        "runtime_s": ["mean", "std"],
        "success": "mean"
    }).reset_index()
    reliability_analysis.columns = ['_'.join(col).strip('_') for col in reliability_analysis.columns.values]

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "reliability_analysis": reliability_analysis,
            "raw_results": perf_df[["k_clusters", "k_effective", "seed", "accuracy", "inertia", "success"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------
    # ---------------- Carbon Footprint Results ----------------
# ------------********------------

    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["k_clusters", "k_effective", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

# ------------********------------ Final summary
    success_count = perf_df["success"].sum()
    total_count = len(perf_df)
    success_rate = success_count / total_count if total_count > 0 else 0

    print("\n" + "="*60)
    print("üéâ QUANTUM K-MEANS EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {total_count}")
    print(f"‚úÖ Successful:      {success_count}")
    print(f"‚ùå Failed:          {total_count - success_count}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¢ Cluster counts:  {sorted(perf_df['k_clusters'].unique())}")
    print(f"üéØ Success rate:    {success_rate * 100:.1f}%")

    if success_count > 0:
        success_df = perf_df[perf_df["success"]]
        print(f"üìè Mean accuracy:   {success_df['accuracy'].mean():.3f}")
        print(f"‚è±Ô∏è  Mean runtime:    {success_df['runtime_s'].mean():.3f}s")
        print(f"üìê Mean inertia:    {success_df['inertia'].mean():.3f}")
    else:
        print(f"üìè Mean accuracy:   N/A (no successful runs)")
        print(f"‚è±Ô∏è  Mean runtime:    N/A (no successful runs)")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ------------********------------
# Main execution for Kaggle
# ------------********------------
if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting Quantum K-Means Benchmark on Kaggle")
    print("=" * 50)

 # ------------********------------ Run the complete experiment
    results_df = run_quantum_kmeans_experiment()

    if results_df is not None:
        # Display final results summary
        success_count = results_df["success"].sum()
        total_count = len(results_df)

        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {total_count}")
        print(f"Successful experiments: {success_count}")
        print(f"Failed experiments: {total_count - success_count}")
        print(f"Cluster counts tested: {sorted(results_df['k_clusters'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.3f}s")
        print(f"Success rate: {success_count / total_count * 100:.1f}%")

        if success_count > 0:
            success_df = results_df[results_df["success"]]
            print(f"Average accuracy: {success_df['accuracy'].mean():.3f}")
            print(f"Average inertia: {success_df['inertia'].mean():.3f}")
        else:
            print(f"Average accuracy: N/A")
            print(f"Average inertia: N/A")

        print(f"Device used: {get_device_name()}")

    # ------------********------------ Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')


        print("\n‚úÖ All Quantum K-Means results generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå Quantum K-Means benchmark failed - no results generated")

