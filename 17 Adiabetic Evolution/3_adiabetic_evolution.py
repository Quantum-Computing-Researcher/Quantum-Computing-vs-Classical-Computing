#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------
# GPU-accelerated Empirical Evaluation: Adiabatic Quantum Simulation (Noiseless) ‚Äì Quantum Baseline

"""16 Adiabetic Evolution
GPU-accelerated Empirical Evaluation: Adiabatic Quantum Simulation (Noiseless) ‚Äì Quantum Baseline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/16-adiabetic-evolution.74dcf540-32ce-46bf-9593-3987a80c5c92.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251122/auto/storage/goog4_request%26X-Goog-Date%3D20251122T192627Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6206dc8608fb4eb1fac729dc8169e7c91d3b1e6ff5947c502acd7b499f638e53e6d7116b0df10ba8cb8b03b093d5991a84299ad5822e4d44823ffe73746d3fe0fcbb8ce3578c52968bb694c9e3b468c8a1fcac73e0289f56d204d2a80b6a013632314dc119f3f302e2151e270c9a28201de2822cadaeef5b0c370320154eebdf55b4ac8ceb6cc0d767323706169018ff6aaf38165d4f8719ee3622ad5bd283dc3a91a04e244aa92e222dbfd723a9b15754370b4d63567f5d1ec54288139027a559b7be7ee527f2b17c6d996e8e346e4e8d5ad7a63053f00210f7d3dd50989ce6ba4695b35ff5af0c307260d465db453c692ccf04c1d728a0c44ac5d726a0917a

Hamiltonian: 1D Transverse-Field Ising
  H(s) = (1 - s) H0 + s H1, where s = t/T (linear schedule)
  H0 = -Gamma * sum_i X_i
  H1 = -J * sum_<i,i+1> Z_i Z_{i+1} - h * sum_i Z_i
"""
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------


# ------------********------------ Imports
#These imports provide core system utilities for timing, memory tracking, filesystem handling, and warnings control.
#Dataclasses and typing helpers support structured data management and clear type annotations.
#Threading and concurrent futures enable parallel task execution and result coordination across threads.

import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------********------------
# ------------********------------ GPU Setup and Dependencies
# ------------********------------
print("üîç Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"‚úÖ GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("‚ùå CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"‚ö†Ô∏è  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("üîÑ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil", "scikit-learn"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
def get_device_name() -> str:
    """Get current device name"""
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    """Safely cleanup GPU memory"""
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

def to_numpy(x):
    """Safely convert CuPy array to NumPy array"""
    if HAS_GPU and isinstance(x, cp.ndarray):
        return cp.asnumpy(x)
    return x

def to_cupy(x):
    """Safely convert NumPy array to CuPy array"""
    if HAS_GPU and isinstance(x, np.ndarray):
        return cp.asarray(x)
    return x

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
    """
    Thread-safe GPU experiment runner.
    Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    """
    n_qubits, steps, seed = args
    with _gpu_lock:
        return _run_single_experiment(n_qubits, steps, seed)

# ------------********------------
# ========================= Adiabatic Quantum Simulation Core =========================
# ------------********------------

#This function defines and returns the four fundamental Pauli operators used in quantum mechanics.
#Each operator is explicitly constructed as a 2√ó2 complex-valued matrix for reuse in simulations.

def paulis() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Return Pauli matrices"""
    I = np.array([[1, 0], [0, 1]], dtype=complex)
    X = np.array([[0, 1], [1, 0]], dtype=complex)
    Y = np.array([[0, -1j], [1j, 0]], dtype=complex)
    Z = np.array([[1, 0], [0, -1]], dtype=complex)
    return I, X, Y, Z

#This function applies a single-qubit unitary gate to qubit i within an n-qubit statevector.
#It reshapes and permutes tensor axes to isolate the target qubit, applies the operation, and restores the vector form.

def apply_1q(U: np.ndarray, psi: np.ndarray, n: int, i: int) -> np.ndarray:
    """Apply single-qubit gate to statevector"""
    psi = psi.reshape([2] * n)
    psi = np.moveaxis(psi, i, 0)
    psi = (U @ psi.reshape(2, -1)).reshape([2] + [2] * (n - 1))
    psi = np.moveaxis(psi, 0, i)
    return psi.reshape(-1)

def apply_2q_nn(U: np.ndarray, psi: np.ndarray, n: int, i: int) -> np.ndarray:
    """Apply two-qubit gate to nearest neighbors"""
    psi = psi.reshape([2] * n)
    psi = np.moveaxis(psi, (i, i + 1), (0, 1))
    psi = (U @ psi.reshape(4, -1)).reshape([2, 2] + [2] * (n - 2))
    psi = np.moveaxis(psi, (0, 1), (i, i + 1))
    return psi.reshape(-1)

def Ux(theta: float) -> np.ndarray:
    """X rotation gate: exp(-i * theta * X)"""
    I, X, _, _ = paulis()
    return np.cos(theta) * I - 1j * np.sin(theta) * X

def Uz(theta: float) -> np.ndarray:
    """Z rotation gate: exp(-i * theta * Z)"""
    return np.array([[np.exp(-1j * theta), 0],
                     [0, np.exp(1j * theta)]], dtype=complex)

def Uzz(theta: float) -> np.ndarray:
    """ZZ rotation gate: exp(-i * theta * (Z ‚äó Z))"""
    return np.diag([np.exp(-1j * theta), np.exp(1j * theta),
                    np.exp(1j * theta), np.exp(-1j * theta)]).astype(complex)

def init_plus_state(n: int) -> np.ndarray:
    """Initialize |+‚ü©^n state"""
    plus = (1/np.sqrt(2)) * np.array([1.0, 1.0], dtype=complex)
    state = plus
    for _ in range(n - 1):
        state = np.kron(state, plus)
    return state

def simulate_adiabatic_gate_noiseless(n: int, J: float, h: float, Gamma: float,
                                      T: float, m_steps: int, psi0: np.ndarray) -> np.ndarray:
    """
    Simulate adiabatic evolution using gate-based approach
    H(s) = (1 - s) H0 + s H1, where s = t/T
    H0 = -Gamma * sum_i X_i
    H1 = -J * sum_<i,i+1> Z_i Z_{i+1} - h * sum_i Z_i
    """
    dt = T / m_steps
    psi = psi0.copy()

    for k in range(m_steps):
        t = (k + 0.5) * dt  # midpoint
        s = t / T

        # Rotation angles for this slice
        theta_x = (dt * (1.0 - s) * Gamma)
        theta_zz = (dt * s * J)
        theta_z = (dt * s * h)

        # Build gates for this slice
        U1x = Ux(+theta_x)
        U1z = Uz(+theta_z)
        U2zz = Uzz(+theta_zz)

        # Apply gates in fixed order (first-order product formula)
        # 1) ZZ on even bonds, then odd bonds
        for i in range(0, n - 1, 2):
            psi = apply_2q_nn(U2zz, psi, n, i)
        for i in range(1, n - 1, 2):
            psi = apply_2q_nn(U2zz, psi, n, i)
        # 2) Z fields
        for i in range(n):
            psi = apply_1q(U1z, psi, n, i)
        # 3) X fields
        for i in range(n):
            psi = apply_1q(U1x, psi, n, i)

    return psi

def build_tfim_components_dense(n: int, J: float, h: float, Gamma: float) -> Tuple[np.ndarray, np.ndarray]:
    """Build dense Hamiltonian matrices for reference simulation"""
    I, X, Y, Z = paulis()
    dim = 2 ** n
    H0 = np.zeros((dim, dim), dtype=complex)
    H1 = np.zeros((dim, dim), dtype=complex)

    def s1(i, op):
        mats = [I] * n
        mats[i] = op
        out = mats[0]
        for m in mats[1:]:
            out = np.kron(out, m)
        return out

    def s2(i, j, opi, opj):
        mats = [I] * n
        mats[i] = opi
        mats[j] = opj
        out = mats[0]
        for m in mats[1:]:
            out = np.kron(out, m)
        return out

    # H0: transverse field
    for i in range(n):
        H0 += -Gamma * s1(i, X)

    # H1: ZZ couplings and longitudinal field
    for i in range(n - 1):
        H1 += -J * s2(i, i + 1, Z, Z)
    for i in range(n):
        H1 += -h * s1(i, Z)

    return H0, H1

def expm_hermitian(H: np.ndarray, dt: float) -> np.ndarray:
    """Matrix exponential for Hermitian matrices"""
    w, V = np.linalg.eigh(H)
    phase = np.exp(-1j * dt * w)
    return (V * phase) @ V.conj().T

def simulate_reference_dense(H0: np.ndarray, H1: np.ndarray, T: float, m_ref: int, psi0: np.ndarray) -> np.ndarray:
    """High-resolution reference simulation using dense matrices"""
    dt = T / m_ref
    psi = psi0.copy()
    for k in range(m_ref):
        t = (k + 0.5) * dt
        s = t / T
        Hs = (1.0 - s) * H0 + s * H1
        U = expm_hermitian(Hs, dt)
        psi = U @ psi
    return psi

def l2_error(a: np.ndarray, b: np.ndarray) -> float:
    """Compute L2 error between statevectors"""
    return float(np.linalg.norm(a - b))

def _run_single_experiment(n_qubits: int, steps: int, seed: int) -> dict:
    """
    Core experiment logic - run single Adiabatic Quantum Simulation experiment.
    """
    # Fixed Hamiltonian parameters
    J = 1.0      # Ising ZZ coupling
    h = 0.5      # Longitudinal field
    Gamma = 1.0  # Transverse field
    T = 1.0      # Total evolution time

    # Set random seed for reproducibility
    np.random.seed(seed)
    if HAS_GPU:
        cp.random.seed(seed)

    tracemalloc.start()
    t0 = time.perf_counter()

    try:
        # Initialize state
        psi0 = init_plus_state(n_qubits)

        # Run adiabatic simulation
        psi_final = simulate_adiabatic_gate_noiseless(n_qubits, J, h, Gamma, T, steps, psi0)

        # Compute reference for accuracy (not counted in runtime)
        H0, H1 = build_tfim_components_dense(n_qubits, J, h, Gamma)
        m_ref = steps * 4  # High-resolution reference
        psi_ref = simulate_reference_dense(H0, H1, T, m_ref, psi0)
        error = l2_error(psi_final, psi_ref)

        success = True

    except Exception as e:
        print(f"‚ùå Experiment failed for n_qubits={n_qubits}, steps={steps}, seed={seed}: {e}")
        error = float('inf')
        success = False

    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "n_qubits": int(n_qubits),
        "steps": int(steps),
        "seed": int(seed),
        "J": float(J),
        "h": float(h),
        "Gamma": float(Gamma),
        "T": float(T),
        "error": float(error),
        "success": bool(success),
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "dimension": int(2 ** n_qubits),
        "device": get_device_name()
    }
# ------------********------------
# ------------********------------ Carbon I/O ==============================
# ------------********------------
def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------
# ------------********------------ Plot Helpers ==============================
# ------------********------------

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    # Runtime vs steps for different qubit counts
    for n_qubits in sorted(df["n_qubits"].unique()):
        subset = df[df["n_qubits"] == n_qubits]
        g = subset.groupby("steps")["runtime_s"].mean().reset_index()
        plt.figure(figsize=(10, 6))
        plt.plot(g["steps"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
        plt.xscale("log", base=2)
        plt.xlabel("Adiabatic Steps")
        plt.ylabel("Runtime (s)")
        plt.title(f"Performance: Runtime vs Steps (n={n_qubits})\n{get_device_name()}")
        plt.grid(True)
        plt.savefig(os.path.join(outdir, f"perf_runtime_vs_steps_n{n_qubits}.png"), **plt_kwargs)
        plt.close()

    # Memory usage vs qubits
    g2 = df.groupby("n_qubits")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["n_qubits"], g2["peak_mem_mb"], marker="s", linewidth=2, markersize=8, color='green')
    plt.xlabel("Number of Qubits")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Performance: Memory vs Qubits\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_memory_vs_qubits.png"), **plt_kwargs)
    plt.close()

    # Error vs steps
    g3 = df.groupby(["n_qubits", "steps"])["error"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    for n_qubits in sorted(g3["n_qubits"].unique()):
        subset = g3[g3["n_qubits"] == n_qubits]
        plt.plot(subset["steps"], subset["error"], marker="^", linewidth=2, markersize=8, label=f"n={n_qubits}")
    plt.xscale("log", base=2)
    plt.yscale("log")
    plt.xlabel("Adiabatic Steps")
    plt.ylabel("L2 Error (log scale)")
    plt.title(f"Performance: Error vs Steps\n{get_device_name()}")
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "perf_error_vs_steps.png"), **plt_kwargs)
    plt.close()

    # Success rate by qubit count
    success_rates = df.groupby("n_qubits")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["n_qubits"].astype(str), success_rates["success"])
    plt.xlabel("Number of Qubits")
    plt.ylabel("Success Rate")
    plt.title(f"Performance: Success Rate by Qubit Count\n{get_device_name()}")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_success_rate.png"), **plt_kwargs)
    plt.close()

# ------------********-----------------------********--------------------********------------
def plot_scalability(df: pd.DataFrame, outdir: str):
    # Runtime vs Hilbert space dimension (log-log)
    g = df.groupby("n_qubits")["runtime_s"].mean().reset_index()
    g["dimension"] = 2 ** g["n_qubits"]
    plt.figure(figsize=(8, 6))
    plt.loglog(g["dimension"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.xlabel("Hilbert Space Dimension")
    plt.ylabel("Runtime (s)")
    plt.title(f"Scalability: Runtime vs Dimension (log-log)\n{get_device_name()}")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_dimension.png"), **plt_kwargs)
    plt.close()

    # Memory vs dimension
    g2 = df.groupby("n_qubits")["peak_mem_mb"].mean().reset_index()
    g2["dimension"] = 2 ** g2["n_qubits"]
    plt.figure(figsize=(8, 6))
    plt.semilogy(g2["dimension"], g2["peak_mem_mb"], marker="^", linewidth=2, markersize=8, color='green')
    plt.xlabel("Hilbert Space Dimension")
    plt.ylabel("Memory Usage (MB)")
    plt.title(f"Scalability: Memory vs Dimension\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_dimension.png"), **plt_kwargs)
    plt.close()

    # Runtime vs steps for largest qubit count
    max_qubits = df["n_qubits"].max()
    g3 = df[df["n_qubits"] == max_qubits].groupby("steps")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(8, 6))
    plt.loglog(g3["steps"], g3["runtime_s"], marker="s", linewidth=2, markersize=8, color='red')
    plt.xlabel("Adiabatic Steps")
    plt.ylabel("Runtime (s)")
    plt.title(f"Scalability: Runtime vs Steps (n={max_qubits})\n{get_device_name()}")
    plt.grid(True, which="both")
    plt.savefig(os.path.join(outdir, "scal_runtime_vs_steps.png"), **plt_kwargs)
    plt.close()

# ------------********----------------------********---------------------********------------
def plot_reliability(df: pd.DataFrame, outdir: str):
    # Error distribution by qubit count
    data = [df[df["n_qubits"] == n]["error"].values for n in sorted(df["n_qubits"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"Qubits={n}" for n in sorted(df["n_qubits"].unique())])
    plt.yscale("log")
    plt.xlabel("Number of Qubits")
    plt.ylabel("L2 Error (log scale)")
    plt.title(f"Reliability: Error Distribution by Qubit Count\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_error_distribution.png"), **plt_kwargs)
    plt.close()

    # Success rate by qubit count
    success_rates = df.groupby("n_qubits")["success"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.bar(success_rates["n_qubits"].astype(str), success_rates["success"])
    plt.xlabel("Number of Qubits")
    plt.ylabel("Success Rate")
    plt.title(f"Reliability: Success Rate by Qubit Count\n{get_device_name()}")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "rel_success_rate.png"), **plt_kwargs)
    plt.close()

    # Runtime distribution
    data = [df[df["n_qubits"] == n]["runtime_s"].values for n in sorted(df["n_qubits"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=[f"Qubits={n}" for n in sorted(df["n_qubits"].unique())])
    plt.yscale("log")
    plt.xlabel("Number of Qubits")
    plt.ylabel("Runtime (s, log scale)")
    plt.title(f"Reliability: Runtime Distribution\n{get_device_name()}")
    plt.grid(True)
    plt.savefig(os.path.join(outdir, "rel_runtime_distribution.png"), **plt_kwargs)
    plt.close()

    # Error heatmap
    pivot_data = df.pivot_table(values="error", index="n_qubits", columns="steps", aggfunc="mean")
    plt.figure(figsize=(10, 6))
    plt.imshow(np.log10(pivot_data.values + 1e-16), aspect="auto", cmap="viridis", origin="lower")
    plt.colorbar(label="log10(Error)")
    plt.xticks(range(len(pivot_data.columns)), pivot_data.columns)
    plt.yticks(range(len(pivot_data.index)), pivot_data.index)
    plt.xlabel("Adiabatic Steps")
    plt.ylabel("Number of Qubits")
    plt.title(f"Reliability: Error Heatmap\n{get_device_name()}")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "rel_error_heatmap.png"), **plt_kwargs)
    plt.close()

# ------------********----------------------********----------------------********------------
def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    """Generate carbon footprint plots - ALWAYS country-wise"""
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()
# ------------********------------
# ------------********------------ Utility Functions ==============================
# ------------********------------
#These utility functions handle output directory creation and structured result export.
#The directory helper safely creates paths without errors if they already exist.
#The Excel writer saves multiple DataFrames into a single workbook using separate sheets.

def ensure_dir(d: str):
    """Ensure directory exists"""
    os.makedirs(d, exist_ok=True)

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    """Save multiple DataFrames to Excel with different sheets"""
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ------------********------------
# ------------********------------ Main Experiment Runner ==============================
# ------------********------------

def run_adiabatic_simulation_experiment():
   
   # Main Adiabatic Quantum Simulation experiment runner with hardcoded parameters for Kaggle.
   # Equivalent to: --n-min 2 --n-max 6 --steps 1,2,4,8,16,32 --repeats 2
   #                --device-power-watts 65 --pue 1.2 --combine
   
    # Experiment parameters
    n_qubits_list = [2, 3, 4, 5, 6]  # Qubit counts to test
    steps_list = [1, 2, 4, 8, 16, 32]  # Adiabatic steps to test
    trials = 2   # trials per configuration
    workers = 4   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"üöÄ Starting Adiabatic Quantum Simulation experiments on {get_device_name()}")
    print(f"üîß Configuration:")
    print(f"   Qubit counts: {n_qubits_list}")
    print(f"   Adiabatic steps: {steps_list}")
    print(f"   Trials per configuration: {trials}")
    print(f"   Hamiltonian: 1D Transverse-Field Ising")
    print(f"   J (ZZ coupling): 1.0")
    print(f"   h (longitudinal field): 0.5")
    print(f"   Gamma (transverse field): 1.0")
    print(f"   Total time T: 1.0")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for n_qubits in n_qubits_list:
        for steps in steps_list:
            for i in range(trials):
                seed = 1000 + 17 * n_qubits + 31 * steps + i
                jobs.append((n_qubits, steps, seed))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        status = "‚úÖ" if row["success"] else "‚ùå"
        error = f"{row['error']:.2e}"
        print(f"  {status} n={row['n_qubits']}, steps={row['steps']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.3f}s, error={error})")

    # Execute jobs with thread-based parallelization
    print(f"üîÄ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"‚ùå Job failed n_qubits={job[0]}, steps={job[1]}, seed={job[2]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            result = run_experiment_thread_safe(job)
            _consume(result)

    # Check if we have any successful results
    if not all_rows:
        print("‚ùå All jobs failed! Running sequentially as fallback...")
        for job in jobs:
            try:
                result = _run_single_experiment(job[0], job[1], job[2])
                _consume(result)
            except Exception as e:
                print(f"‚ùå Sequential fallback also failed for n_qubits={job[0]}, steps={job[1]}, seed={job[2]}: {e}")

    if not all_rows:
        print("üí• CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------ Performance Results ----------------
    print("üìä Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")

    # Only aggregate successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0:
        perf_agg = perf_df.groupby(["n_qubits", "steps"]).agg({
            "runtime_s": ["mean", "std"],
            "peak_mem_mb": ["mean", "std"],
            "error": ["mean", "std"],
            "success": "mean"
        }).reset_index()
        # Flatten column names
        perf_agg.columns = ['_'.join(col).strip('_') for col in perf_agg.columns.values]
    else:
        perf_agg = pd.DataFrame({"note": ["No successful experiments"]})

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------ Scalability Results ----------------
    print("üìà Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")

    # Compute scaling coefficients only for successful runs
    success_df = perf_df[perf_df["success"]]
    if len(success_df) > 0 and len(success_df["n_qubits"].unique()) >= 2:
        # Log-log fit for scaling analysis
        scaling_data = success_df.groupby("n_qubits")["runtime_s"].mean().reset_index()
        scaling_data["dimension"] = 2 ** scaling_data["n_qubits"]
        log_dim = np.log(scaling_data["dimension"].values)
        log_time = np.log(scaling_data["runtime_s"].values + 1e-12)

        # Only compute if we have enough data and variation
        if len(log_dim) >= 2 and np.std(log_dim) > 1e-6 and np.std(log_time) > 1e-6:
            try:
                scaling_coeff = np.polyfit(log_dim, log_time, 1)[0]
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [scaling_coeff],
                    "description": ["Exponent in time ~ dimension^exponent"]
                })
            except:
                scaling_summary = pd.DataFrame({
                    "parameter": ["scaling_exponent"],
                    "value": [np.nan],
                    "description": ["Numerical issues in scaling analysis"]
                })
        else:
            scaling_summary = pd.DataFrame({
                "parameter": ["scaling_exponent"],
                "value": [np.nan],
                "description": ["Insufficient data variation for scaling analysis"]
            })
    else:
        scaling_summary = pd.DataFrame({
            "parameter": ["scaling_exponent"],
            "value": [np.nan],
            "description": ["No successful experiments for scaling analysis"]
        })

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_data": perf_df[["n_qubits", "steps", "dimension", "runtime_s", "peak_mem_mb", "success"]],
            "scaling_analysis": scaling_summary
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------ Reliability Results ----------------
    print("üéØ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    reliability_analysis = perf_df.groupby(["n_qubits", "steps"]).agg({
        "error": ["mean", "std", "min", "max"],
        "runtime_s": ["mean", "std"],
        "success": "mean"
    }).reset_index()
    reliability_analysis.columns = ['_'.join(col).strip('_') for col in reliability_analysis.columns.values]

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "reliability_analysis": reliability_analysis,
            "raw_results": perf_df[["n_qubits", "steps", "seed", "error", "success"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------ Carbon Footprint Results ----------------
    print("üåç Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"‚úÖ Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"‚úÖ Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"‚ùå ERROR loading carbon data: {e}")
        print("üí° Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["n_qubits", "steps", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"‚úÖ Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    success_count = perf_df["success"].sum()
    total_count = len(perf_df)
    success_rate = success_count / total_count if total_count > 0 else 0

    print("\n" + "="*60)
    print("üéâ ADIABATIC QUANTUM SIMULATION EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"üìÅ Performance:     {perf_excel}")
    print(f"üìÅ Scalability:     {scal_excel}")
    print(f"üìÅ Reliability:     {rel_excel}")
    print(f"üìÅ Carbon:          {carb_excel}")
    print(f"üñ•Ô∏è  Device:          {get_device_name()}")
    print(f"üìä Total runs:      {total_count}")
    print(f"‚úÖ Successful:      {success_count}")
    print(f"‚ùå Failed:          {total_count - success_count}")
    print(f"‚ö° Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"üî¢ Qubits tested:   {sorted(perf_df['n_qubits'].unique())}")
    print(f"üîÑ Steps tested:    {sorted(perf_df['steps'].unique())}")
    print(f"üéØ Success rate:    {success_rate * 100:.1f}%")

    if success_count > 0:
        success_df = perf_df[perf_df["success"]]
        print(f"üìè Mean error:      {success_df['error'].mean():.2e}")
        print(f"‚è±Ô∏è  Mean runtime:    {success_df['runtime_s'].mean():.3f}s")
        print(f"üßÆ Max dimension:   {success_df['dimension'].max()}")
    else:
        print(f"üìè Mean error:      N/A (no successful runs)")
        print(f"‚è±Ô∏è  Mean runtime:    N/A (no successful runs)")

    print(f"\nüìÇ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   ‚Ä¢ {d}")

    return perf_df

# ------------********------------
# ------------********------------ Main execution for Kaggle
# ------------********------------
if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("üöÄ Starting Adiabatic Quantum Simulation Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_adiabatic_simulation_experiment()

    if results_df is not None:
        # Display final results summary
        success_count = results_df["success"].sum()
        total_count = len(results_df)

        print("\nüìä FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {total_count}")
        print(f"Successful experiments: {success_count}")
        print(f"Failed experiments: {total_count - success_count}")
        print(f"Qubits tested: {sorted(results_df['n_qubits'].unique())}")
        print(f"Steps tested: {sorted(results_df['steps'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.3f}s")
        print(f"Success rate: {success_count / total_count * 100:.1f}%")

        if success_count > 0:
            success_df = results_df[results_df["success"]]
            print(f"Average error: {success_df['error'].mean():.2e}")
            print(f"Max Hilbert dimension: {success_df['dimension'].max()}")
        else:
            print(f"Average error: N/A")
            print(f"Max Hilbert dimension: N/A")

        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nüìÅ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}üìÅ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "üìä" if file.endswith('.xlsx') else "üñºÔ∏è"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\n‚úÖ All Adiabatic Quantum Simulation deliverables generated successfully!")
        print("   ‚Ä¢ Performance/performance_results.xlsx + perf_*.png")
        print("   ‚Ä¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   ‚Ä¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   ‚Ä¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("‚ùå Adiabatic Quantum Simulation benchmark failed - no results generated")

