#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------------------------------------------------------------------------------------------------------------
"""
GPU-accelerated Quantum Hamiltonian Simulation

14 Hamiltonian Simulation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/i222070tabidahusmani/14-hamiltonian-simulation.70485376-8553-4af3-8b73-4ea1a4883796.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251122/auto/storage/goog4_request%26X-Goog-Date%3D20251122T180045Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da4b030c44528838f044c4a3854fd2b2b6421d85e8705ec28aabd362e649d7571e93a8de7d6439679e4e7d59a763d465317188a115a5aaa87018c3568b54284d27bed7c1b6770ebc160a6061ed4e337affc1a049752db99b7692ae03003ae3a832304a73b3d375203088db17e2d9c7cef03ff780e016e5d41b57067448039c02cd32fa4c4cd6b72bdb9eaa4fe96e273140f4c49188e6f3ee23b74d1898c14ad4e7175fc429c19517e9d603ff14b92a8433aa1c093547f460689b5fcc527ed7b20f7c6ee4b9c9b7011e81cca533d71006b5b48c1284ce517776e3af7996a1513223a89a2f38998bd7ca701798ffaf8004bdb804c57ea9f2e5b2a86996ccc1df901

"""
# ----------------------------------------------------------------------------------------------------------------

# Notes: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Code focuses on Performance, Scalability, Reliability, Carbon footprints
# Output folders are:
    #   Performance = runtime
    #   Scalability = growth of runtime as N increases (digits/bits).
    #   Reliability = success rate + dispersion across repetitions.
    #   Carbon footprints = derived *only* from PERFORMANCE totals
    #   (sum of successful wall-times), device power 65 Watts fixed for comparasion, PUE 1.2 fixed, 
    #   country carbon intensity (kg CO2/kWh). I did not use scalability or reliability results for CO2.

#      Carbon Data File: 
#      Uses ONLY Performance runtimes (not Scalability/Reliability), 
#      Placed on my Desktop Excel of country intensities "Filtered CO2 intensity 236 Countries.xlsx"
#      (CSV or XLSX; year column optional. I have selected --year-select latest.)
# ----------------------------------------------------------------------------------------------------------------


# ------------********------------ Imports
import os, sys, time, tracemalloc, pathlib, warnings, math
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------********------------
# GPU Setup and Dependencies
# ------------********------------

#This block probes the system for a usable CUDA-enabled GPU via CuPy.
#A lightweight allocation and reduction test verifies that the GPU is functional.
#Hardware details such as device name, memory, and compute capability are reported when available.
#If GPU setup fails, the code gracefully falls back to NumPy for CPU-based execution.

print("ğŸ” Checking GPU availability...")

HAS_GPU = False
cp = None

try:
    import cupy as cp
    # Test if GPU is available and working
    with cp.cuda.Device(0):
        # Simple test to verify GPU functionality
        test_array = cp.zeros(10, dtype=cp.float32)
        result = cp.sum(test_array)
        HAS_GPU = True
        print(f"âœ… GPU detected: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")
        print(f"   Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB")
        print(f"   Compute Capability: {cp.cuda.runtime.getDeviceProperties(0)['major']}.{cp.cuda.runtime.getDeviceProperties(0)['minor']}")
except ImportError:
    print("âŒ CuPy not available, falling back to NumPy")
    import numpy as cp
except Exception as e:
    print(f"âš ï¸  GPU setup failed: {e}, falling back to NumPy")
    import numpy as cp

# If GPU failed, use NumPy
if not HAS_GPU:
    import numpy as cp
    print("ğŸ”„ Using NumPy (CPU) backend")

# Install required packages
def _ensure(pkgs: List[str]):
    import importlib, subprocess
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except ImportError:
            missing.append(p)
    if missing:
        print(f"[setup] Installing missing packages: {missing}")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *missing])
        except Exception as e:
            print(f"[setup] Auto-install failed: {e}")

_ensure(["pandas", "matplotlib", "openpyxl", "psutil"])

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# GPU utility functions
def get_device_name() -> str:
    #  Get current device name
    if HAS_GPU:
        try:
            return f"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}"
        except:
            return "GPU: Unknown"
    return "CPU: NumPy"

def safe_gpu_cleanup():
    #  Safely cleanup GPU memory
    if HAS_GPU:
        try:
            cp.get_default_memory_pool().free_all_blocks()
        except:
            pass

# Thread-safe GPU operations
_gpu_lock = threading.Lock()

def run_experiment_thread_safe(args):
    
   # Thread-safe GPU experiment runner.
   # Uses a lock to prevent multiple threads from accessing GPU simultaneously.
    
    N, steps, seed, T, r_steps, order, J, h, periodic, acc_tol, ref_mult = args
    with _gpu_lock:  # Ensure only one thread uses GPU at a time
        return _run_single_experiment(N, steps, seed, T, r_steps, order, J, h, periodic, acc_tol, ref_mult)

# ------------********------------
# ========================= Quantum Hamiltonian Simulation Core =========================
# ------------********------------
#This helper generates index arrays needed for statevector-based quantum operations.
#It computes the full Hilbert space dimension from the number of qubits.
#The resulting index array is created using the active array backend for efficiency.

def _idx_arrays(n):
    """Generate index arrays for statevector operations"""
    dim = 1 << n
    idx = cp.arange(dim, dtype=cp.int64)
    return dim, idx


#This routine applies a single-qubit Rx rotation to an n-qubit statevector in a fully vectorized manner.
#Bit masks are used to separate basis states where the target qubit is 0 or 1.
#The rotation is implemented via cosine and sine coefficients corresponding to the Rx unitary.
#State slices are updated in place to efficiently simulate the gate on CPU or GPU backends.

def _apply_rx_qubit(state: cp.ndarray, n: int, q: int, theta: float):
    # Apply Rx(theta) = exp(-i theta X / 2) on qubit q of an n-qubit statevector.
    # Vectorized via bit-masks.
   
    mask = 1 << q
    dim = state.size
    idx = cp.arange(dim, dtype=cp.int64)
    idx0 = idx[(idx & mask) == 0]
    idx1 = idx0 | mask
    c = math.cos(theta/2.0)
    s = math.sin(theta/2.0)
    a = state[idx0].copy()
    b = state[idx1].copy()
    # Rx: [c  -i s; -i s  c]
    state[idx0] = c*a - 1j*s*b
    state[idx1] = c*b - 1j*s*a



def _precompute_zz_phase(n: int, pair, theta: float, idx: cp.ndarray):
   
    #For Z_i Z_j rotation with angle theta': exp(-i theta' Z_i Z_j).
    #For computational basis |z>, phase = exp(-i theta' * z_i z_j), with zâˆˆ{+1,-1}.
    #Returns a vector 'phase' of shape (2^n,) to multiply elementwise.
    
    i, j = pair
    zi = 1 - 2*((idx >> i) & 1)  # +1 if bit=0, -1 if bit=1
    zj = 1 - 2*((idx >> j) & 1)
    sgn = zi * zj
    return cp.exp(-1j * theta * sgn)

def _apply_phase_layer(state: cp.ndarray, phase: cp.ndarray):
    # Apply phase multiplication elementwise
    state *= phase


#This function def simulate_trotter simulates real-time quantum evolution using a Trotter product formula.
#       It supports first- and second-order decompositions and optional periodic boundary conditions.
#       Neighbor interactions and single-qubit rotations are parameterized by physical constants and time step size.
#       Phase factors for ZZ interactions are cached to avoid redundant computation across runs.

def simulate_trotter(n: int, T: float, r: int, order: int, J: float, h: float,
                     periodic: bool, init_state: cp.ndarray,
                     cache=None):
    """
    Simulate e^{-i H T} |psi> by a product formula with r steps.
    order: 1 (Lie-Trotter) or 2 (Strang)
    cache: dict to reuse precomputed phase vectors for ZZ layer.
    """
    state = init_state.copy()
    dim, idx = _idx_arrays(n)

    # neighbor pairs for ZZ
    pairs = [(i, i+1) for i in range(n-1)]
    if periodic and n >= 2:
        pairs.append((n-1, 0))

    dt = T / max(1, r)
    # angles
    theta_x = 2.0 * h * dt       # because e^{-i h dt X} = Rx(2 h dt)
    theta_zz = J * dt            # e^{-i J dt Z Z}

    # Precompute ZZ phase vectors once
    if cache is None:
        cache = {}
    key = (n, periodic, J, dt)
    if key not in cache:
        phase_list = [ _precompute_zz_phase(n, pr, theta_zz, idx) for pr in pairs ]
        cache[key] = phase_list
    phase_list = cache[key]


#These following inner helper functions define the building blocks of a single Trotter step.
#       They apply half or full layers of single-qubit X rotations across all qubits.
#       A separate layer applies precomputed ZZ interaction phases to the statevector.
#       Encapsulating these operations simplifies constructing different Trotter orders.

    # One step kernels
    def layer_X_half():
        th = theta_x/2.0
        for q in range(n):
            _apply_rx_qubit(state, n, q, th)
    def layer_X_full():
        for q in range(n):
            _apply_rx_qubit(state, n, q, theta_x)
    def layer_ZZ_full():
        for ph in phase_list:
            _apply_phase_layer(state, ph)

    # Evolve
    if order == 1:
        for _ in range(r):
            layer_ZZ_full()
            layer_X_full()
    else:  # order==2
        for _ in range(r):
            layer_X_half()
            layer_ZZ_full()
            layer_X_half()

    # Normalize (protect from tiny numerical drift)
    norm = cp.linalg.norm(state)
    if norm == 0:
        return state
    return state / norm


#This function generates a normalized random quantum state of size 2^n.
#It supports both GPU and CPU backends, generating randomness on the appropriate device.
#The state is normalized to unit length to represent a valid quantum statevector.

def random_state(n: int, rng) -> cp.ndarray:
    """Generate random quantum state on GPU"""
    dim = 1 << n
    if HAS_GPU:
        # Generate random numbers on GPU
        a_real = cp.random.normal(size=dim)
        a_imag = cp.random.normal(size=dim)
        a = a_real + 1j * a_imag
    else:
        # Generate on CPU then transfer
        a_real = rng.normal(size=dim)
        a_imag = rng.normal(size=dim)
        a = cp.asarray(a_real + 1j * a_imag)
    a /= cp.linalg.norm(a)
    return a.astype(cp.complex128)

def _run_single_experiment(N: int, steps: int, seed: int, T: float, r_steps: int,
                          order: int, J: float, h: float, periodic: int,
                          acc_tol: float, ref_mult: int) -> dict:
    """
    Core experiment logic - run single Hamiltonian simulation benchmark.
    """
    # Use numpy RNG for consistency
    rng_np = np.random.default_rng(seed)
    periodic = bool(periodic)
    order = int(order)
    r_steps = int(r_steps)
    r_ref = max(1, int(ref_mult) * max(1, r_steps))

    tracemalloc.start()
    t0 = time.perf_counter()

    # Coarse simulation runtime only
    cache_coarse = {}
    cache_ref = {}  # for ref phase precomputation
    fid_sum = 0.0
    rmse_sum = 0.0

    # Measure coarse-only wall time
    for _ in range(steps):
        psi0 = random_state(N, rng_np)
        _ = simulate_trotter(N, T, r_steps, order, J, h, periodic, psi0, cache=cache_coarse)
    runtime = time.perf_counter() - t0
    _, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Now compute accuracy (not counted in runtime on purpose)
    for _ in range(steps):
        psi0 = random_state(N, rng_np)
        psi_coarse = simulate_trotter(N, T, r_steps, order, J, h, periodic, psi0, cache=cache_coarse)
        psi_ref    = simulate_trotter(N, T, r_ref,   2,     J, h, periodic, psi0, cache=cache_ref)  # high-accuracy Strang

        # Compute fidelity and RMSE
        inner = cp.vdot(psi_ref, psi_coarse)
        fid = float(cp.abs(inner)**2)
        err = float(cp.sqrt(cp.mean(cp.abs(psi_ref - psi_coarse)**2)))
        fid_sum += fid
        rmse_sum += err

    accuracy_mean = fid_sum / max(1, steps)
    rmse_mean = rmse_sum / max(1, steps)

    # Work units (coarse only): number of exponentials per Trotter step
    zz_ops = N if periodic and N>=2 else max(0, N-1)
    x_ops = N
    if order == 1:
        exp_per_step = zz_ops + x_ops
    else:
        exp_per_step = zz_ops + 2*x_ops
    ops_total = exp_per_step * steps * r_steps

    success = bool(accuracy_mean >= acc_tol)

    # Cleanup GPU memory
    safe_gpu_cleanup()

    return {
        "N": int(N),
        "steps": int(steps),
        "seed": int(seed),
        "T": float(T),
        "r_steps": int(r_steps),
        "order": int(order),
        "J": float(J),
        "h": float(h),
        "periodic": int(periodic),
        "ref_mult": int(ref_mult),
        "accuracy_mean": float(accuracy_mean),
        "rmse_mean": float(rmse_mean),
        "success": success,
        "runtime_s": float(runtime),
        "peak_mem_mb": float(peak / (1024**2)),
        "exp_ops_total": int(ops_total),
        "device": get_device_name()
    }

# ------------********------------
# ============================== Carbon I/O ==============================
# ------------********------------

def resolve_excel_path(excel_arg: str) -> str:
    p = pathlib.Path(excel_arg)
    if p.is_absolute() and p.exists():
        return str(p)
    desktop = pathlib.Path.home() / "Desktop" / excel_arg
    return str(desktop if desktop.exists() else p)

def load_carbon_excel(path: str, year_select: str = "latest") -> pd.DataFrame:
    df = pd.read_excel(path)
    cols = {c.lower(): c for c in df.columns}
    cand_country = next((v for k, v in cols.items() if "country" in k or "nation" in k or k == "location"), None)
    if cand_country is None:
        raise ValueError("No 'Country' column found.")
    cand_year = next((v for k, v in cols.items() if "year" in k or "date" in k), None)
    cand_intensity = next((v for k, v in cols.items()
                           if "intensity" in k or ("co2" in k and "kwh" in k) or "kgco2" in k or "gco2" in k), None)
    if cand_intensity is None:
        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        if cand_year in numeric_cols:
            numeric_cols.remove(cand_year)
        if not numeric_cols:
            raise ValueError("No numeric intensity column detected.")
        cand_intensity = numeric_cols[0]
    keep = [cand_country] + ([cand_year] if cand_year else []) + [cand_intensity]
    df = df[keep].copy()
    df.columns = ["Country", "Year", "Intensity"] if len(keep) == 3 else ["Country", "Intensity"]
    if "Year" in df.columns and year_select.lower() == "latest":
        df = df.sort_values(["Country", "Year"]).groupby("Country", as_index=False).tail(1)

    # Heuristic unit fix: if median looks like g/kWh, convert to kg/kWh
    med = float(df["Intensity"].dropna().median())
    if med > 50:
        df["Intensity"] = df["Intensity"] / 1000.0

    return df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)

def compute_carbon(perf_df: pd.DataFrame, intensity_df: pd.DataFrame,
                   power_watts: float, pue: float, combine: bool) -> tuple[pd.DataFrame, pd.DataFrame]:
    total_runtime_s = float(perf_df["runtime_s"].sum())
    kWh_total = power_watts * pue * total_runtime_s / 3_600_000.0
    df = intensity_df.copy()
    df["kWh"] = kWh_total
    df["kgCO2e"] = df["Intensity"] * df["kWh"]

    # ALWAYS return country-wise results (ignore combine parameter)
    carbon_output = df.sort_values("kgCO2e", ascending=False)

    summary = pd.DataFrame({
        "total_runtime_s": [total_runtime_s],
        "power_watts": [power_watts],
        "PUE": [pue],
        "kWh_total": [kWh_total],
        "median_intensity_kg_per_kWh": [float(df["Intensity"].median())],
        "mean_kgCO2e_across_countries": [float(df["kgCO2e"].mean())],
        "device": [perf_df["device"].iloc[0] if "device" in perf_df.columns else "Unknown"],
        "combine_mode": [combine],
        "countries_analyzed": [len(df)]
    })
    return carbon_output, summary

# ------------********------------
# ============================== Plot Helpers ==============================
# ------------********------------

plt_kwargs = dict(dpi=140, bbox_inches="tight")

def _boxplot_with_labels(data, labels):
    # Matplotlib 3.9 renamed 'labels' -> 'tick_labels'
    try:
        plt.boxplot(data, tick_labels=labels)
    except TypeError:
        plt.boxplot(data, labels=labels)

def plot_performance(df: pd.DataFrame, outdir: str):
    """Plot performance metrics"""
    # Runtime vs N
    g = df.groupby("N")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g["N"], g["runtime_s"], marker="o", linewidth=2, markersize=8)
    plt.title(f"Performance: Runtime vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Runtime (s)")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_runtime_vs_N.png"), **plt_kwargs)
    plt.close()

    # Operations vs N
    g2 = df.groupby("N")["exp_ops_total"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["N"], g2["exp_ops_total"], marker="s", linewidth=2, markersize=8)
    plt.title(f"Performance: Exponential Operations vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Exponential Operations")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "perf_ops_vs_N.png"), **plt_kwargs)
    plt.close()

    # Fidelity vs N
    g3 = df.groupby("N")["accuracy_mean"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g3["N"], g3["accuracy_mean"], marker="^", linewidth=2, markersize=8, color='green')
    plt.title(f"Performance: Mean Fidelity vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Mean Fidelity")
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1.05)
    plt.savefig(os.path.join(outdir, "perf_fidelity_vs_N.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_scalability(df: pd.DataFrame, outdir: str):
    """Plot scalability metrics"""
    # Runtime scaling with N (log-log)
    g = df.groupby("N")["runtime_s"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.loglog(g["N"], g["runtime_s"], marker="o", markersize=8)
    plt.title(f"Scalability: Runtime vs Qubits (log-log)\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Runtime (s)")
    plt.grid(True, which="both", alpha=0.3)
    plt.savefig(os.path.join(outdir, "scal_loglog_runtime_vs_N.png"), **plt_kwargs)
    plt.close()

    # Memory scaling with N
    g2 = df.groupby("N")["peak_mem_mb"].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(g2["N"], g2["peak_mem_mb"], marker="s", linewidth=2, markersize=8, color='red')
    plt.title(f"Scalability: Memory vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Peak Memory (MB)")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "scal_memory_vs_N.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_reliability(df: pd.DataFrame, outdir: str):
    """Plot reliability metrics"""
    # Fidelity distribution by N
    data = [df[df["N"] == n]["accuracy_mean"].values for n in sorted(df["N"].unique())]
    plt.figure(figsize=(10, 6))
    _boxplot_with_labels(data, labels=sorted(df["N"].unique()))
    plt.title(f"Reliability: Fidelity Distribution by Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Fidelity")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "rel_fidelity_distribution.png"), **plt_kwargs)
    plt.close()

    # Success rate trend
    success_rates = df.groupby('N')['success'].mean().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(success_rates['N'], success_rates['success'], marker='o', linewidth=2, markersize=8)
    plt.title(f"Reliability: Batch Success Rate vs Qubits\n{get_device_name()}")
    plt.xlabel("Number of Qubits (N)")
    plt.ylabel("Batch Success Rate")
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1.1)
    plt.savefig(os.path.join(outdir, "rel_batch_success.png"), **plt_kwargs)
    plt.close()

# ------------********------------
def plot_carbon(df: pd.DataFrame, combine: bool, outdir: str):
    # Generates carbon footprint plots - ALWAYS country-wise
    # Top 15 highest emissions
    top = df.nlargest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(top["Country"][::-1], top["kgCO2e"][::-1], color='red', alpha=0.7)
    plt.xlabel("kg CO2e (higher = dirtier grid)", fontsize=12)
    plt.title(f"Carbon: Top 15 Highest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_top15.png"), **plt_kwargs)
    plt.close()

    # Bottom 15 lowest emissions
    bot = df.nsmallest(15, "kgCO2e")
    plt.figure(figsize=(12, 8))
    plt.barh(bot["Country"][::-1], bot["kgCO2e"][::-1], color='green', alpha=0.7)
    plt.xlabel("kg CO2e (lower = cleaner grid)", fontsize=12)
    plt.title(f"Carbon: Bottom 15 Lowest Emissions\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "carbon_bottom15.png"), **plt_kwargs)
    plt.close()

    # Distribution histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df["kgCO2e"], bins=30, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel("kg CO2e for this experiment", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.title(f"Carbon: Emission Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_distribution.png"), **plt_kwargs)
    plt.close()

    # CDF plot
    xs = np.sort(df["kgCO2e"].values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    plt.figure(figsize=(10, 6))
    plt.plot(xs, ys, linewidth=2, color='purple')
    plt.xlabel("kg CO2e", fontsize=12)
    plt.ylabel("CDF", fontsize=12)
    plt.title(f"Carbon: Cumulative Distribution\n{get_device_name()}", fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(outdir, "carbon_cdf.png"), **plt_kwargs)
    plt.close()

# ------------********------------
# ============================== Utility Functions ==============================
# ------------********------------

#These utility functions manage output directories and structured result export.
#   One ensures that required filesystem paths exist without raising errors.
#       The other def to_excel () writes multiple pandas DataFrames into a single Excel file using separate sheets.

def ensure_dir(d: str):
    os.makedirs(d, exist_ok=True)

def to_excel(dfs: Dict[str, pd.DataFrame], path: str):
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        for name, df in dfs.items():
            df.to_excel(w, index=False, sheet_name=name[:31])

# ============================== Main Experiment Runner ==============================

def run_hamiltonian_simulation_experiment():

    """
    Main Quantum Hamiltonian Simulation experiment runner with hardcoded parameters for Kaggle.
    Equivalent to: --sizes 6 8 10 --steps 50 --trials 10 --time 1.0 --trotter-steps 100 --order 2
                   --J 1.0 --h 1.0 --periodic 0 --acc-tol 0.90 --ref-mult 20
                   --device-power-watts 65 --pue 1.2 --combine
    """


    # Experiment parameters
    sizes = [6, 8, 10]            # Number of qubits
    trials = 10                   # seeds per N
    steps = 50                    # random initial states per trial
    T = 1.0                       # total evolution time
    r_steps = 100                 # Trotter steps for coarse simulation
    order = 2                     # product formula order (2 = Strang)
    J = 1.0                       # Ising ZZ coupling
    h = 1.0                       # transverse field
    periodic = 0                  # periodic boundary conditions
    acc_tol = 0.90                # accuracy tolerance
    ref_mult = 20                 # reference multiplier
    method = "hamiltonian_simulation"
    workers = 4                   # Number of parallel threads

    # Carbon parameters
    device_power_watts = 65.0
    pue = 1.2
    combine = True

    print(f"ğŸš€ Starting Quantum Hamiltonian Simulation experiments on {get_device_name()}")
    print(f"ğŸ”§ Configuration:")
    print(f"   Qubits: {sizes}")
    print(f"   Trials per size: {trials}")
    print(f"   States per trial: {steps}")
    print(f"   Evolution time: {T}")
    print(f"   Trotter steps: {r_steps}")
    print(f"   Order: {order}")
    print(f"   J (coupling): {J}")
    print(f"   h (field): {h}")
    print(f"   Periodic: {bool(periodic)}")
    print(f"   Accuracy tolerance: {acc_tol}")
    print(f"   Reference multiplier: {ref_mult}")
    print(f"   Method: {method}")
    print(f"   Workers: {workers}")
    print(f"   Device power: {device_power_watts}W")
    print(f"   PUE: {pue}")
    print(f"   Combine carbon: {combine}")

    # Create output folders
    perf_dir = os.path.join(os.getcwd(), "Performance")
    scal_dir = os.path.join(os.getcwd(), "Scalability")
    rel_dir  = os.path.join(os.getcwd(), "Reliability")
    carb_root = os.path.join(os.getcwd(), "Carbon footprints")
    carb_dir  = os.path.join(carb_root, "carbon_by_country")

    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        ensure_dir(d)

    # Prepare jobs
    jobs = []
    for N in sizes:
        for i in range(trials):
            seed = 1000 + 53 * N + i
            jobs.append((N, steps, seed, T, r_steps, order, J, h, periodic, acc_tol, ref_mult))

    all_rows = []

    def _consume(row):
        """Process completed job results"""
        all_rows.append(row)
        print(f"  âœ… N={row['N']}, seed={row['seed']} "
              f"(runtime={row['runtime_s']:.3f}s, fidelity={row['accuracy_mean']:.3f}, "
              f"RMSE={row['rmse_mean']:.4f}, success={row['success']})")

    # Execute jobs with thread-based parallelization
    print(f"ğŸ”€ Running {len(jobs)} jobs in parallel with {workers} threads...")

    if workers > 1 and len(jobs) > 1:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all jobs
            future_to_job = {executor.submit(run_experiment_thread_safe, job): job for job in jobs}

            # Collect results as they complete
            for future in as_completed(future_to_job):
                try:
                    result = future.result()
                    _consume(result)
                except Exception as e:
                    job = future_to_job[future]
                    print(f"âŒ Job failed N={job[0]}, seed={job[2]}: {e}")
    else:
        # Fallback to sequential execution
        for job in jobs:
            try:
                result = run_experiment_thread_safe(job)
                _consume(result)
            except Exception as e:
                print(f"âŒ Job failed N={job[0]}, seed={job[2]}: {e}")

    # Check if we have any successful results
    if not all_rows:
        print("ğŸ’¥ CRITICAL: No experiments completed successfully!")
        return None

    perf_df = pd.DataFrame(all_rows)

# ------------********------------
    # ---------------- Performance Results ----------------
# ------------********------------

    print("ğŸ“Š Generating Performance results...")
    perf_excel = os.path.join(perf_dir, "performance_results.xlsx")
    perf_agg = perf_df.groupby("N").agg({
        "runtime_s": "mean",
        "accuracy_mean": "mean",
        "rmse_mean": "mean",
        "exp_ops_total": "mean",
        "peak_mem_mb": "mean",
        "success": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "raw_runs": perf_df,
            "aggregated": perf_agg
        }, perf_excel)

    plot_performance(perf_df, perf_dir)

# ------------********------------
    # ---------------- Scalability Results ----------------
# ------------********------------

    print("ğŸ“ˆ Generating Scalability results...")
    scal_excel = os.path.join(scal_dir, "scalability_results.xlsx")
    scal_agg = perf_df.groupby("N").agg({
        "runtime_s": "mean",
        "peak_mem_mb": "mean",
        "exp_ops_total": "mean"
    }).reset_index()

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "from_perf_runs": perf_df[["N", "seed", "runtime_s", "peak_mem_mb", "exp_ops_total"]],
            "aggregated": scal_agg
        }, scal_excel)

    plot_scalability(perf_df, scal_dir)

# ------------********------------
    # ---------------- Reliability Results ----------------
# ------------********------------

    print("ğŸ¯ Generating Reliability results...")
    rel_excel = os.path.join(rel_dir, "reliability_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "fidelity_analysis": perf_df[["N", "seed", "accuracy_mean", "rmse_mean", "success"]]
        }, rel_excel)

    plot_reliability(perf_df, rel_dir)

# ------------********------------
    # ---------------- Carbon Footprint Results ----------------
# ------------********------------

    print("ğŸŒ Generating Carbon footprint results...")

    # Load carbon data from Kaggle input directory
    excel_path = "/kaggle/input/masters/Filtered CO2 intensity 236 Countries.xlsx"
    intensity_df = None

    try:
        # Try to load from the exact path
        intensity_df = pd.read_excel(excel_path)
        print(f"âœ… Loaded carbon data from: {excel_path}")
        print(f"   Columns found: {list(intensity_df.columns)}")
        print(f"   Countries loaded: {len(intensity_df)}")

        # Auto-detect and process columns
        cols = {c.lower(): c for c in intensity_df.columns}
        cand_country = next((v for k,v in cols.items() if "country" in k or "nation" in k or k=="location"), None)
        if cand_country is None:
            raise ValueError("No 'Country' column found.")

        cand_intensity = next((v for k,v in cols.items() if "intensity" in k or ("co2" in k and ("kwh" in k or "/kwh" in k)) or "kgco2" in k or "gco2" in k), None)

        if cand_intensity is None:
            numeric_cols = [c for c in intensity_df.columns if pd.api.types.is_numeric_dtype(intensity_df[c])]
            if not numeric_cols:
                raise ValueError("No numeric intensity column detected.")
            cand_intensity = numeric_cols[0]

        # Keep only necessary columns
        intensity_df = intensity_df[[cand_country, cand_intensity]].copy()
        intensity_df.columns = ["Country", "Intensity"]

        # Convert gCO2/kWh to kgCO2/kWh if needed
        med = float(intensity_df["Intensity"].dropna().median())
        if med > 50:
            intensity_df["Intensity"] = intensity_df["Intensity"] / 1000.0

        intensity_df = intensity_df.dropna(subset=["Country", "Intensity"]).reset_index(drop=True)
        print(f"âœ… Processed carbon data for {len(intensity_df)} countries")

    except Exception as e:
        print(f"âŒ ERROR loading carbon data: {e}")
        print("ğŸ’¡ Using sample carbon data as fallback...")
        # Create sample data similar to other codes
        countries = ["France", "Sweden", "Norway", "Switzerland", "Austria", "Germany", "United States", "China", "India"]
        intensities = [52, 41, 32, 45, 62, 385, 389, 537, 708]
        intensity_df = pd.DataFrame({
            "Country": countries,
            "Intensity": [i/1000.0 for i in intensities]  # Convert to kg/kWh
        })

    ensure_dir(carb_dir)
    carbon_df, summary_df = compute_carbon(perf_df, intensity_df,
                                           power_watts=device_power_watts,
                                           pue=pue, combine=combine)
    carb_excel = os.path.join(carb_dir, "carbon_results.xlsx")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        to_excel({
            "carbon_results": carbon_df,
            "summary": summary_df,
            "intensity_input": intensity_df,
            "performance_reference": perf_df[["N", "seed", "runtime_s"]]
        }, carb_excel)

    # Generate carbon plots
    plot_carbon(carbon_df, combine, carb_dir)
    print(f"âœ… Carbon analysis complete: {len(carbon_df)} entries")

    # Final summary
    print("\n" + "="*60)
    print("ğŸ‰ QUANTUM HAMILTONIAN SIMULATION EXPERIMENT COMPLETE - RESULTS SUMMARY")
    print("="*60)
    print(f"ğŸ“ Performance:     {perf_excel}")
    print(f"ğŸ“ Scalability:     {scal_excel}")
    print(f"ğŸ“ Reliability:     {rel_excel}")
    print(f"ğŸ“ Carbon:          {carb_excel}")
    print(f"ğŸ–¥ï¸  Device:          {get_device_name()}")
    print(f"ğŸ“Š Total runs:      {len(perf_df)}")
    print(f"âš¡ Total runtime:   {perf_df['runtime_s'].sum():.2f}s")
    print(f"ğŸ”¬ Qubit range:     {sizes}")
    print(f"ğŸ¯ Success rate:    {perf_df['success'].mean() * 100:.1f}%")
    print(f"ğŸ”¢ Total operations: {perf_df['exp_ops_total'].sum():,}")

    print(f"\nğŸ“‚ All results saved to:")
    for d in [perf_dir, scal_dir, rel_dir, carb_dir]:
        print(f"   â€¢ {d}")

    return perf_df

# ============================================================
# ------------********------------     Main execution for Kaggle
# ============================================================
#This main execution block initializes resources and launches the Hamiltonian simulation benchmark.
#It ensures GPU state is clean, runs all experiments, and summarizes key performance and accuracy metrics.
#Generated outputs and directory structures are printed for easy inspection.
#The workflow concludes with a clear success or failure message based on experiment completion.

if __name__ == "__main__":
    # Clean up GPU state
    safe_gpu_cleanup()

    print("ğŸš€ Starting Quantum Hamiltonian Simulation Benchmark on Kaggle")
    print("=" * 50)

    # Run the complete experiment
    results_df = run_hamiltonian_simulation_experiment()

    if results_df is not None:
        # Display final results summary
        print("\nğŸ“Š FINAL RESULTS SUMMARY")
        print("=" * 30)
        print(f"Total experiments: {len(results_df)}")
        print(f"Qubit range: {sorted(results_df['N'].unique())}")
        print(f"Average runtime: {results_df['runtime_s'].mean():.6f}s")
        print(f"Success rate: {results_df['success'].mean() * 100:.1f}%")
        print(f"Average fidelity: {results_df['accuracy_mean'].mean():.3f}")
        print(f"Total exponential operations: {results_df['exp_ops_total'].sum():,}")
        print(f"Device used: {get_device_name()}")

        # Show folder structure
        print("\nğŸ“ GENERATED OUTPUT STRUCTURE:")
        base_dir = '/kaggle/working'
        for root, dirs, files in os.walk(base_dir):
            # Only show our output directories
            if any(x in root for x in ['Performance', 'Scalability', 'Reliability', 'Carbon footprints']):
                level = root.replace(base_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}ğŸ“ {os.path.basename(root)}/')
                sub_indent = ' ' * 2 * (level + 1)
                for file in files:
                    if file.endswith(('.xlsx', '.png')):
                        file_path = os.path.join(root, file)
                        size = os.path.getsize(file_path) / 1024  # KB
                        icon = "ğŸ“Š" if file.endswith('.xlsx') else "ğŸ–¼ï¸"
                        print(f'{sub_indent}{icon} {file} ({size:.1f} KB)')

        print("\nâœ… All Quantum Hamiltonian Simulation deliverables generated successfully!")
        print("   â€¢ Performance/performance_results.xlsx + perf_*.png")
        print("   â€¢ Scalability/scalability_results.xlsx + scal_*.png")
        print("   â€¢ Reliability/reliability_results.xlsx + rel_*.png")
        print("   â€¢ Carbon footprints/carbon_by_country/carbon_results.xlsx + carbon_*.png")
    else:
        print("âŒ Quantum Hamiltonian Simulation benchmark failed - no results generated")